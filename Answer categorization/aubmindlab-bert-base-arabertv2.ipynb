{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12507227,"sourceType":"datasetVersion","datasetId":7894059},{"sourceId":12592865,"sourceType":"datasetVersion","datasetId":7953705}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers>=4.51.0 torch torchvision torchaudio accelerate bitsandbytes -q\n!pip install sentencepiece protobuf -q\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport gc\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\n# Check GPU availability\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nimport re\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSequenceClassification\nfrom typing import List, Dict, Tuple, Any, Optional\nfrom sklearn.metrics import f1_score, jaccard_score\nimport ast\nimport warnings\nfrom huggingface_hub import login\nwarnings.filterwarnings('ignore')\n\nHF_TOKEN = \"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\"\nlogin(HF_TOKEN)\n\nclass ArabicMedicalAnswerClassifier:\n    \"\"\"\n    Arabic Medical Answer Classifier supporting Arabic-optimized LLM models\n    \"\"\"\n    \n    # Model configurations with Arabic-specific models\n    MODEL_CONFIGS = {\n        # Arabic-specific models\n        'jais': {\n            'name': 'inception-mbzuai/jais-13b-chat',\n            'type': 'jais',\n            'context_length': 4096,\n            'description': 'Jais 13B Chat - Arabic-English bilingual model',\n            'arabic_native': True\n        },\n        'aragpt2': {\n            'name': 'aubmindlab/aragpt2-mega',\n            'type': 'aragpt',\n            'context_length': 1024,\n            'description': 'AraGPT2 Mega - Arabic GPT model',\n            'arabic_native': True\n        },\n        'arabert': {\n            'name': 'aubmindlab/bert-base-arabertv2',\n            'type': 'arabert',\n            'context_length': 512,\n            'description': 'AraBERT v2 - Arabic BERT for classification',\n            'arabic_native': True,\n            'classification_model': True\n        },\n        'camelbert': {\n            'name': 'CAMeL-Lab/bert-base-arabic-camelbert-mix',\n            'type': 'camelbert',\n            'context_length': 512,\n            'description': 'CAMeLBERT - NYU Arabic BERT model',\n            'arabic_native': True,\n            'classification_model': True\n        },\n        'marbert': {\n            'name': 'UBC-NLP/MARBERT',\n            'type': 'marbert',\n            'context_length': 512,\n            'description': 'MARBERT - Multilingual Arabic BERT',\n            'arabic_native': True,\n            'classification_model': True\n        },\n        'arat5': {\n            'name': 'UBC-NLP/AraT5-base',\n            'type': 'arat5',\n            'context_length': 512,\n            'description': 'AraT5 - Arabic T5 model',\n            'arabic_native': True\n        },\n        # Multilingual models with good Arabic support\n        'qwen3': {\n            'name': 'Qwen/Qwen3-14B',\n            'type': 'qwen',\n            'context_length': 32768,\n            'description': 'Qwen3 14B - Latest Qwen model with Arabic support',\n            'arabic_native': False\n        },\n        'qwen2': {\n            'name': 'Qwen/Qwen2-7B-Instruct',\n            'type': 'qwen',\n            'context_length': 32768,\n            'description': 'Qwen2 7B - Strong multilingual model',\n            'arabic_native': False\n        },\n        'llama3': {\n            'name': 'meta-llama/Llama-3.1-8B-Instruct',\n            'type': 'llama',\n            'context_length': 128000,\n            'description': 'Llama 3.1 8B - Meta\\'s latest model',\n            'arabic_native': False\n        },\n        'aya': {\n            'name': 'CohereForAI/aya-23-8B',\n            'type': 'aya',\n            'context_length': 8192,\n            'description': 'Aya 23 8B - Multilingual model with Arabic focus',\n            'arabic_native': False\n        },\n        'mt5': {\n            'name': 'google/mt5-large',\n            'type': 'mt5',\n            'context_length': 512,\n            'description': 'mT5 Large - Multilingual T5 with Arabic',\n            'arabic_native': False\n        }\n    }\n    \n    def __init__(self, model_key: str = 'jais', use_quantization: bool = True, use_thinking_mode: bool = True):\n        \"\"\"\n        Initialize classifier with specified model\n        \n        Args:\n            model_key: Key from MODEL_CONFIGS\n            use_quantization: Whether to use 4-bit quantization\n            use_thinking_mode: Enable thinking mode for supported models\n        \"\"\"\n        if model_key not in self.MODEL_CONFIGS:\n            raise ValueError(f\"Model '{model_key}' not supported. Available models: {list(self.MODEL_CONFIGS.keys())}\")\n        \n        self.model_config = self.MODEL_CONFIGS[model_key]\n        self.model_key = model_key\n        self.use_quantization = use_quantization\n        self.use_thinking_mode = use_thinking_mode and not self.model_config.get('classification_model', False)\n        self.tokenizer = None\n        self.model = None\n        self.is_classification_model = self.model_config.get('classification_model', False)\n        self.answer_strategies = {\n            '1': 'Information (answers providing information, resources, etc.)',\n            '2': 'Direct Guidance (answers providing suggestions, instructions, or advice)',\n            '3': 'Emotional Support (answers providing approval, reassurance, or other forms of emotional support)'\n        }\n        self.load_model()\n    \n    def load_model(self):\n        \"\"\"Load the specified model with appropriate configurations\"\"\"\n        print(f\"Loading {self.model_config['description']}...\")\n        print(f\"Model: {self.model_config['name']}\")\n        print(f\"Arabic Native: {'Yes' if self.model_config.get('arabic_native', False) else 'No'}\")\n        \n        quantization_config = None\n        if self.use_quantization and not self.is_classification_model:\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\"\n            )\n        \n        try:\n            # Load tokenizer\n            tokenizer_kwargs = {\n                'trust_remote_code': True,\n                'padding_side': 'left' if not self.is_classification_model else 'right'\n            }\n            \n            if HF_TOKEN:\n                tokenizer_kwargs['token'] = HF_TOKEN\n                \n            self.tokenizer = AutoTokenizer.from_pretrained(\n                self.model_config['name'],\n                **tokenizer_kwargs\n            )\n            \n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            # Load model\n            model_kwargs = {\n                'trust_remote_code': True,\n                'low_cpu_mem_usage': True,\n                'device_map': 'auto'\n            }\n            \n            if not self.is_classification_model:\n                model_kwargs['torch_dtype'] = torch.float16\n                if quantization_config:\n                    model_kwargs['quantization_config'] = quantization_config\n                \n                self.model = AutoModelForCausalLM.from_pretrained(\n                    self.model_config['name'],\n                    token=HF_TOKEN,\n                    **model_kwargs\n                )\n            else:\n                # For classification models like AraBERT\n                self.model = AutoModelForSequenceClassification.from_pretrained(\n                    self.model_config['name'],\n                    num_labels=3,  # For our 3 strategies\n                    token=HF_TOKEN,\n                    **model_kwargs\n                )\n            \n            print(f\"‚úÖ {self.model_config['description']} loaded successfully!\")\n            self.print_model_info()\n            \n        except Exception as e:\n            print(f\"‚ùå Error loading model: {e}\")\n            raise\n    \n    def print_model_info(self):\n        \"\"\"Print model and memory information\"\"\"\n        print(f\"\\nüìä Model Information:\")\n        print(f\"Model: {self.model_config['name']}\")\n        print(f\"Type: {self.model_config['type']}\")\n        print(f\"Context Length: {self.model_config['context_length']:,} tokens\")\n        print(f\"Arabic Native: {'Yes' if self.model_config.get('arabic_native', False) else 'No'}\")\n        print(f\"Classification Model: {'Yes' if self.is_classification_model else 'No'}\")\n        print(f\"Quantization: {'4-bit' if self.use_quantization and not self.is_classification_model else 'Full precision'}\")\n        print(f\"Thinking Mode: {'Enabled' if self.use_thinking_mode else 'Disabled'}\")\n        print(f\"Tokenizer Vocab Size: {len(self.tokenizer):,}\")\n        \n        if torch.cuda.is_available():\n            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n    \n    def create_arabic_prompt(self, answer: str) -> str:\n        \"\"\"Create Arabic-optimized prompt for classification\"\"\"\n        strategy_descriptions = \"\"\"\nÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿ∑ÿ®Ÿäÿ©:\n(1) ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ - ÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿ™ŸÇÿØŸÖ ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸàŸÖŸàÿßÿ±ÿØ Ÿàÿ≠ŸÇÿßÿ¶ŸÇ ÿ∑ÿ®Ÿäÿ©\n(2) ÿßŸÑÿ™Ÿàÿ¨ŸäŸá ÿßŸÑŸÖÿ®ÿßÿ¥ÿ± - ÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿ™ŸÇÿØŸÖ ÿßŸÇÿ™ÿ±ÿßÿ≠ÿßÿ™ Ÿàÿ™ÿπŸÑŸäŸÖÿßÿ™ ŸàŸÜÿµÿßÿ¶ÿ≠ ŸÖÿ≠ÿØÿØÿ©  \n(3) ÿßŸÑÿØÿπŸÖ ÿßŸÑÿπÿßÿ∑ŸÅŸä - ÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿ™ŸÇÿØŸÖ ÿßŸÑŸÖŸàÿßŸÅŸÇÿ© ŸàÿßŸÑÿ∑ŸÖÿ£ŸÜŸäŸÜÿ© ÿ£Ÿà ÿ£ÿ¥ŸÉÿßŸÑ ÿ£ÿÆÿ±Ÿâ ŸÖŸÜ ÿßŸÑÿØÿπŸÖ ÿßŸÑÿπÿßÿ∑ŸÅŸä\n\"\"\"\n        \n        if self.use_thinking_mode:\n            base_prompt = f\"\"\"ÿ£ŸÜÿ™ ÿÆÿ®Ÿäÿ± ŸÅŸä ÿ™ÿ≠ŸÑŸäŸÑ Ÿàÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿßŸÑÿ∑ÿ®Ÿäÿ© ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©. ŸÖŸáŸÖÿ™ŸÉ ŸáŸä ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÖÿπÿ∑ÿßÿ© ÿ•ŸÑŸâ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ÿ£Ÿà ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ÿßŸÑÿ™ÿßŸÑŸäÿ©:\n\n{strategy_descriptions}\n\nÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÖÿ±ÿßÿØ ÿ™ÿ≠ŸÑŸäŸÑŸáÿß Ÿàÿ™ÿµŸÜŸäŸÅŸáÿß:\n\"{answer}\"\n\nÿ™ÿπŸÑŸäŸÖÿßÿ™ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ:\n1. ÿßŸÇÿ±ÿ£ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿπŸÜÿßŸäÿ© Ÿàÿ≠ŸÑŸÑ ŸÖÿ≠ÿ™ŸàÿßŸáÿß ÿ®ÿßŸÑÿ™ŸÅÿµŸäŸÑ\n2. ÿ≠ÿØÿØ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸàÿßŸÑÿπÿ®ÿßÿ±ÿßÿ™ ÿßŸÑŸÖŸÅÿ™ÿßÿ≠Ÿäÿ© ŸÅŸä ÿßŸÑŸÜÿµ\n3. ÿßÿ±ÿ®ÿ∑ ŸÉŸÑ ÿ¨ÿ≤ÿ° ŸÖŸÜ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ©\n4. ŸÅŸÉÿ± ŸÅŸä ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑÿ∑ÿ®Ÿä ŸàÿßŸÑÿ∫ÿ±ÿ∂ ŸÖŸÜ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©\n5. ŸäŸÖŸÉŸÜ ÿ£ŸÜ ÿ™ŸÜÿ™ŸÖŸä ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸÑÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© Ÿàÿßÿ≠ÿØÿ©\n6. ÿßŸÉÿ™ÿ® ÿ™ÿ≠ŸÑŸäŸÑŸÉ ÿÆÿ∑Ÿàÿ© ÿ®ÿÆÿ∑Ÿàÿ©\n7. ŸÅŸä ÿßŸÑŸÜŸáÿßŸäÿ©ÿå ÿßŸÉÿ™ÿ® ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿ®ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑÿ™ÿßŸÑŸä ÿ®ÿßŸÑÿ∂ÿ®ÿ∑:\n   \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ ŸÖŸÅÿµŸàŸÑÿ© ÿ®ŸÅŸàÿßÿµŸÑ]\"\n\nŸÖÿ´ÿßŸÑ ÿπŸÑŸâ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ:\nÿ•ÿ¨ÿßÿ®ÿ©: \"ÿßŸÑÿµÿØÿßÿπ ŸÇÿØ ŸäŸÉŸàŸÜ ÿ®ÿ≥ÿ®ÿ® ÿßŸÑÿ•ÿ¨ŸáÿßÿØ ÿ£Ÿà ŸÇŸÑÿ© ÿßŸÑŸÜŸàŸÖ. ÿ£ŸÜÿµÿ≠ŸÉ ÿ®ÿ£ÿÆÿ∞ ŸÇÿ≥ÿ∑ ŸÉÿßŸÅ ŸÖŸÜ ÿßŸÑÿ±ÿßÿ≠ÿ© Ÿàÿ¥ÿ±ÿ® ÿßŸÑŸÖÿßÿ°.\"\nÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ: \n- \"ŸÇÿØ ŸäŸÉŸàŸÜ ÿ®ÿ≥ÿ®ÿ®\" = ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ∑ÿ®Ÿäÿ© (ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© 1)\n- \"ÿ£ŸÜÿµÿ≠ŸÉ\" = ÿ™Ÿàÿ¨ŸäŸá ŸÖÿ®ÿßÿ¥ÿ± (ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© 2)\nÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [1,2]\n\"\"\"\n        else:\n            base_prompt = f\"\"\"ÿ£ŸÜÿ™ ÿÆÿ®Ÿäÿ± ŸÅŸä ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿßŸÑÿ∑ÿ®Ÿäÿ© ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©. ÿ≠ŸÑŸÑ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿ™ÿßŸÑŸäÿ© ŸàÿµŸÜŸÅŸáÿß ÿ≠ÿ≥ÿ® ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ÿßŸÑŸÖÿ≠ÿØÿØÿ©:\n\n{strategy_descriptions}\n\nÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\n\"{answer}\"\n\nÿßŸÑŸÖÿ∑ŸÑŸàÿ®:\n1. ÿ≠ŸÑŸÑ ŸÖÿ≠ÿ™ŸàŸâ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© Ÿàÿ≠ÿØÿØ ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ÿ£Ÿà ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ©\n2. ŸäŸÖŸÉŸÜ ÿ£ŸÜ ÿ™ŸÜÿ™ŸÖŸä ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸÑÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ©\n3. ÿßŸÉÿ™ÿ® ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿ®ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑÿ™ÿßŸÑŸä ÿ®ÿßŸÑÿ∂ÿ®ÿ∑:\n   \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ ŸÖŸÅÿµŸàŸÑÿ© ÿ®ŸÅŸàÿßÿµŸÑ]\"\n\nÿ£ŸÖÿ´ŸÑÿ©:\n- ÿ•ÿ¨ÿßÿ®ÿ© ŸÖÿπŸÑŸàŸÖÿßÿ™Ÿäÿ© ŸÅŸÇÿ∑: \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [1]\"\n- ÿ•ÿ¨ÿßÿ®ÿ© ÿ™ÿ≠ÿ™ŸàŸä ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸàŸÜÿµÿßÿ¶ÿ≠: \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [1,2]\"\n- ÿ•ÿ¨ÿßÿ®ÿ© ÿØÿßÿπŸÖÿ© ÿπÿßÿ∑ŸÅŸäÿßŸã: \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [3]\"\n\"\"\"\n        \n        return base_prompt\n    \n    def create_prompt(self, answer: str) -> str:\n        \"\"\"Create model-specific prompt for classification\"\"\"\n        base_prompt = self.create_arabic_prompt(answer)\n        \n        # Model-specific prompt formatting\n        if self.model_config['type'] == 'jais':\n            return f\"### Instruction: {base_prompt}\\n### Response:\"\n        elif self.model_config['type'] in ['llama']:\n            return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{base_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        elif self.model_config['type'] == 'qwen':\n            messages = [{\"role\": \"user\", \"content\": base_prompt}]\n            return self.tokenizer.apply_chat_template(\n                messages, \n                tokenize=False, \n                add_generation_prompt=True,\n                enable_thinking=self.use_thinking_mode\n            )\n        elif self.model_config['type'] == 'aya':\n            return f\"<|USER|>{base_prompt}<|ASSISTANT|>\"\n        elif self.model_config['type'] in ['aragpt', 'arat5', 'mt5']:\n            return base_prompt\n        else:\n            return f\"Human: {base_prompt}\\n\\nAssistant:\"\n    \n    def classify_answer_generative(self, answer: str, max_new_tokens: int = 2000) -> List[str]:\n        \"\"\"\n        Classify using generative models (GPT-style)\n        \"\"\"\n        prompt = self.create_prompt(answer)\n        \n        # Adjust max_length based on model context length\n        max_input_length = min(self.model_config['context_length'] - max_new_tokens, 2048)\n        \n        model_inputs = self.tokenizer(\n            prompt, \n            return_tensors=\"pt\", \n            truncation=True,\n            max_length=max_input_length\n        ).to(self.model.device)\n        \n        # Model-specific generation parameters\n        gen_kwargs = {\n            'max_new_tokens': max_new_tokens,\n            'temperature': 0.3,  # Lower temperature for more consistent results\n            'top_p': 0.9,\n            'top_k': 50,\n            'do_sample': True,\n            'pad_token_id': self.tokenizer.pad_token_id,\n            'eos_token_id': self.tokenizer.eos_token_id,\n            'repetition_penalty': 1.1\n        }\n        \n        # Arabic model optimizations\n        if self.model_config.get('arabic_native', False):\n            gen_kwargs.update({\n                'temperature': 0.2,  # Even lower for Arabic models\n                'top_p': 0.85,\n                'repetition_penalty': 1.05\n            })\n        \n        if self.model_config['type'] == 'jais':\n            gen_kwargs.update({\n                'temperature': 0.1,\n                'top_p': 0.8,\n                'max_new_tokens': min(max_new_tokens, 1000)\n            })\n        elif self.model_config['type'] in ['aragpt', 'arat5']:\n            gen_kwargs.update({\n                'temperature': 0.2,\n                'max_new_tokens': min(max_new_tokens, 512)\n            })\n        \n        with torch.no_grad():\n            generated_ids = self.model.generate(\n                **model_inputs,\n                **gen_kwargs\n            )\n        \n        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n        content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n        \n        strategies = self.extract_answer_strategies(content)\n        return strategies\n    \n    def classify_answer_bert(self, answer: str) -> List[str]:\n        \"\"\"\n        Classify using BERT-style models (classification head)\n        \"\"\"\n        # For BERT models, we need to implement a different approach\n        # This is a simplified version - in practice, you'd need to fine-tune these models\n        inputs = self.tokenizer(\n            answer, \n            return_tensors=\"pt\", \n            truncation=True,\n            padding=True,\n            max_length=self.model_config['context_length']\n        ).to(self.model.device)\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            predictions = torch.sigmoid(outputs.logits)  # Multi-label classification\n            \n        # Convert predictions to strategy labels (threshold = 0.5)\n        strategies = []\n        threshold = 0.5\n        for i, prob in enumerate(predictions[0]):\n            if prob > threshold:\n                strategies.append(str(i + 1))\n        \n        return strategies if strategies else ['1']  # Default to strategy 1\n    \n    def classify_answer(self, answer: str, max_new_tokens: int = 2000) -> List[str]:\n        \"\"\"\n        Classify Arabic medical answer into strategy categories\n        \"\"\"\n        if self.is_classification_model:\n            return self.classify_answer_bert(answer)\n        else:\n            return self.classify_answer_generative(answer, max_new_tokens)\n    \n    def extract_answer_strategies(self, response: str) -> List[str]:\n        \"\"\"Extract answer strategies from Arabic response text\"\"\"\n        # Arabic patterns (primary)\n        arabic_patterns = [\n            r'ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä:\\s*\\[([123,\\s]+)\\]',\n            r'ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™:\\s*\\[([123,\\s]+)\\]',\n            r'ÿßŸÑÿ™ÿµŸÜŸäŸÅ:\\s*\\[([123,\\s]+)\\]',\n            r'ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©:\\s*\\[([123,\\s]+)\\]',\n            r'ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\\s*\\[([123,\\s]+)\\]',\n            r'ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä:\\s*([123,\\s]+)',\n            r'ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ©:\\s*([123,\\s]+)',\n        ]\n        \n        for pattern in arabic_patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                strategies_str = match.group(1)\n                # Remove brackets if present\n                strategies_str = re.sub(r'[\\[\\]]', '', strategies_str)\n                strategies = [strat.strip() for strat in strategies_str.split(',')]\n                valid_strategies = [strat for strat in strategies if strat in ['1', '2', '3']]\n                if valid_strategies:\n                    return valid_strategies\n        \n        # English patterns (fallback)\n        english_patterns = [\n            r'Final Classification:\\s*\\[([123,\\s]+)\\]',\n            r'Strategies:\\s*\\[([123,\\s]+)\\]',\n            r'Classification:\\s*\\[([123,\\s]+)\\]',\n            r'Answer:\\s*\\[([123,\\s]+)\\]',\n        ]\n        \n        for pattern in english_patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                strategies_str = match.group(1)\n                strategies = [strat.strip() for strat in strategies_str.split(',')]\n                valid_strategies = [strat for strat in strategies if strat in ['1', '2', '3']]\n                if valid_strategies:\n                    return valid_strategies\n        \n        # Fallback: look for individual numbers\n        found_strategies = []\n        for strategy in ['1', '2', '3']:\n            if f'({strategy})' in response or f'[{strategy}]' in response or f' {strategy} ' in response:\n                found_strategies.append(strategy)\n        \n        if found_strategies:\n            return found_strategies\n        \n        # Last resort: analyze content for keywords\n        return self.fallback_classification(response)\n    \n    def fallback_classification(self, response: str) -> List[str]:\n        \"\"\"Fallback classification based on Arabic keywords\"\"\"\n        strategies = []\n        \n        # Keywords for each strategy in Arabic\n        info_keywords = ['ŸÖÿπŸÑŸàŸÖÿßÿ™', 'ÿ≠ŸÇÿßÿ¶ŸÇ', 'ÿ®ŸäÿßŸÜÿßÿ™', 'ÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™', 'ÿØÿ±ÿßÿ≥ÿßÿ™', 'ÿ£ÿ®ÿ≠ÿßÿ´', 'ŸäŸèÿπÿ±ŸÅ', 'ŸäŸèÿ≥ŸÖŸâ']\n        guidance_keywords = ['ÿßŸÜÿµÿ≠', 'Ÿäÿ¨ÿ®', 'ŸäŸÜÿ®ÿ∫Ÿä', 'ÿ≠ÿßŸàŸÑ', 'ÿ™ÿ¨ŸÜÿ®', 'ÿßÿ™ÿ®ÿπ', 'ÿßÿ≥ÿ™ÿ¥ÿ±', 'ÿ±ÿßÿ¨ÿπ']\n        support_keywords = ['ŸÑÿß ÿ™ŸÇŸÑŸÇ', 'ÿ∑ÿ®ŸäÿπŸä', 'ÿ¥ÿßÿ¶ÿπ', 'ÿ™ÿ≠ÿ≥ŸÜ', 'ÿ®ÿÆŸäÿ±', 'ŸÖÿ∑ŸÖÿ¶ŸÜ', 'ÿØÿπŸÖ', 'ŸÖÿ≥ÿßŸÜÿØÿ©']\n        \n        response_lower = response.lower()\n        \n        if any(keyword in response_lower for keyword in info_keywords):\n            strategies.append('1')\n        if any(keyword in response_lower for keyword in guidance_keywords):\n            strategies.append('2')\n        if any(keyword in response_lower for keyword in support_keywords):\n            strategies.append('3')\n        \n        return strategies if strategies else ['1']  # Default to information\n    \n    def process_test_dataset(self, df: pd.DataFrame, max_new_tokens: int = 2000, show_progress: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Process dataset for answer classification\n        \"\"\"\n        print(f\"üöÄ Starting Arabic Medical Answer Classification with {self.model_config['description']}...\")\n        print(f\"Dataset size: {len(df)} samples\")\n        print(f\"Arabic Native Model: {'Yes' if self.model_config.get('arabic_native', False) else 'No'}\")\n        print(\"-\" * 80)\n        \n        predictions = []\n        \n        for idx, row in df.iterrows():\n            if show_progress and idx % 10 == 0:\n                print(f\"Processing sample {idx+1}/{len(df)} - Current GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            \n            try:\n                strategies = self.classify_answer(row['answer'], max_new_tokens=max_new_tokens)\n                prediction_str = ', '.join(sorted(strategies))\n                predictions.append(prediction_str)\n                \n                if show_progress and idx < 3:\n                    print(f\"Sample {idx+1} prediction: {prediction_str}\")\n                    \n            except Exception as e:\n                print(f\"Error processing answer {idx}: {e}\")\n                predictions.append('1')\n            \n            if idx % 20 == 0:\n                self.cleanup_memory()\n        \n        result_df = pd.DataFrame({'prediction': predictions})\n        return result_df\n    \n    def cleanup_memory(self):\n        \"\"\"Clean up GPU memory\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    \n    @classmethod\n    def list_available_models(cls):\n        \"\"\"List all available models with descriptions\"\"\"\n        print(\"üìã Available Arabic LLM Models:\")\n        print(\"-\" * 80)\n        \n        # Group by Arabic native vs multilingual\n        arabic_native = []\n        multilingual = []\n        \n        for key, config in cls.MODEL_CONFIGS.items():\n            if config.get('arabic_native', False):\n                arabic_native.append((key, config))\n            else:\n                multilingual.append((key, config))\n        \n        print(\"üá∏üá¶ Arabic Native Models:\")\n        for key, config in arabic_native:\n            print(f\"  Key: '{key}'\")\n            print(f\"    Model: {config['name']}\")\n            print(f\"    Description: {config['description']}\")\n            print(f\"    Context Length: {config['context_length']:,} tokens\")\n            print(f\"    Type: {config['type']}\")\n            print()\n        \n        print(\"üåç Multilingual Models with Arabic Support:\")\n        for key, config in multilingual:\n            print(f\"  Key: '{key}'\")\n            print(f\"    Model: {config['name']}\")\n            print(f\"    Description: {config['description']}\")\n            print(f\"    Context Length: {config['context_length']:,} tokens\")\n            print(f\"    Type: {config['type']}\")\n            print()\n\ndef evaluate_model_on_training_data(\n    train_file_path: str,\n    model_key: str = 'jais',\n    use_quantization: bool = True,\n    max_new_tokens: int = 2000\n) -> dict:\n    \"\"\"\n    Evaluate the model on the training dataset using Weighted F1 Score and Jaccard Score.\n    \"\"\"\n    \n    print(f\"üöÄ Initializing {model_key} for evaluation...\")\n    try:\n        classifier = ArabicMedicalAnswerClassifier(\n            model_key=model_key,\n            use_quantization=use_quantization\n        )\n    except Exception as e:\n        print(f\"‚ùå Failed to initialize model: {e}\")\n        return None\n\n    # Load training dataset\n    try:\n        df = pd.read_csv(train_file_path, sep='\\t')\n        if 'answer' not in df.columns or 'final_AS' not in df.columns:\n            raise ValueError(\"Dataset must contain 'answer' and 'final_AS' columns\")\n        print(f\"‚úÖ Training dataset loaded: {len(df)} samples\")\n    except Exception as e:\n        print(f\"‚ùå Error loading training dataset: {e}\")\n        return None\n\n    # Parse final_AS labels\n    def parse_labels(label):\n        if isinstance(label, str):\n            try:\n                if label.startswith('['):\n                    return ast.literal_eval(label)\n                else:\n                    return [strat.strip() for strat in label.split(',')]\n            except:\n                print(f\"Warning: Could not parse label '{label}', defaulting to ['1']\")\n                return ['1']\n        return label\n\n    df['final_AS'] = df['final_AS'].apply(parse_labels)\n\n    # Generate predictions\n    print(f\"üöÄ Generating predictions for {len(df)} samples...\")\n    predictions = classifier.process_test_dataset(df, max_new_tokens=max_new_tokens)\n\n    # Convert predictions and true labels to multi-label binary format\n    all_strategies = ['1', '2', '3']\n    y_true = []\n    y_pred = []\n\n    for true_labels, pred_labels in zip(df['final_AS'], predictions['prediction']):\n        true_vec = [1 if strat in true_labels else 0 for strat in all_strategies]\n        y_true.append(true_vec)\n        pred_strats = [strat.strip() for strat in pred_labels.split(',') if strat.strip() in all_strategies]\n        pred_vec = [1 if strat in pred_strats else 0 for strat in all_strategies]\n        y_pred.append(pred_vec)\n\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate metrics\n    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n    jaccard = jaccard_score(y_true, y_pred, average='samples')\n\n    # Print results\n    print(\"\\nüìä Evaluation Results:\")\n    print(f\"Model: {classifier.model_config['description']}\")\n    print(f\"Arabic Native: {'Yes' if classifier.model_config.get('arabic_native', False) else 'No'}\")\n    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n    print(f\"Jaccard Score (samples): {jaccard:.4f}\")\n\n    # Detailed per-strategy metrics\n    print(\"\\nüìã Per-Strategy Metrics:\")\n    for i, strat in enumerate(all_strategies):\n        strat_f1 = f1_score(y_true[:, i], y_pred[:, i])\n        strat_jaccard = jaccard_score(y_true[:, i], y_pred[:, i])\n        print(f\"Strategy {strat} ({classifier.answer_strategies[strat]}):\")\n        print(f\"  F1 Score: {strat_f1:.4f}\")\n        print(f\"  Jaccard Score: {strat_jaccard:.4f}\")\n\n    # Cleanup memory\n    classifier.cleanup_memory()\n\n    return {\n        'model_name': classifier.model_config['name'],\n        'arabic_native': classifier.model_config.get('arabic_native', False),\n        'weighted_f1': weighted_f1,\n        'jaccard_score': jaccard,\n        'per_strategy_f1': {strat: f1_score(y_true[:, i], y_pred[:, i]) for i, strat in enumerate(all_strategies)},\n        'per_strategy_jaccard': {strat: jaccard_score(y_true[:, i], y_pred[:, i]) for i, strat in enumerate(all_strategies)}\n    }\n\ndef compare_arabic_models(train_file_path: str, models_to_test: List[str] = None):\n    \"\"\"Compare multiple Arabic models\"\"\"\n    if models_to_test is None:\n        models_to_test = ['jais', 'arabert', 'aragpt2', 'aya']  # Start with a few key models\n    \n    results = {}\n    \n    for model_key in models_to_test:\n        print(f\"\\n{'='*60}\")\n        print(f\"Testing Model: {model_key}\")\n        print(f\"{'='*60}\")\n        \n        try:\n            result = evaluate_model_on_training_data(\n                train_file_path=train_file_path,\n                model_key=model_key,\n                use_quantization=True,\n                max_new_tokens=1500\n            )\n            \n            if result:\n                results[model_key] = result\n                print(f\"‚úÖ {model_key} completed successfully\")\n            else:\n                print(f\"‚ùå {model_key} failed\")\n                \n        except Exception as e:\n            print(f\"‚ùå Error testing {model_key}: {e}\")\n            results[model_key] = None\n        \n        # Clean up memory between models\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    \n    # Summary comparison\n    print(f\"\\n{'='*80}\")\n    print(\"üìä FINAL COMPARISON RESULTS\")\n    print(f\"{'='*80}\")\n    \n    valid_results = {k: v for k, v in results.items() if v is not None}\n    \n    if valid_results:\n        # Sort by weighted F1 score\n        sorted_results = sorted(valid_results.items(), key=lambda x: x[1]['weighted_f1'], reverse=True)\n        \n        print(f\"{'Rank':<5} {'Model':<15} {'Arabic Native':<15} {'Weighted F1':<12} {'Jaccard':<10}\")\n        print(\"-\" * 70)\n        \n        for rank, (model_key, result) in enumerate(sorted_results, 1):\n            arabic_native = \"Yes\" if result['arabic_native'] else \"No\"\n            print(f\"{rank:<5} {model_key:<15} {arabic_native:<15} {result['weighted_f1']:<12.4f} {result['jaccard_score']:<10.4f}\")\n        \n        # Best model details\n        best_model, best_result = sorted_results[0]\n        print(f\"\\nüèÜ Best Performing Model: {best_model}\")\n        print(f\"Model Name: {best_result['model_name']}\")\n        print(f\"Arabic Native: {'Yes' if best_result['arabic_native'] else 'No'}\")\n        print(f\"Weighted F1 Score: {best_result['weighted_f1']:.4f}\")\n        print(f\"Jaccard Score: {best_result['jaccard_score']:.4f}\")\n    \n    return results\n\nif __name__ == \"__main__\":\n    # Update this path to your training dataset\n    TRAIN_DATASET_PATH = '/kaggle/input/train-df/Train_Dev.tsv'  # Replace with actual path\n\n    # List available models\n    ArabicMedicalAnswerClassifier.list_available_models()\n\n    # Test individual model (Arabic native)\n    print(\"\\n\" + \"=\"*80)\n    print(\"Testing Arabic Native Model: Jais\")\n    print(\"=\"*80)\n    \n    jais_results = evaluate_model_on_training_data(\n        train_file_path=TRAIN_DATASET_PATH,\n        model_key='jais',\n        use_quantization=True,\n        max_new_tokens=1500\n    )\n\n    if jais_results:\n        print(f\"\\nüìà Jais Results Summary:\")\n        print(f\"Weighted F1 Score: {jais_results['weighted_f1']:.4f}\")\n        print(f\"Jaccard Score: {jais_results['jaccard_score']:.4f}\")\n\n    # Compare multiple models\n    print(\"\\n\" + \"=\"*80)\n    print(\"Comparing Multiple Arabic Models\")\n    print(\"=\"*80)\n    \n    # Test both Arabic native and multilingual models\n    models_to_compare = [\n        'jais',      # Arabic native\n        'arabert',     # Multilingual with good Arabic\n        'aragpt2',    # Multilingual\n        'aya'        # Arabic-focused multilingual\n    ]\n    \n    comparison_results = compare_arabic_models(\n        train_file_path=TRAIN_DATASET_PATH,\n        models_to_test=models_to_compare\n    )\n    \n    # Additional analysis for Arabic models\n    print(\"\\n\" + \"=\"*80)\n    print(\"üìä ARABIC MODEL ANALYSIS\")\n    print(\"=\"*80)\n    \n    arabic_models = []\n    multilingual_models = []\n    \n    for model_key, result in comparison_results.items():\n        if result is not None:\n            if result['arabic_native']:\n                arabic_models.append((model_key, result))\n            else:\n                multilingual_models.append((model_key, result))\n    \n    if arabic_models:\n        print(\"\\nüá∏üá¶ Arabic Native Models Performance:\")\n        for model_key, result in arabic_models:\n            print(f\"{model_key}: F1={result['weighted_f1']:.4f}, Jaccard={result['jaccard_score']:.4f}\")\n    \n    if multilingual_models:\n        print(\"\\nüåç Multilingual Models Performance:\")\n        for model_key, result in multilingual_models:\n            print(f\"{model_key}: F1={result['weighted_f1']:.4f}, Jaccard={result['jaccard_score']:.4f}\")\n    \n    # Recommendations\n    print(\"\\n\" + \"=\"*80)\n    print(\"üí° RECOMMENDATIONS\")\n    print(\"=\"*80)\n    \n    if comparison_results:\n        valid_results = {k: v for k, v in comparison_results.items() if v is not None}\n        if valid_results:\n            best_model = max(valid_results.items(), key=lambda x: x[1]['weighted_f1'])\n            \n            print(f\"üèÜ Best Overall Model: {best_model[0]}\")\n            print(f\"   - Model: {best_model[1]['model_name']}\")\n            print(f\"   - Arabic Native: {'Yes' if best_model[1]['arabic_native'] else 'No'}\")\n            print(f\"   - Performance: F1={best_model[1]['weighted_f1']:.4f}\")\n            \n            if best_model[1]['arabic_native']:\n                print(f\"   - ‚úÖ Recommended for Arabic medical texts due to native Arabic support\")\n            else:\n                print(f\"   - ‚ö†Ô∏è  Multilingual model - consider Arabic native alternatives if available\")\n            \n            print(f\"\\nüìã Usage Example:\")\n            print(f\"classifier = ArabicMedicalAnswerClassifier(model_key='{best_model[0]}')\")\n            print(f\"strategies = classifier.classify_answer('your_arabic_medical_answer_here')\")\n\n# Additional utility functions for Arabic text processing\nclass ArabicTextPreprocessor:\n    \"\"\"Utility class for Arabic text preprocessing\"\"\"\n    \n    @staticmethod\n    def normalize_arabic(text: str) -> str:\n        \"\"\"Normalize Arabic text\"\"\"\n        import re\n        \n        # Remove diacritics\n        text = re.sub(r'[\\u064B-\\u065F\\u0670\\u0640]', '', text)\n        \n        # Normalize different forms of alef\n        text = re.sub(r'[ÿ•ÿ£ÿ¢ÿß]', 'ÿß', text)\n        \n        # Normalize teh marbuta\n        text = re.sub(r'ÿ©', 'Ÿá', text)\n        \n        # Normalize different forms of yeh\n        text = re.sub(r'[ŸäŸâ]', 'Ÿä', text)\n        \n        return text.strip()\n    \n    @staticmethod\n    def is_arabic_text(text: str) -> bool:\n        \"\"\"Check if text is primarily Arabic\"\"\"\n        arabic_chars = len(re.findall(r'[\\u0600-\\u06FF]', text))\n        total_chars = len(re.findall(r'[^\\s\\d\\W]', text))\n        return arabic_chars / max(total_chars, 1) > 0.5\n\ndef create_arabic_optimized_classifier(model_key: str = 'jais') -> ArabicMedicalAnswerClassifier:\n    \"\"\"Create an Arabic-optimized classifier with best practices\"\"\"\n    \n    print(f\"üöÄ Creating Arabic-optimized classifier...\")\n    \n    # Recommended settings for Arabic models\n    arabic_native_models = ['jais', 'aragpt2', 'arabert', 'camelbert', 'marbert', 'arat5']\n    \n    if model_key in arabic_native_models:\n        print(f\"‚úÖ Using Arabic native model: {model_key}\")\n        use_quantization = True  # Arabic models tend to be smaller, quantization helps with memory\n    else:\n        print(f\"‚ö†Ô∏è  Using multilingual model: {model_key}\")\n        use_quantization = True  # Still recommended for memory efficiency\n    \n    classifier = ArabicMedicalAnswerClassifier(\n        model_key=model_key,\n        use_quantization=use_quantization,\n        use_thinking_mode=True  # Helps with Arabic reasoning\n    )\n    \n    return classifier","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}