{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12507227,"sourceType":"datasetVersion","datasetId":7894059},{"sourceId":12592865,"sourceType":"datasetVersion","datasetId":7953705}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers>=4.51.0 torch torchvision torchaudio accelerate bitsandbytes -q\n!pip install sentencepiece protobuf -q\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport gc\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\n# Check GPU availability\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nimport re\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSequenceClassification\nfrom typing import List, Dict, Tuple, Any, Optional\nfrom sklearn.metrics import f1_score, jaccard_score\nimport ast\nimport warnings\nfrom huggingface_hub import login\nwarnings.filterwarnings('ignore')\n\nHF_TOKEN = \"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\"\nlogin(HF_TOKEN)\n\nclass ArabicMedicalAnswerClassifier:\n    \"\"\"\n    Arabic Medical Answer Classifier supporting Arabic-optimized LLM models\n    \"\"\"\n    \n    # Model configurations with Arabic-specific models\n    MODEL_CONFIGS = {\n        # Arabic-specific models\n        'jais': {\n            'name': 'inception-mbzuai/jais-13b-chat',\n            'type': 'jais',\n            'context_length': 4096,\n            'description': 'Jais 13B Chat - Arabic-English bilingual model',\n            'arabic_native': True\n        },\n        'aragpt2': {\n            'name': 'aubmindlab/aragpt2-mega',\n            'type': 'aragpt',\n            'context_length': 1024,\n            'description': 'AraGPT2 Mega - Arabic GPT model',\n            'arabic_native': True\n        },\n        'arabert': {\n            'name': 'aubmindlab/bert-base-arabertv2',\n            'type': 'arabert',\n            'context_length': 512,\n            'description': 'AraBERT v2 - Arabic BERT for classification',\n            'arabic_native': True,\n            'classification_model': True\n        },\n        'camelbert': {\n            'name': 'CAMeL-Lab/bert-base-arabic-camelbert-mix',\n            'type': 'camelbert',\n            'context_length': 512,\n            'description': 'CAMeLBERT - NYU Arabic BERT model',\n            'arabic_native': True,\n            'classification_model': True\n        },\n        'marbert': {\n            'name': 'UBC-NLP/MARBERT',\n            'type': 'marbert',\n            'context_length': 512,\n            'description': 'MARBERT - Multilingual Arabic BERT',\n            'arabic_native': True,\n            'classification_model': True\n        },\n        'arat5': {\n            'name': 'UBC-NLP/AraT5-base',\n            'type': 'arat5',\n            'context_length': 512,\n            'description': 'AraT5 - Arabic T5 model',\n            'arabic_native': True\n        },\n        # Multilingual models with good Arabic support\n        'qwen3': {\n            'name': 'Qwen/Qwen3-14B',\n            'type': 'qwen',\n            'context_length': 32768,\n            'description': 'Qwen3 14B - Latest Qwen model with Arabic support',\n            'arabic_native': False\n        },\n        'qwen2': {\n            'name': 'Qwen/Qwen2-7B-Instruct',\n            'type': 'qwen',\n            'context_length': 32768,\n            'description': 'Qwen2 7B - Strong multilingual model',\n            'arabic_native': False\n        },\n        'llama3': {\n            'name': 'meta-llama/Llama-3.1-8B-Instruct',\n            'type': 'llama',\n            'context_length': 128000,\n            'description': 'Llama 3.1 8B - Meta\\'s latest model',\n            'arabic_native': False\n        },\n        'aya': {\n            'name': 'CohereForAI/aya-23-8B',\n            'type': 'aya',\n            'context_length': 8192,\n            'description': 'Aya 23 8B - Multilingual model with Arabic focus',\n            'arabic_native': False\n        },\n        'mt5': {\n            'name': 'google/mt5-large',\n            'type': 'mt5',\n            'context_length': 512,\n            'description': 'mT5 Large - Multilingual T5 with Arabic',\n            'arabic_native': False\n        }\n    }\n    \n    def __init__(self, model_key: str = 'jais', use_quantization: bool = True, use_thinking_mode: bool = True):\n        \"\"\"\n        Initialize classifier with specified model\n        \n        Args:\n            model_key: Key from MODEL_CONFIGS\n            use_quantization: Whether to use 4-bit quantization\n            use_thinking_mode: Enable thinking mode for supported models\n        \"\"\"\n        if model_key not in self.MODEL_CONFIGS:\n            raise ValueError(f\"Model '{model_key}' not supported. Available models: {list(self.MODEL_CONFIGS.keys())}\")\n        \n        self.model_config = self.MODEL_CONFIGS[model_key]\n        self.model_key = model_key\n        self.use_quantization = use_quantization\n        self.use_thinking_mode = use_thinking_mode and not self.model_config.get('classification_model', False)\n        self.tokenizer = None\n        self.model = None\n        self.is_classification_model = self.model_config.get('classification_model', False)\n        self.answer_strategies = {\n            '1': 'Information (answers providing information, resources, etc.)',\n            '2': 'Direct Guidance (answers providing suggestions, instructions, or advice)',\n            '3': 'Emotional Support (answers providing approval, reassurance, or other forms of emotional support)'\n        }\n        self.load_model()\n    \n    def load_model(self):\n        \"\"\"Load the specified model with appropriate configurations\"\"\"\n        print(f\"Loading {self.model_config['description']}...\")\n        print(f\"Model: {self.model_config['name']}\")\n        print(f\"Arabic Native: {'Yes' if self.model_config.get('arabic_native', False) else 'No'}\")\n        \n        quantization_config = None\n        if self.use_quantization and not self.is_classification_model:\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\"\n            )\n        \n        try:\n            # Load tokenizer\n            tokenizer_kwargs = {\n                'trust_remote_code': True,\n                'padding_side': 'left' if not self.is_classification_model else 'right'\n            }\n            \n            if HF_TOKEN:\n                tokenizer_kwargs['token'] = HF_TOKEN\n                \n            self.tokenizer = AutoTokenizer.from_pretrained(\n                self.model_config['name'],\n                **tokenizer_kwargs\n            )\n            \n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            # Load model\n            model_kwargs = {\n                'trust_remote_code': True,\n                'low_cpu_mem_usage': True,\n                'device_map': 'auto'\n            }\n            \n            if not self.is_classification_model:\n                model_kwargs['torch_dtype'] = torch.float16\n                if quantization_config:\n                    model_kwargs['quantization_config'] = quantization_config\n                \n                self.model = AutoModelForCausalLM.from_pretrained(\n                    self.model_config['name'],\n                    token=HF_TOKEN,\n                    **model_kwargs\n                )\n            else:\n                # For classification models like AraBERT\n                self.model = AutoModelForSequenceClassification.from_pretrained(\n                    self.model_config['name'],\n                    num_labels=3,  # For our 3 strategies\n                    token=HF_TOKEN,\n                    **model_kwargs\n                )\n            \n            print(f\"✅ {self.model_config['description']} loaded successfully!\")\n            self.print_model_info()\n            \n        except Exception as e:\n            print(f\"❌ Error loading model: {e}\")\n            raise\n    \n    def print_model_info(self):\n        \"\"\"Print model and memory information\"\"\"\n        print(f\"\\n📊 Model Information:\")\n        print(f\"Model: {self.model_config['name']}\")\n        print(f\"Type: {self.model_config['type']}\")\n        print(f\"Context Length: {self.model_config['context_length']:,} tokens\")\n        print(f\"Arabic Native: {'Yes' if self.model_config.get('arabic_native', False) else 'No'}\")\n        print(f\"Classification Model: {'Yes' if self.is_classification_model else 'No'}\")\n        print(f\"Quantization: {'4-bit' if self.use_quantization and not self.is_classification_model else 'Full precision'}\")\n        print(f\"Thinking Mode: {'Enabled' if self.use_thinking_mode else 'Disabled'}\")\n        print(f\"Tokenizer Vocab Size: {len(self.tokenizer):,}\")\n        \n        if torch.cuda.is_available():\n            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n    \n    def create_arabic_prompt(self, answer: str) -> str:\n        \"\"\"Create Arabic-optimized prompt for classification\"\"\"\n        strategy_descriptions = \"\"\"\nاستراتيجيات الإجابة الطبية:\n(1) المعلومات - إجابات تقدم معلومات وموارد وحقائق طبية\n(2) التوجيه المباشر - إجابات تقدم اقتراحات وتعليمات ونصائح محددة  \n(3) الدعم العاطفي - إجابات تقدم الموافقة والطمأنينة أو أشكال أخرى من الدعم العاطفي\n\"\"\"\n        \n        if self.use_thinking_mode:\n            base_prompt = f\"\"\"أنت خبير في تحليل وتصنيف الإجابات الطبية باللغة العربية. مهمتك هي تصنيف الإجابة المعطاة إلى استراتيجية أو أكثر من الاستراتيجيات التالية:\n\n{strategy_descriptions}\n\nالإجابة المراد تحليلها وتصنيفها:\n\"{answer}\"\n\nتعليمات التحليل:\n1. اقرأ الإجابة بعناية وحلل محتواها بالتفصيل\n2. حدد الكلمات والعبارات المفتاحية في النص\n3. اربط كل جزء من الإجابة بالاستراتيجية المناسبة\n4. فكر في السياق الطبي والغرض من الإجابة\n5. يمكن أن تنتمي الإجابة لأكثر من استراتيجية واحدة\n6. اكتب تحليلك خطوة بخطوة\n7. في النهاية، اكتب التصنيف بالتنسيق التالي بالضبط:\n   \"التصنيف النهائي: [الأرقام مفصولة بفواصل]\"\n\nمثال على التحليل:\nإجابة: \"الصداع قد يكون بسبب الإجهاد أو قلة النوم. أنصحك بأخذ قسط كاف من الراحة وشرب الماء.\"\nالتحليل: \n- \"قد يكون بسبب\" = معلومات طبية (استراتيجية 1)\n- \"أنصحك\" = توجيه مباشر (استراتيجية 2)\nالتصنيف النهائي: [1,2]\n\"\"\"\n        else:\n            base_prompt = f\"\"\"أنت خبير في تصنيف الإجابات الطبية باللغة العربية. حلل الإجابة التالية وصنفها حسب الاستراتيجيات المحددة:\n\n{strategy_descriptions}\n\nالإجابة:\n\"{answer}\"\n\nالمطلوب:\n1. حلل محتوى الإجابة وحدد الاستراتيجية أو الاستراتيجيات المناسبة\n2. يمكن أن تنتمي الإجابة لأكثر من استراتيجية\n3. اكتب التصنيف بالتنسيق التالي بالضبط:\n   \"التصنيف النهائي: [الأرقام مفصولة بفواصل]\"\n\nأمثلة:\n- إجابة معلوماتية فقط: \"التصنيف النهائي: [1]\"\n- إجابة تحتوي معلومات ونصائح: \"التصنيف النهائي: [1,2]\"\n- إجابة داعمة عاطفياً: \"التصنيف النهائي: [3]\"\n\"\"\"\n        \n        return base_prompt\n    \n    def create_prompt(self, answer: str) -> str:\n        \"\"\"Create model-specific prompt for classification\"\"\"\n        base_prompt = self.create_arabic_prompt(answer)\n        \n        # Model-specific prompt formatting\n        if self.model_config['type'] == 'jais':\n            return f\"### Instruction: {base_prompt}\\n### Response:\"\n        elif self.model_config['type'] in ['llama']:\n            return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{base_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        elif self.model_config['type'] == 'qwen':\n            messages = [{\"role\": \"user\", \"content\": base_prompt}]\n            return self.tokenizer.apply_chat_template(\n                messages, \n                tokenize=False, \n                add_generation_prompt=True,\n                enable_thinking=self.use_thinking_mode\n            )\n        elif self.model_config['type'] == 'aya':\n            return f\"<|USER|>{base_prompt}<|ASSISTANT|>\"\n        elif self.model_config['type'] in ['aragpt', 'arat5', 'mt5']:\n            return base_prompt\n        else:\n            return f\"Human: {base_prompt}\\n\\nAssistant:\"\n    \n    def classify_answer_generative(self, answer: str, max_new_tokens: int = 2000) -> List[str]:\n        \"\"\"\n        Classify using generative models (GPT-style)\n        \"\"\"\n        prompt = self.create_prompt(answer)\n        \n        # Adjust max_length based on model context length\n        max_input_length = min(self.model_config['context_length'] - max_new_tokens, 2048)\n        \n        model_inputs = self.tokenizer(\n            prompt, \n            return_tensors=\"pt\", \n            truncation=True,\n            max_length=max_input_length\n        ).to(self.model.device)\n        \n        # Model-specific generation parameters\n        gen_kwargs = {\n            'max_new_tokens': max_new_tokens,\n            'temperature': 0.3,  # Lower temperature for more consistent results\n            'top_p': 0.9,\n            'top_k': 50,\n            'do_sample': True,\n            'pad_token_id': self.tokenizer.pad_token_id,\n            'eos_token_id': self.tokenizer.eos_token_id,\n            'repetition_penalty': 1.1\n        }\n        \n        # Arabic model optimizations\n        if self.model_config.get('arabic_native', False):\n            gen_kwargs.update({\n                'temperature': 0.2,  # Even lower for Arabic models\n                'top_p': 0.85,\n                'repetition_penalty': 1.05\n            })\n        \n        if self.model_config['type'] == 'jais':\n            gen_kwargs.update({\n                'temperature': 0.1,\n                'top_p': 0.8,\n                'max_new_tokens': min(max_new_tokens, 1000)\n            })\n        elif self.model_config['type'] in ['aragpt', 'arat5']:\n            gen_kwargs.update({\n                'temperature': 0.2,\n                'max_new_tokens': min(max_new_tokens, 512)\n            })\n        \n        with torch.no_grad():\n            generated_ids = self.model.generate(\n                **model_inputs,\n                **gen_kwargs\n            )\n        \n        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n        content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n        \n        strategies = self.extract_answer_strategies(content)\n        return strategies\n    \n    def classify_answer_bert(self, answer: str) -> List[str]:\n        \"\"\"\n        Classify using BERT-style models (classification head)\n        \"\"\"\n        # For BERT models, we need to implement a different approach\n        # This is a simplified version - in practice, you'd need to fine-tune these models\n        inputs = self.tokenizer(\n            answer, \n            return_tensors=\"pt\", \n            truncation=True,\n            padding=True,\n            max_length=self.model_config['context_length']\n        ).to(self.model.device)\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            predictions = torch.sigmoid(outputs.logits)  # Multi-label classification\n            \n        # Convert predictions to strategy labels (threshold = 0.5)\n        strategies = []\n        threshold = 0.5\n        for i, prob in enumerate(predictions[0]):\n            if prob > threshold:\n                strategies.append(str(i + 1))\n        \n        return strategies if strategies else ['1']  # Default to strategy 1\n    \n    def classify_answer(self, answer: str, max_new_tokens: int = 2000) -> List[str]:\n        \"\"\"\n        Classify Arabic medical answer into strategy categories\n        \"\"\"\n        if self.is_classification_model:\n            return self.classify_answer_bert(answer)\n        else:\n            return self.classify_answer_generative(answer, max_new_tokens)\n    \n    def extract_answer_strategies(self, response: str) -> List[str]:\n        \"\"\"Extract answer strategies from Arabic response text\"\"\"\n        # Arabic patterns (primary)\n        arabic_patterns = [\n            r'التصنيف النهائي:\\s*\\[([123,\\s]+)\\]',\n            r'الاستراتيجيات:\\s*\\[([123,\\s]+)\\]',\n            r'التصنيف:\\s*\\[([123,\\s]+)\\]',\n            r'النتيجة:\\s*\\[([123,\\s]+)\\]',\n            r'الإجابة:\\s*\\[([123,\\s]+)\\]',\n            r'التصنيف النهائي:\\s*([123,\\s]+)',\n            r'الاستراتيجية:\\s*([123,\\s]+)',\n        ]\n        \n        for pattern in arabic_patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                strategies_str = match.group(1)\n                # Remove brackets if present\n                strategies_str = re.sub(r'[\\[\\]]', '', strategies_str)\n                strategies = [strat.strip() for strat in strategies_str.split(',')]\n                valid_strategies = [strat for strat in strategies if strat in ['1', '2', '3']]\n                if valid_strategies:\n                    return valid_strategies\n        \n        # English patterns (fallback)\n        english_patterns = [\n            r'Final Classification:\\s*\\[([123,\\s]+)\\]',\n            r'Strategies:\\s*\\[([123,\\s]+)\\]',\n            r'Classification:\\s*\\[([123,\\s]+)\\]',\n            r'Answer:\\s*\\[([123,\\s]+)\\]',\n        ]\n        \n        for pattern in english_patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                strategies_str = match.group(1)\n                strategies = [strat.strip() for strat in strategies_str.split(',')]\n                valid_strategies = [strat for strat in strategies if strat in ['1', '2', '3']]\n                if valid_strategies:\n                    return valid_strategies\n        \n        # Fallback: look for individual numbers\n        found_strategies = []\n        for strategy in ['1', '2', '3']:\n            if f'({strategy})' in response or f'[{strategy}]' in response or f' {strategy} ' in response:\n                found_strategies.append(strategy)\n        \n        if found_strategies:\n            return found_strategies\n        \n        # Last resort: analyze content for keywords\n        return self.fallback_classification(response)\n    \n    def fallback_classification(self, response: str) -> List[str]:\n        \"\"\"Fallback classification based on Arabic keywords\"\"\"\n        strategies = []\n        \n        # Keywords for each strategy in Arabic\n        info_keywords = ['معلومات', 'حقائق', 'بيانات', 'إحصائيات', 'دراسات', 'أبحاث', 'يُعرف', 'يُسمى']\n        guidance_keywords = ['انصح', 'يجب', 'ينبغي', 'حاول', 'تجنب', 'اتبع', 'استشر', 'راجع']\n        support_keywords = ['لا تقلق', 'طبيعي', 'شائع', 'تحسن', 'بخير', 'مطمئن', 'دعم', 'مساندة']\n        \n        response_lower = response.lower()\n        \n        if any(keyword in response_lower for keyword in info_keywords):\n            strategies.append('1')\n        if any(keyword in response_lower for keyword in guidance_keywords):\n            strategies.append('2')\n        if any(keyword in response_lower for keyword in support_keywords):\n            strategies.append('3')\n        \n        return strategies if strategies else ['1']  # Default to information\n    \n    def process_test_dataset(self, df: pd.DataFrame, max_new_tokens: int = 2000, show_progress: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Process dataset for answer classification\n        \"\"\"\n        print(f\"🚀 Starting Arabic Medical Answer Classification with {self.model_config['description']}...\")\n        print(f\"Dataset size: {len(df)} samples\")\n        print(f\"Arabic Native Model: {'Yes' if self.model_config.get('arabic_native', False) else 'No'}\")\n        print(\"-\" * 80)\n        \n        predictions = []\n        \n        for idx, row in df.iterrows():\n            if show_progress and idx % 10 == 0:\n                print(f\"Processing sample {idx+1}/{len(df)} - Current GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            \n            try:\n                strategies = self.classify_answer(row['answer'], max_new_tokens=max_new_tokens)\n                prediction_str = ', '.join(sorted(strategies))\n                predictions.append(prediction_str)\n                \n                if show_progress and idx < 3:\n                    print(f\"Sample {idx+1} prediction: {prediction_str}\")\n                    \n            except Exception as e:\n                print(f\"Error processing answer {idx}: {e}\")\n                predictions.append('1')\n            \n            if idx % 20 == 0:\n                self.cleanup_memory()\n        \n        result_df = pd.DataFrame({'prediction': predictions})\n        return result_df\n    \n    def cleanup_memory(self):\n        \"\"\"Clean up GPU memory\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    \n    @classmethod\n    def list_available_models(cls):\n        \"\"\"List all available models with descriptions\"\"\"\n        print(\"📋 Available Arabic LLM Models:\")\n        print(\"-\" * 80)\n        \n        # Group by Arabic native vs multilingual\n        arabic_native = []\n        multilingual = []\n        \n        for key, config in cls.MODEL_CONFIGS.items():\n            if config.get('arabic_native', False):\n                arabic_native.append((key, config))\n            else:\n                multilingual.append((key, config))\n        \n        print(\"🇸🇦 Arabic Native Models:\")\n        for key, config in arabic_native:\n            print(f\"  Key: '{key}'\")\n            print(f\"    Model: {config['name']}\")\n            print(f\"    Description: {config['description']}\")\n            print(f\"    Context Length: {config['context_length']:,} tokens\")\n            print(f\"    Type: {config['type']}\")\n            print()\n        \n        print(\"🌍 Multilingual Models with Arabic Support:\")\n        for key, config in multilingual:\n            print(f\"  Key: '{key}'\")\n            print(f\"    Model: {config['name']}\")\n            print(f\"    Description: {config['description']}\")\n            print(f\"    Context Length: {config['context_length']:,} tokens\")\n            print(f\"    Type: {config['type']}\")\n            print()\n\ndef evaluate_model_on_training_data(\n    train_file_path: str,\n    model_key: str = 'jais',\n    use_quantization: bool = True,\n    max_new_tokens: int = 2000\n) -> dict:\n    \"\"\"\n    Evaluate the model on the training dataset using Weighted F1 Score and Jaccard Score.\n    \"\"\"\n    \n    print(f\"🚀 Initializing {model_key} for evaluation...\")\n    try:\n        classifier = ArabicMedicalAnswerClassifier(\n            model_key=model_key,\n            use_quantization=use_quantization\n        )\n    except Exception as e:\n        print(f\"❌ Failed to initialize model: {e}\")\n        return None\n\n    # Load training dataset\n    try:\n        df = pd.read_csv(train_file_path, sep='\\t')\n        if 'answer' not in df.columns or 'final_AS' not in df.columns:\n            raise ValueError(\"Dataset must contain 'answer' and 'final_AS' columns\")\n        print(f\"✅ Training dataset loaded: {len(df)} samples\")\n    except Exception as e:\n        print(f\"❌ Error loading training dataset: {e}\")\n        return None\n\n    # Parse final_AS labels\n    def parse_labels(label):\n        if isinstance(label, str):\n            try:\n                if label.startswith('['):\n                    return ast.literal_eval(label)\n                else:\n                    return [strat.strip() for strat in label.split(',')]\n            except:\n                print(f\"Warning: Could not parse label '{label}', defaulting to ['1']\")\n                return ['1']\n        return label\n\n    df['final_AS'] = df['final_AS'].apply(parse_labels)\n\n    # Generate predictions\n    print(f\"🚀 Generating predictions for {len(df)} samples...\")\n    predictions = classifier.process_test_dataset(df, max_new_tokens=max_new_tokens)\n\n    # Convert predictions and true labels to multi-label binary format\n    all_strategies = ['1', '2', '3']\n    y_true = []\n    y_pred = []\n\n    for true_labels, pred_labels in zip(df['final_AS'], predictions['prediction']):\n        true_vec = [1 if strat in true_labels else 0 for strat in all_strategies]\n        y_true.append(true_vec)\n        pred_strats = [strat.strip() for strat in pred_labels.split(',') if strat.strip() in all_strategies]\n        pred_vec = [1 if strat in pred_strats else 0 for strat in all_strategies]\n        y_pred.append(pred_vec)\n\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate metrics\n    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n    jaccard = jaccard_score(y_true, y_pred, average='samples')\n\n    # Print results\n    print(\"\\n📊 Evaluation Results:\")\n    print(f\"Model: {classifier.model_config['description']}\")\n    print(f\"Arabic Native: {'Yes' if classifier.model_config.get('arabic_native', False) else 'No'}\")\n    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n    print(f\"Jaccard Score (samples): {jaccard:.4f}\")\n\n    # Detailed per-strategy metrics\n    print(\"\\n📋 Per-Strategy Metrics:\")\n    for i, strat in enumerate(all_strategies):\n        strat_f1 = f1_score(y_true[:, i], y_pred[:, i])\n        strat_jaccard = jaccard_score(y_true[:, i], y_pred[:, i])\n        print(f\"Strategy {strat} ({classifier.answer_strategies[strat]}):\")\n        print(f\"  F1 Score: {strat_f1:.4f}\")\n        print(f\"  Jaccard Score: {strat_jaccard:.4f}\")\n\n    # Cleanup memory\n    classifier.cleanup_memory()\n\n    return {\n        'model_name': classifier.model_config['name'],\n        'arabic_native': classifier.model_config.get('arabic_native', False),\n        'weighted_f1': weighted_f1,\n        'jaccard_score': jaccard,\n        'per_strategy_f1': {strat: f1_score(y_true[:, i], y_pred[:, i]) for i, strat in enumerate(all_strategies)},\n        'per_strategy_jaccard': {strat: jaccard_score(y_true[:, i], y_pred[:, i]) for i, strat in enumerate(all_strategies)}\n    }\n\ndef compare_arabic_models(train_file_path: str, models_to_test: List[str] = None):\n    \"\"\"Compare multiple Arabic models\"\"\"\n    if models_to_test is None:\n        models_to_test = ['jais', 'arabert', 'aragpt2', 'aya']  # Start with a few key models\n    \n    results = {}\n    \n    for model_key in models_to_test:\n        print(f\"\\n{'='*60}\")\n        print(f\"Testing Model: {model_key}\")\n        print(f\"{'='*60}\")\n        \n        try:\n            result = evaluate_model_on_training_data(\n                train_file_path=train_file_path,\n                model_key=model_key,\n                use_quantization=True,\n                max_new_tokens=1500\n            )\n            \n            if result:\n                results[model_key] = result\n                print(f\"✅ {model_key} completed successfully\")\n            else:\n                print(f\"❌ {model_key} failed\")\n                \n        except Exception as e:\n            print(f\"❌ Error testing {model_key}: {e}\")\n            results[model_key] = None\n        \n        # Clean up memory between models\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    \n    # Summary comparison\n    print(f\"\\n{'='*80}\")\n    print(\"📊 FINAL COMPARISON RESULTS\")\n    print(f\"{'='*80}\")\n    \n    valid_results = {k: v for k, v in results.items() if v is not None}\n    \n    if valid_results:\n        # Sort by weighted F1 score\n        sorted_results = sorted(valid_results.items(), key=lambda x: x[1]['weighted_f1'], reverse=True)\n        \n        print(f\"{'Rank':<5} {'Model':<15} {'Arabic Native':<15} {'Weighted F1':<12} {'Jaccard':<10}\")\n        print(\"-\" * 70)\n        \n        for rank, (model_key, result) in enumerate(sorted_results, 1):\n            arabic_native = \"Yes\" if result['arabic_native'] else \"No\"\n            print(f\"{rank:<5} {model_key:<15} {arabic_native:<15} {result['weighted_f1']:<12.4f} {result['jaccard_score']:<10.4f}\")\n        \n        # Best model details\n        best_model, best_result = sorted_results[0]\n        print(f\"\\n🏆 Best Performing Model: {best_model}\")\n        print(f\"Model Name: {best_result['model_name']}\")\n        print(f\"Arabic Native: {'Yes' if best_result['arabic_native'] else 'No'}\")\n        print(f\"Weighted F1 Score: {best_result['weighted_f1']:.4f}\")\n        print(f\"Jaccard Score: {best_result['jaccard_score']:.4f}\")\n    \n    return results\n\nif __name__ == \"__main__\":\n    # Update this path to your training dataset\n    TRAIN_DATASET_PATH = '/kaggle/input/train-df/Train_Dev.tsv'  # Replace with actual path\n\n    # List available models\n    ArabicMedicalAnswerClassifier.list_available_models()\n\n    # Test individual model (Arabic native)\n    print(\"\\n\" + \"=\"*80)\n    print(\"Testing Arabic Native Model: Jais\")\n    print(\"=\"*80)\n    \n    jais_results = evaluate_model_on_training_data(\n        train_file_path=TRAIN_DATASET_PATH,\n        model_key='jais',\n        use_quantization=True,\n        max_new_tokens=1500\n    )\n\n    if jais_results:\n        print(f\"\\n📈 Jais Results Summary:\")\n        print(f\"Weighted F1 Score: {jais_results['weighted_f1']:.4f}\")\n        print(f\"Jaccard Score: {jais_results['jaccard_score']:.4f}\")\n\n    # Compare multiple models\n    print(\"\\n\" + \"=\"*80)\n    print(\"Comparing Multiple Arabic Models\")\n    print(\"=\"*80)\n    \n    # Test both Arabic native and multilingual models\n    models_to_compare = [\n        'jais',      # Arabic native\n        'arabert',     # Multilingual with good Arabic\n        'aragpt2',    # Multilingual\n        'aya'        # Arabic-focused multilingual\n    ]\n    \n    comparison_results = compare_arabic_models(\n        train_file_path=TRAIN_DATASET_PATH,\n        models_to_test=models_to_compare\n    )\n    \n    # Additional analysis for Arabic models\n    print(\"\\n\" + \"=\"*80)\n    print(\"📊 ARABIC MODEL ANALYSIS\")\n    print(\"=\"*80)\n    \n    arabic_models = []\n    multilingual_models = []\n    \n    for model_key, result in comparison_results.items():\n        if result is not None:\n            if result['arabic_native']:\n                arabic_models.append((model_key, result))\n            else:\n                multilingual_models.append((model_key, result))\n    \n    if arabic_models:\n        print(\"\\n🇸🇦 Arabic Native Models Performance:\")\n        for model_key, result in arabic_models:\n            print(f\"{model_key}: F1={result['weighted_f1']:.4f}, Jaccard={result['jaccard_score']:.4f}\")\n    \n    if multilingual_models:\n        print(\"\\n🌍 Multilingual Models Performance:\")\n        for model_key, result in multilingual_models:\n            print(f\"{model_key}: F1={result['weighted_f1']:.4f}, Jaccard={result['jaccard_score']:.4f}\")\n    \n    # Recommendations\n    print(\"\\n\" + \"=\"*80)\n    print(\"💡 RECOMMENDATIONS\")\n    print(\"=\"*80)\n    \n    if comparison_results:\n        valid_results = {k: v for k, v in comparison_results.items() if v is not None}\n        if valid_results:\n            best_model = max(valid_results.items(), key=lambda x: x[1]['weighted_f1'])\n            \n            print(f\"🏆 Best Overall Model: {best_model[0]}\")\n            print(f\"   - Model: {best_model[1]['model_name']}\")\n            print(f\"   - Arabic Native: {'Yes' if best_model[1]['arabic_native'] else 'No'}\")\n            print(f\"   - Performance: F1={best_model[1]['weighted_f1']:.4f}\")\n            \n            if best_model[1]['arabic_native']:\n                print(f\"   - ✅ Recommended for Arabic medical texts due to native Arabic support\")\n            else:\n                print(f\"   - ⚠️  Multilingual model - consider Arabic native alternatives if available\")\n            \n            print(f\"\\n📋 Usage Example:\")\n            print(f\"classifier = ArabicMedicalAnswerClassifier(model_key='{best_model[0]}')\")\n            print(f\"strategies = classifier.classify_answer('your_arabic_medical_answer_here')\")\n\n# Additional utility functions for Arabic text processing\nclass ArabicTextPreprocessor:\n    \"\"\"Utility class for Arabic text preprocessing\"\"\"\n    \n    @staticmethod\n    def normalize_arabic(text: str) -> str:\n        \"\"\"Normalize Arabic text\"\"\"\n        import re\n        \n        # Remove diacritics\n        text = re.sub(r'[\\u064B-\\u065F\\u0670\\u0640]', '', text)\n        \n        # Normalize different forms of alef\n        text = re.sub(r'[إأآا]', 'ا', text)\n        \n        # Normalize teh marbuta\n        text = re.sub(r'ة', 'ه', text)\n        \n        # Normalize different forms of yeh\n        text = re.sub(r'[يى]', 'ي', text)\n        \n        return text.strip()\n    \n    @staticmethod\n    def is_arabic_text(text: str) -> bool:\n        \"\"\"Check if text is primarily Arabic\"\"\"\n        arabic_chars = len(re.findall(r'[\\u0600-\\u06FF]', text))\n        total_chars = len(re.findall(r'[^\\s\\d\\W]', text))\n        return arabic_chars / max(total_chars, 1) > 0.5\n\ndef create_arabic_optimized_classifier(model_key: str = 'jais') -> ArabicMedicalAnswerClassifier:\n    \"\"\"Create an Arabic-optimized classifier with best practices\"\"\"\n    \n    print(f\"🚀 Creating Arabic-optimized classifier...\")\n    \n    # Recommended settings for Arabic models\n    arabic_native_models = ['jais', 'aragpt2', 'arabert', 'camelbert', 'marbert', 'arat5']\n    \n    if model_key in arabic_native_models:\n        print(f\"✅ Using Arabic native model: {model_key}\")\n        use_quantization = True  # Arabic models tend to be smaller, quantization helps with memory\n    else:\n        print(f\"⚠️  Using multilingual model: {model_key}\")\n        use_quantization = True  # Still recommended for memory efficiency\n    \n    classifier = ArabicMedicalAnswerClassifier(\n        model_key=model_key,\n        use_quantization=use_quantization,\n        use_thinking_mode=True  # Helps with Arabic reasoning\n    )\n    \n    return classifier","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}