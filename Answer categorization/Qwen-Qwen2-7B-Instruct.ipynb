{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers>=4.51.0 torch torchvision torchaudio accelerate bitsandbytes -q\n",
    "!pip install sentencepiece protobuf -q\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import gc\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from sklearn.metrics import f1_score, jaccard_score\n",
    "import ast\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ArabicMedicalAnswerClassifier:\n",
    "    \"\"\"\n",
    "    Arabic Medical Answer Classifier supporting multiple LLM models\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model configurations with their specific settings\n",
    "    MODEL_CONFIGS = {\n",
    "        'qwen3': {\n",
    "            'name': 'Qwen/Qwen3-14B',\n",
    "            'type': 'qwen',\n",
    "            'context_length': 32768,\n",
    "            'description': 'Qwen3 14B - Latest Qwen model'\n",
    "        },\n",
    "        'qwen2': {\n",
    "            'name': 'Qwen/Qwen2-7B-Instruct',\n",
    "            'type': 'qwen',\n",
    "            'context_length': 32768,\n",
    "            'description': 'Qwen2 7B - Strong multilingual model'\n",
    "        },\n",
    "        'llama3': {\n",
    "            'name': 'meta-llama/Llama-3.1-8B-Instruct',\n",
    "            'type': 'llama',\n",
    "            'context_length': 128000,\n",
    "            'description': 'Llama 3.1 8B - Meta\\'s latest model'\n",
    "        },\n",
    "        'gemma2': {\n",
    "            'name': 'google/gemma-2-9b-it',\n",
    "            'type': 'gemma',\n",
    "            'context_length': 8192,\n",
    "            'description': 'Gemma 2 9B - Google\\'s instruction-tuned model'\n",
    "        },\n",
    "        'phi3': {\n",
    "            'name': 'microsoft/Phi-3-medium-14b-instruct',\n",
    "            'type': 'phi',\n",
    "            'context_length': 128000,\n",
    "            'description': 'Phi-3 Medium - Microsoft\\'s efficient model'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_key: str = 'qwen3', use_quantization: bool = True, use_thinking_mode: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize classifier with specified model\n",
    "        \n",
    "        Args:\n",
    "            model_key: Key from MODEL_CONFIGS\n",
    "            use_quantization: Whether to use 4-bit quantization\n",
    "            use_thinking_mode: Enable thinking mode for supported models\n",
    "        \"\"\"\n",
    "        if model_key not in self.MODEL_CONFIGS:\n",
    "            raise ValueError(f\"Model '{model_key}' not supported. Available models: {list(self.MODEL_CONFIGS.keys())}\")\n",
    "        \n",
    "        self.model_config = self.MODEL_CONFIGS[model_key]\n",
    "        self.model_key = model_key\n",
    "        self.use_quantization = use_quantization\n",
    "        self.use_thinking_mode = use_thinking_mode\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.answer_strategies = {\n",
    "            '1': 'Information (answers providing information, resources, etc.)',\n",
    "            '2': 'Direct Guidance (answers providing suggestions, instructions, or advice)',\n",
    "            '3': 'Emotional Support (answers providing approval, reassurance, or other forms of emotional support)'\n",
    "        }\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the specified model with appropriate configurations\"\"\"\n",
    "        print(f\"Loading {self.model_config['description']}...\")\n",
    "        print(f\"Model: {self.model_config['name']}\")\n",
    "        \n",
    "        quantization_config = None\n",
    "        if self.use_quantization:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_config['name'],\n",
    "                trust_remote_code=True,\n",
    "                padding_side='left'\n",
    "            )\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            model_kwargs = {\n",
    "                'trust_remote_code': True,\n",
    "                'torch_dtype': torch.float16,\n",
    "                'low_cpu_mem_usage': True,\n",
    "                'device_map': 'auto'\n",
    "            }\n",
    "            \n",
    "            if quantization_config:\n",
    "                model_kwargs['quantization_config'] = quantization_config\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_config['name'],\n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "            print(f\"โ {self.model_config['description']} loaded successfully!\")\n",
    "            self.print_model_info()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"โ Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def print_model_info(self):\n",
    "        \"\"\"Print model and memory information\"\"\"\n",
    "        print(f\"\\n๐ Model Information:\")\n",
    "        print(f\"Model: {self.model_config['name']}\")\n",
    "        print(f\"Type: {self.model_config['type']}\")\n",
    "        print(f\"Context Length: {self.model_config['context_length']:,} tokens\")\n",
    "        print(f\"Quantization: {'4-bit' if self.use_quantization else 'Full precision'}\")\n",
    "        print(f\"Thinking Mode: {'Enabled' if self.use_thinking_mode else 'Disabled'}\")\n",
    "        print(f\"Tokenizer Vocab Size: {len(self.tokenizer):,}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "            print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    \n",
    "    def create_prompt(self, answer: str) -> str:\n",
    "        \"\"\"Create model-specific prompt for classification\"\"\"\n",
    "        strategy_descriptions = \"\"\"\n",
    "ุงุณุชุฑุงุชูุฌูุงุช ุงูุฅุฌุงุจุฉ ุงูุทุจูุฉ:\n",
    "(1) ุงููุนูููุงุช - ุฅุฌุงุจุงุช ุชูุฏู ูุนูููุงุช ูููุงุฑุฏ ูุญูุงุฆู ุทุจูุฉ\n",
    "(2) ุงูุชูุฌูู ุงููุจุงุดุฑ - ุฅุฌุงุจุงุช ุชูุฏู ุงูุชุฑุงุญุงุช ูุชุนูููุงุช ููุตุงุฆุญ ูุญุฏุฏุฉ\n",
    "(3) ุงูุฏุนู ุงูุนุงุทูู - ุฅุฌุงุจุงุช ุชูุฏู ุงูููุงููุฉ ูุงูุทูุฃูููุฉ ุฃู ุฃุดูุงู ุฃุฎุฑู ูู ุงูุฏุนู ุงูุนุงุทูู\n",
    "\"\"\"\n",
    "        \n",
    "        if self.use_thinking_mode:\n",
    "            base_prompt = f\"\"\"ุฃูุช ุฎุจูุฑ ูู ุชุตููู ุงูุฅุฌุงุจุงุช ุงูุทุจูุฉ ุจุงููุบุฉ ุงูุนุฑุจูุฉ. ูููุชู ูู ุชุตููู ุงูุฅุฌุงุจุฉ ุงูุชุงููุฉ ุฅูู ุงุณุชุฑุงุชูุฌูุฉ ุฃู ุฃูุซุฑ ูู ุงูุงุณุชุฑุงุชูุฌูุงุช ุงููุญุฏุฏุฉ.\n",
    "\n",
    "{strategy_descriptions}\n",
    "\n",
    "ุงูุฅุฌุงุจุฉ ุงููุฑุงุฏ ุชุตููููุง:\n",
    "{answer}\n",
    "\n",
    "ุชุนูููุงุช:\n",
    "1. ููุฑ ุจุนูู ูู ูุญุชูู ุงูุฅุฌุงุจุฉ ูุญูู ูู ุฌุงูุจ ูููุง\n",
    "2. ุงุดุฑุญ ุชูููุฑู ุฎุทูุฉ ุจุฎุทูุฉ\n",
    "3. ุญุฏุฏ ุงููููุงุช ุงูููุชุงุญูุฉ ูุงูููุงููู ุงูุทุจูุฉ\n",
    "4. ุงุฑุจุท ุงูุฅุฌุงุจุฉ ุจุงูุงุณุชุฑุงุชูุฌูุงุช ุงูููุงุณุจุฉ ูุน ุงูุชุจุฑูุฑ\n",
    "5. ุงุฎุชุฑ ุงูุงุณุชุฑุงุชูุฌูุฉ ุฃู ุงูุงุณุชุฑุงุชูุฌูุงุช ุงูุฃูุณุจ (ูููู ุฃู ูููู ุฃูุซุฑ ูู ุงุณุชุฑุงุชูุฌูุฉ)\n",
    "6. ูู ุงูููุงูุฉุ ุงูุชุจ ุงูุฅุฌุงุจุฉ ุจุงูุชูุณูู ุงูุชุงูู:\n",
    "   \"ุงูุชุตููู ุงูููุงุฆู: [1,2,3]\" (ุงุณุชุฎุฏู ุงูุฃุฑูุงู ุงูููุงุณุจุฉ ููุตููุฉ ุจููุงุตู)\n",
    "\n",
    "ูุซุงู ุนูู ุงูุชูููุฑ ูุงูุชูุณูู:\n",
    "- ุญูู ุงูุฅุฌุงุจุฉ: \"ุงูุตุฏุงุน ูุฏ ูููู ุจุณุจุจ ุงูุฅุฌูุงุฏ. ุญุงูู ุงูุฑุงุญุฉ.\"\n",
    "- ุงููููุงุช ุงูููุชุงุญูุฉ: \"ุณุจุจุ ุฅุฌูุงุฏุ ุญุงูู ุงูุฑุงุญุฉ\"\n",
    "- ุงูุชูููุฑ: ุงูุฅุฌุงุจุฉ ุชููุฑ ูุนูููุงุช ุนู ุณุจุจ (ุงูุฅุฌูุงุฏ) ูุชูุฏู ูุตูุญุฉ (ุงูุฑุงุญุฉ)\n",
    "- ุงูุชุตููู ุงูููุงุฆู: [1,2]\n",
    "\"\"\"\n",
    "        else:\n",
    "            base_prompt = f\"\"\"ุฃูุช ุฎุจูุฑ ูู ุชุตููู ุงูุฅุฌุงุจุงุช ุงูุทุจูุฉ ุจุงููุบุฉ ุงูุนุฑุจูุฉ. ูููุชู ูู ุชุตููู ุงูุฅุฌุงุจุฉ ุงูุชุงููุฉ ุฅูู ุงุณุชุฑุงุชูุฌูุฉ ุฃู ุฃูุซุฑ ูู ุงูุงุณุชุฑุงุชูุฌูุงุช ุงููุญุฏุฏุฉ.\n",
    "\n",
    "{strategy_descriptions}\n",
    "\n",
    "ุงูุฅุฌุงุจุฉ ุงููุฑุงุฏ ุชุตููููุง:\n",
    "{answer}\n",
    "\n",
    "ุชุนูููุงุช:\n",
    "1. ุงูุฑุฃ ุงูุฅุฌุงุจุฉ ุจุนูุงูุฉ ูุญูู ูุญุชูุงูุง\n",
    "2. ุญุฏุฏ ุงูุงุณุชุฑุงุชูุฌูุฉ ุฃู ุงูุงุณุชุฑุงุชูุฌูุงุช ุงูููุงุณุจุฉ (ูููู ุฃู ูููู ููุงู ุฃูุซุฑ ูู ุงุณุชุฑุงุชูุฌูุฉ ูุงุญุฏุฉ)\n",
    "3. ุงุดุฑุญ ุณุจุจ ุงุฎุชูุงุฑู ููู ุงุณุชุฑุงุชูุฌูุฉ\n",
    "4. ูู ุงูููุงูุฉุ ุงูุชุจ ุงูุฅุฌุงุจุฉ ุจุงูุชูุณูู ุงูุชุงูู:\n",
    "   \"ุงูุชุตููู ุงูููุงุฆู: [1,2,3]\" (ุงุณุชุฎุฏู ุงูุฃุฑูุงู ุงูููุงุณุจุฉ ููุตููุฉ ุจููุงุตู)\n",
    "\n",
    "ูุซุงู ุนูู ุงูุชูุณูู:\n",
    "- ุฅุฐุง ูุงูุช ุงูุฅุฌุงุจุฉ ูุนูููุงุชูุฉ ููุท: \"ุงูุชุตููู ุงูููุงุฆู: [1]\"\n",
    "- ุฅุฐุง ูุงูุช ุงูุฅุฌุงุจุฉ ุชุญุชูู ุนูู ูุนูููุงุช ูุชูุฌูู: \"ุงูุชุตููู ุงูููุงุฆู: [1,2]\"\n",
    "\"\"\"\n",
    "        \n",
    "        if self.model_config['type'] in ['llama']:\n",
    "            return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{base_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        elif self.model_config['type'] == 'qwen':\n",
    "            messages = [{\"role\": \"user\", \"content\": base_prompt}]\n",
    "            return self.tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=self.use_thinking_mode\n",
    "            )\n",
    "        elif self.model_config['type'] == 'gemma':\n",
    "            return f\"<start_of_turn>user\\n{base_prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "        elif self.model_config['type'] == 'phi':\n",
    "            return f\"<|user|>\\n{base_prompt}<|end|>\\n<|assistant|>\\n\"\n",
    "        else:\n",
    "            return f\"Human: {base_prompt}\\n\\nAssistant:\"\n",
    "    \n",
    "    def classify_answer(self, answer: str, max_new_tokens: int = 8000) -> List[str]:\n",
    "        \"\"\"\n",
    "        Classify Arabic medical answer into strategy categories\n",
    "        \"\"\"\n",
    "        prompt = self.create_prompt(answer)\n",
    "        \n",
    "        model_inputs = self.tokenizer(\n",
    "            prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True,\n",
    "            max_length=self.model_config['context_length'] - max_new_tokens\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        gen_kwargs = {\n",
    "            'max_new_tokens': max_new_tokens,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.9,\n",
    "            'top_k': 50,\n",
    "            'do_sample': True,\n",
    "            'pad_token_id': self.tokenizer.pad_token_id,\n",
    "            'eos_token_id': self.tokenizer.eos_token_id,\n",
    "            'repetition_penalty': 1.1\n",
    "        }\n",
    "        \n",
    "        if self.model_config['type'] == 'qwen':\n",
    "            gen_kwargs.update({\n",
    "                'temperature': 0.6,\n",
    "                'top_p': 0.95,\n",
    "                'top_k': 20\n",
    "            })\n",
    "        elif self.model_config['type'] == 'llama':\n",
    "            gen_kwargs.update({\n",
    "                'temperature': 0.8,\n",
    "                'top_p': 0.95\n",
    "            })\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(\n",
    "                **model_inputs,\n",
    "                **gen_kwargs\n",
    "            )\n",
    "        \n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n",
    "        content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "        \n",
    "        strategies = self.extract_answer_strategies(content)\n",
    "        return strategies\n",
    "    \n",
    "    def extract_answer_strategies(self, response: str) -> List[str]:\n",
    "        \"\"\"Extract answer strategies from Arabic response text\"\"\"\n",
    "        patterns = [\n",
    "            r'ุงูุชุตููู ุงูููุงุฆู:\\s*\\[([123,\\s]+)\\]',\n",
    "            r'ุงูุงุณุชุฑุงุชูุฌูุงุช:\\s*\\[([123,\\s]+)\\]',\n",
    "            r'ุงูุชุตููู:\\s*\\[([123,\\s]+)\\]',\n",
    "            r'ุงููุชูุฌุฉ:\\s*\\[([123,\\s]+)\\]',\n",
    "            r'ุงูุฅุฌุงุจุฉ:\\s*\\[([123,\\s]+)\\]',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, response, re.IGNORECASE)\n",
    "            if match:\n",
    "                strategies_str = match.group(1)\n",
    "                strategies = [strat.strip() for strat in strategies_str.split(',')]\n",
    "                return [strat for strat in strategies if strat in ['1', '2', '3']]\n",
    "        \n",
    "        english_patterns = [\n",
    "            r'Final Classification:\\s*\\[([123,\\s]+)\\]',\n",
    "            r'Strategies:\\s*\\[([123,\\s]+)\\]',\n",
    "            r'Classification:\\s*\\[([123,\\s]+)\\]',\n",
    "            r'Answer:\\s*\\[([123,\\s]+)\\]',\n",
    "        ]\n",
    "        \n",
    "        for pattern in english_patterns:\n",
    "            match = re.search(pattern, response, re.IGNORECASE)\n",
    "            if match:\n",
    "                strategies_str = match.group(1)\n",
    "                strategies = [strat.strip() for strat in strategies_str.split(',')]\n",
    "                return [strat for strat in strategies if strat in ['1', '2', '3']]\n",
    "        \n",
    "        found_strategies = []\n",
    "        for strategy in ['1', '2', '3']:\n",
    "            if f'({strategy})' in response or f'[{strategy}]' in response or f' {strategy} ' in response:\n",
    "                found_strategies.append(strategy)\n",
    "        \n",
    "        return found_strategies if found_strategies else ['1']\n",
    "    \n",
    "    def process_test_dataset(self, df: pd.DataFrame, max_new_tokens: int = 8000, show_progress: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process dataset for answer classification\n",
    "        \"\"\"\n",
    "        print(f\"๐ Starting Arabic Medical Answer Classification with {self.model_config['description']}...\")\n",
    "        print(f\"Dataset size: {len(df)} samples\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if show_progress and idx % 10 == 0:\n",
    "                print(f\"Processing sample {idx+1}/{len(df)} - Current GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "            \n",
    "            try:\n",
    "                strategies = self.classify_answer(row['answer'], max_new_tokens=max_new_tokens)\n",
    "                prediction_str = ', '.join(sorted(strategies))\n",
    "                predictions.append(prediction_str)\n",
    "                \n",
    "                if show_progress and idx < 3:\n",
    "                    print(f\"Sample {idx+1} prediction: {prediction_str}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing answer {idx}: {e}\")\n",
    "                predictions.append('1')\n",
    "            \n",
    "            if idx % 20 == 0:\n",
    "                self.cleanup_memory()\n",
    "        \n",
    "        result_df = pd.DataFrame({'prediction': predictions})\n",
    "        return result_df\n",
    "    \n",
    "    def cleanup_memory(self):\n",
    "        \"\"\"Clean up GPU memory\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    @classmethod\n",
    "    def list_available_models(cls):\n",
    "        \"\"\"List all available models with descriptions\"\"\"\n",
    "        print(\"๐ Available Models:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for key, config in cls.MODEL_CONFIGS.items():\n",
    "            print(f\"Key: '{key}'\")\n",
    "            print(f\"  Model: {config['name']}\")\n",
    "            print(f\"  Description: {config['description']}\")\n",
    "            print(f\"  Context Length: {config['context_length']:,} tokens\")\n",
    "            print(f\"  Type: {config['type']}\")\n",
    "            print()\n",
    "\n",
    "def evaluate_model_on_training_data(\n",
    "    train_file_path: str,\n",
    "    model_key: str = 'qwen3',\n",
    "    use_quantization: bool = True,\n",
    "    max_new_tokens: int = 8000\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the training dataset using Weighted F1 Score and Jaccard Score.\n",
    "\n",
    "    Args:\n",
    "        train_file_path: Path to the training dataset (TSV with 'answer' and 'final_AS' columns)\n",
    "        model_key: Model key from MODEL_CONFIGS\n",
    "        use_quantization: Use 4-bit quantization\n",
    "        max_new_tokens: Maximum tokens to generate per answer\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics (Weighted F1 Score, Jaccard Score)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"๐ Initializing {model_key} for evaluation...\")\n",
    "    try:\n",
    "        classifier = ArabicMedicalAnswerClassifier(\n",
    "            model_key=model_key,\n",
    "            use_quantization=use_quantization\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"โ Failed to initialize model: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Load training dataset\n",
    "    try:\n",
    "        df = pd.read_csv(train_file_path, sep='\\t')\n",
    "        if 'answer' not in df.columns or 'final_AS' not in df.columns:\n",
    "            raise ValueError(\"Dataset must contain 'answer' and 'final_AS' columns\")\n",
    "        print(f\"โ Training dataset loaded: {len(df)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"โ Error loading training dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Parse final_AS labels\n",
    "    def parse_labels(label):\n",
    "        if isinstance(label, str):\n",
    "            try:\n",
    "                if label.startswith('['):\n",
    "                    return ast.literal_eval(label)\n",
    "                else:\n",
    "                    return [strat.strip() for strat in label.split(',')]\n",
    "            except:\n",
    "                print(f\"Warning: Could not parse label '{label}', defaulting to ['1']\")\n",
    "                return ['1']\n",
    "        return label\n",
    "\n",
    "    df['final_AS'] = df['final_AS'].apply(parse_labels)\n",
    "\n",
    "    # Generate predictions\n",
    "    print(f\"๐ Generating predictions for {len(df)} samples...\")\n",
    "    predictions = classifier.process_test_dataset(df, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Convert predictions and true labels to multi-label binary format\n",
    "    all_strategies = ['1', '2', '3']\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for true_labels, pred_labels in zip(df['final_AS'], predictions['prediction']):\n",
    "        true_vec = [1 if strat in true_labels else 0 for strat in all_strategies]\n",
    "        y_true.append(true_vec)\n",
    "        pred_strats = [strat.strip() for strat in pred_labels.split(',') if strat.strip() in all_strategies]\n",
    "        pred_vec = [1 if strat in pred_strats else 0 for strat in all_strategies]\n",
    "        y_pred.append(pred_vec)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calculate metrics\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    jaccard = jaccard_score(y_true, y_pred, average='samples')\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n๐ Evaluation Results:\")\n",
    "    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
    "    print(f\"Jaccard Score (samples): {jaccard:.4f}\")\n",
    "\n",
    "    # Detailed per-strategy metrics\n",
    "    print(\"\\n๐ Per-Strategy Metrics:\")\n",
    "    for i, strat in enumerate(all_strategies):\n",
    "        strat_f1 = f1_score(y_true[:, i], y_pred[:, i])\n",
    "        strat_jaccard = jaccard_score(y_true[:, i], y_pred[:, i])\n",
    "        print(f\"Strategy {strat}:\")\n",
    "        print(f\"  F1 Score: {strat_f1:.4f}\")\n",
    "        print(f\"  Jaccard Score: {strat_jaccard:.4f}\")\n",
    "\n",
    "    # Cleanup memory\n",
    "    classifier.cleanup_memory()\n",
    "\n",
    "    return {\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'jaccard_score': jaccard,\n",
    "        'per_strategy_f1': {strat: f1_score(y_true[:, i], y_pred[:, i]) for i, strat in enumerate(all_strategies)},\n",
    "        'per_strategy_jaccard': {strat: jaccard_score(y_true[:, i], y_pred[:, i]) for i, strat in enumerate(all_strategies)}\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update this path to your training dataset\n",
    "    TRAIN_DATASET_PATH = '/kaggle/input/train-df/Train_Dev.tsv'  # Replace with actual path\n",
    "\n",
    "    # List available models\n",
    "    ArabicMedicalAnswerClassifier.list_available_models()\n",
    "\n",
    "    # Evaluate with a specific model\n",
    "    results = evaluate_model_on_training_data(\n",
    "        train_file_path=TRAIN_DATASET_PATH,\n",
    "        model_key='qwen2',\n",
    "        use_quantization=True,\n",
    "        max_new_tokens=8000\n",
    "    )\n",
    "\n",
    "    if results:\n",
    "        print(\"\\n๐ Summary of Results:\")\n",
    "        print(f\"Weighted F1 Score: {results['weighted_f1']:.4f}\")\n",
    "        print(f\"Jaccard Score: {results['jaccard_score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7894059,
     "sourceId": 12507227,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7953705,
     "sourceId": 12592865,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
