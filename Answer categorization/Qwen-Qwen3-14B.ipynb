{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12507227,"sourceType":"datasetVersion","datasetId":7894059},{"sourceId":12592865,"sourceType":"datasetVersion","datasetId":7953705}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers>=4.51.0 torch torchvision torchaudio accelerate bitsandbytes -q\n!pip install sentencepiece protobuf -q\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport gc\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\n# Check GPU availability\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nimport re\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom typing import List, Dict, Tuple, Any\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass ArabicMedicalAnswerClassifier:\n    def __init__(self):\n        self.model_name = \"Qwen/Qwen3-14B\"\n        self.tokenizer = None\n        self.model = None\n        self.answer_strategies = {\n            '1': 'Information (answers providing information, resources, etc.)',\n            '2': 'Direct Guidance (answers providing suggestions, instructions, or advice)',\n            '3': 'Emotional Support (answers providing approval, reassurance, or other forms of emotional support)'\n        }\n        self.load_model()\n    \n    def load_model(self):\n        \"\"\"Load Qwen3-14B with optimizations for Arabic medical text classification\"\"\"\n        print(\"Loading Qwen3-14B model for Arabic Medical Answer Classification...\")\n        \n        # Configure quantization for memory efficiency\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.model_name,\n            trust_remote_code=True\n        )\n        \n        # Load model with quantization\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            quantization_config=quantization_config,\n            device_map=\"auto\",\n            trust_remote_code=True,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True\n        )\n        \n        print(\"Qwen3-14B model loaded successfully!\")\n        self.print_model_info()\n    \n    def print_model_info(self):\n        \"\"\"Print model and memory information\"\"\"\n        print(f\"\\nModel Information:\")\n        print(f\"Model Name: {self.model_name}\")\n        print(f\"Model Parameters: 14.8B (13.2B non-embedding)\")\n        print(f\"Context Length: 32,768 tokens\")\n        print(f\"Tokenizer Vocab Size: {len(self.tokenizer):,}\")\n        \n        if torch.cuda.is_available():\n            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n    \n    def classify_answer(self, answer: str, use_thinking_mode: bool = True, max_new_tokens: int = 8000) -> List[str]:\n        \"\"\"\n        Classify Arabic medical answer into strategy categories\n        \n        Args:\n            answer: Arabic medical answer\n            use_thinking_mode: Enable Qwen3's thinking capabilities\n            max_new_tokens: Maximum tokens to generate\n        \n        Returns:\n            List[str]: extracted_strategies\n        \"\"\"\n        \n        # Create strategy descriptions in Arabic\n        strategy_descriptions = \"\"\"\nØ§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø·Ø¨ÙŠØ©:\n(1) Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª - Ø¥Ø¬Ø§Ø¨Ø§Øª ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆÙ…ÙˆØ§Ø±Ø¯ ÙˆØ­Ù‚Ø§Ø¦Ù‚ Ø·Ø¨ÙŠØ©\n(2) Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± - Ø¥Ø¬Ø§Ø¨Ø§Øª ØªÙ‚Ø¯Ù… Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª ÙˆØªØ¹Ù„ÙŠÙ…Ø§Øª ÙˆÙ†ØµØ§Ø¦Ø­ Ù…Ø­Ø¯Ø¯Ø©\n(3) Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø§Ø·ÙÙŠ - Ø¥Ø¬Ø§Ø¨Ø§Øª ØªÙ‚Ø¯Ù… Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø© ÙˆØ§Ù„Ø·Ù…Ø£Ù†ÙŠÙ†Ø© Ø£Ùˆ Ø£Ø´ÙƒØ§Ù„ Ø£Ø®Ø±Ù‰ Ù…Ù† Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø§Ø·ÙÙŠ\n\"\"\"\n        \n        messages = [\n            {\n                \"role\": \"user\", \n                \"content\": f\"\"\"Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¥Ù„Ù‰ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø£Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.\n\n{strategy_descriptions}\n\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØµÙ†ÙŠÙÙ‡Ø§:\n{answer}\n\nØªØ¹Ù„ÙŠÙ…Ø§Øª:\n1. Ø§Ù‚Ø±Ø£ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø¹Ù†Ø§ÙŠØ© ÙˆØ­Ù„Ù„ Ù…Ø­ØªÙˆØ§Ù‡Ø§ ÙˆØ£Ø³Ù„ÙˆØ¨Ù‡Ø§\n2. Ø­Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø£Ùˆ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© (ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ÙˆØ§Ø­Ø¯Ø©)\n3. Ø§Ø´Ø±Ø­ Ø³Ø¨Ø¨ Ø§Ø®ØªÙŠØ§Ø±Ùƒ Ù„ÙƒÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©\n4. ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ø§ÙƒØªØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ:\n   \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [1,2,3]\" (Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„)\n\nÙ…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚:\n- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠØ© ÙÙ‚Ø·: \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [1]\"\n- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØªÙˆØ¬ÙŠÙ‡: \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [1,2]\"\n- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ù„ØªÙˆØ¬ÙŠÙ‡ ÙˆØ§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø§Ø·ÙÙŠ: \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [1,2,3]\"\n\"\"\"\n            }\n        ]\n        \n        # Apply chat template with thinking mode control\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n            enable_thinking=use_thinking_mode\n        )\n        \n        # Tokenize input\n        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n        \n        # Generate response\n        with torch.no_grad():\n            if use_thinking_mode:\n                generated_ids = self.model.generate(\n                    **model_inputs,\n                    max_new_tokens=max_new_tokens,\n                    temperature=0.6,\n                    top_p=0.95,\n                    top_k=20,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    repetition_penalty=1.1\n                )\n            else:\n                generated_ids = self.model.generate(\n                    **model_inputs,\n                    max_new_tokens=max_new_tokens,\n                    temperature=0.7,\n                    top_p=0.8,\n                    top_k=20,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    repetition_penalty=1.1\n                )\n        \n        # Extract output tokens\n        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n        \n        # Parse thinking content (if thinking mode is enabled)\n        content = \"\"\n        \n        if use_thinking_mode:\n            try:\n                # Find </think> token (151668)\n                index = len(output_ids) - output_ids[::-1].index(151668)\n                content = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()\n            except ValueError:\n                content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n        else:\n            content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n        \n        strategies = self.extract_answer_strategies(content)\n        \n        return strategies\n    \n    def extract_answer_strategies(self, response: str) -> List[str]:\n        \"\"\"Extract answer strategies from Arabic response text\"\"\"\n        \n        # Arabic patterns for strategy extraction\n        patterns = [\n            r'Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:\\s*\\[([123,\\s]+)\\]',  # \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [1,2,3]\"\n            r'Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª:\\s*\\[([123,\\s]+)\\]',  # \"Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª: [1,2,3]\"\n            r'Ø§Ù„ØªØµÙ†ÙŠÙ:\\s*\\[([123,\\s]+)\\]',  # \"Ø§Ù„ØªØµÙ†ÙŠÙ: [1,2,3]\"\n            r'Ø§Ù„Ù†ØªÙŠØ¬Ø©:\\s*\\[([123,\\s]+)\\]',  # \"Ø§Ù„Ù†ØªÙŠØ¬Ø©: [1,2,3]\"\n        ]\n        \n        for pattern in patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                strategies_str = match.group(1)\n                strategies = [strat.strip() for strat in strategies_str.split(',')]\n                return [strat for strat in strategies if strat in ['1', '2', '3']]\n        \n        # English patterns as fallback\n        english_patterns = [\n            r'Final Classification:\\s*\\[([123,\\s]+)\\]',\n            r'Strategies:\\s*\\[([123,\\s]+)\\]',\n            r'Classification:\\s*\\[([123,\\s]+)\\]',\n        ]\n        \n        for pattern in english_patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                strategies_str = match.group(1)\n                strategies = [strat.strip() for strat in strategies_str.split(',')]\n                return [strat for strat in strategies if strat in ['1', '2', '3']]\n        \n        # Look for individual strategy mentions\n        found_strategies = []\n        for strategy in ['1', '2', '3']:\n            if f'({strategy})' in response or f'[{strategy}]' in response:\n                found_strategies.append(strategy)\n        \n        return found_strategies if found_strategies else ['1']  # Default to 'Information' if nothing found\n    \n    def process_test_dataset(self, df: pd.DataFrame, use_thinking: bool = True, show_progress: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Process test dataset for answer classification\n        \n        Args:\n            df: DataFrame with 'answer' column\n            use_thinking: Enable thinking mode\n            show_progress: Show progress information\n        \n        Returns:\n            DataFrame with predictions\n        \"\"\"\n        \n        print(\"ğŸš€ Starting Arabic Medical Answer Classification for Test Dataset...\")\n        print(f\"Test dataset size: {len(df)} samples\")\n        print(\"-\" * 60)\n        \n        # Initialize result list\n        predictions = []\n        \n        for idx, row in df.iterrows():\n            if show_progress and idx % 10 == 0:\n                print(f\"Processing sample {idx+1}/{len(df)}\")\n            \n            # Classify answer\n            try:\n                strategies = self.classify_answer(\n                    row['answer'], \n                    use_thinking_mode=use_thinking\n                )\n                # Convert list to comma-separated string as required by submission format\n                prediction_str = ', '.join(sorted(strategies))\n                predictions.append(prediction_str)\n            except Exception as e:\n                print(f\"Error processing answer {idx}: {e}\")\n                predictions.append('1')\n            \n            # Clean up memory periodically\n            if idx % 20 == 0:\n                self.cleanup_memory()\n        \n        # Create result dataframe with just the predictions\n        result_df = pd.DataFrame({'prediction': predictions})\n        \n        return result_df\n    \n    def cleanup_memory(self):\n        \"\"\"Clean up GPU memory\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n\n# Initialize the classifier\nprint(\"Initializing Arabic Medical Answer Classifier with Qwen3-14B...\")\nclassifier = ArabicMedicalAnswerClassifier()\n\ndef generate_test_predictions(test_file_path: str, output_file_path: str = 'prediction_subtask_2.tsv', use_thinking: bool = True):\n    \"\"\"\n    Generate predictions for test dataset and save to TSV file\n    \n    Args:\n        test_file_path: Path to test dataset TSV file\n        output_file_path: Path for output predictions file\n        use_thinking: Enable thinking mode\n    \"\"\"\n    \n    print(\"ğŸš€ Loading test dataset...\")\n    \n    # Load test dataset\n    try:\n        df = pd.read_csv(test_file_path, sep='\\t',header=None)\n        df.columns=['answer']\n        # df=df[:1]\n        print(f\"âœ… Test dataset loaded successfully: {len(df)} samples\")\n    except Exception as e:\n        print(f\"âŒ Error loading test dataset: {e}\")\n        return\n    \n    # Check if 'answer' column exists\n    if 'answer' not in df.columns:\n        print(\"âŒ Error: 'answer' column not found in test dataset\")\n        print(f\"Available columns: {list(df.columns)}\")\n        return\n    \n    # Process test dataset\n    print(\"ğŸ” Processing test dataset...\")\n    results_df = classifier.process_test_dataset(df, use_thinking=use_thinking)\n    \n    # Save predictions to TSV file\n    try:\n        # Save as TSV without header, just predictions\n        results_df['prediction'].to_csv(output_file_path, sep='\\t', index=False, header=False)\n        print(f\"âœ… Predictions saved to: {output_file_path}\")\n        print(f\"ğŸ“„ Total predictions: {len(results_df)}\")\n        \n        # Show sample predictions\n        print(\"\\nğŸ“‹ Sample predictions:\")\n        for i in range(min(5, len(results_df))):\n            print(f\"Sample {i+1}: {results_df['prediction'].iloc[i]}\")\n            \n    except Exception as e:\n        print(f\"âŒ Error saving predictions: {e}\")\n    \n    # Clean up memory\n    classifier.cleanup_memory()\n    \n    return results_df\n\ndef get_model_status():\n    \"\"\"Display current model status\"\"\"\n    print(\"\\nğŸ“Š Model Status:\")\n    print(f\"Model: {classifier.model_name}\")\n    print(f\"Model loaded: {'âœ…' if classifier.model else 'âŒ'}\")\n    if torch.cuda.is_available():\n        print(f\"GPU memory usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    classifier.print_model_info()\n\n# Main execution\nif __name__ == \"__main__\":\n    print(\"ğŸš€ Arabic Medical Answer Classifier for Test Dataset Ready!\")\n    \n    # Replace with your actual test dataset path\n    TEST_DATASET_PATH = '/kaggle/input/testdf/subtask2_input_test.tsv'  # Update this path\n    \n    # Generate predictions\n    # Uncomment the following line and update the path:\n    generate_test_predictions(TEST_DATASET_PATH)\n    \n    # For demonstration with sample data (remove this in actual usage):\n    # print(\"ğŸ“š Creating sample test to demonstrate functionality...\")\n    # sample_data = pd.DataFrame({\n    #     'answer': [\n    #         'Ø§Ù„ØµØ¯Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø¹Ù„Ø§Ù…Ø© Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø§Øª Ù…Ø®ØªÙ„ÙØ©. Ù…Ù† Ø§Ù„Ù…Ù‡Ù… Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.',\n    #         'ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ†Ø§ÙˆÙ„ Ù…Ø³ÙƒÙ† Ù„Ù„Ø£Ù„Ù… ÙˆØ§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø¥Ø°Ø§ Ø§Ø³ØªÙ…Ø± Ø§Ù„Ø£Ù„Ù…. Ù„Ø§ ØªÙ‚Ù„Ù‚ØŒ Ù…Ø¹Ø¸Ù… Ø§Ù„Ø­Ø§Ù„Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø¹Ù„Ø§Ø¬.',\n    #         'Ø§Ù„Ø³ÙƒØ±ÙŠ Ù…Ø±Ø¶ Ù…Ø²Ù…Ù† ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø³ÙƒØ± ÙÙŠ Ø§Ù„Ø¯Ù…. ÙŠØªØ·Ù„Ø¨ Ù…ØªØ§Ø¨Ø¹Ø© Ø·Ø¨ÙŠØ© Ù…Ù†ØªØ¸Ù…Ø© ÙˆØ¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØºØ°Ø§Ø¦ÙŠ.'\n    #     ]\n    # })\n    \n    # print(\"Processing sample data...\")\n    # sample_results = classifier.process_test_dataset(sample_data, use_thinking=True)\n    # print(\"\\nSample predictions:\")\n    # for i, pred in enumerate(sample_results['prediction']):\n    #     print(f\"Answer {i+1}: {pred}\")\n    \n    # # Save sample predictions\n    # sample_results['prediction'].to_csv('sample_prediction_subtask_2.tsv', sep='\\t', index=False, header=False)\n    # print(\"Sample predictions saved to: sample_prediction_subtask_2.tsv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}