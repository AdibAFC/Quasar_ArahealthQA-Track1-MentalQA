{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12507227,"sourceType":"datasetVersion","datasetId":7894059},{"sourceId":12592865,"sourceType":"datasetVersion","datasetId":7953705}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers>=4.51.0 torch torchvision torchaudio accelerate bitsandbytes -q\n!pip install sentencepiece protobuf -q\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport gc\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\n# Check GPU availability\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nimport re\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom typing import List, Dict, Tuple, Any\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass ArabicMedicalAnswerClassifier:\n    def __init__(self):\n        self.model_name = \"Qwen/Qwen3-14B\"\n        self.tokenizer = None\n        self.model = None\n        self.answer_strategies = {\n            '1': 'Information (answers providing information, resources, etc.)',\n            '2': 'Direct Guidance (answers providing suggestions, instructions, or advice)',\n            '3': 'Emotional Support (answers providing approval, reassurance, or other forms of emotional support)'\n        }\n        self.load_model()\n    \n    def load_model(self):\n        \"\"\"Load Qwen3-14B with optimizations for Arabic medical text classification\"\"\"\n        print(\"Loading Qwen3-14B model for Arabic Medical Answer Classification...\")\n        \n        # Configure quantization for memory efficiency\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.model_name,\n            trust_remote_code=True\n        )\n        \n        # Load model with quantization\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            quantization_config=quantization_config,\n            device_map=\"auto\",\n            trust_remote_code=True,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True\n        )\n        \n        print(\"Qwen3-14B model loaded successfully!\")\n        self.print_model_info()\n    \n    def print_model_info(self):\n        \"\"\"Print model and memory information\"\"\"\n        print(f\"\\nModel Information:\")\n        print(f\"Model Name: {self.model_name}\")\n        print(f\"Model Parameters: 14.8B (13.2B non-embedding)\")\n        print(f\"Context Length: 32,768 tokens\")\n        print(f\"Tokenizer Vocab Size: {len(self.tokenizer):,}\")\n        \n        if torch.cuda.is_available():\n            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n    \n    def classify_answer(self, answer: str, use_thinking_mode: bool = True, max_new_tokens: int = 8000) -> List[str]:\n        \"\"\"\n        Classify Arabic medical answer into strategy categories\n        \n        Args:\n            answer: Arabic medical answer\n            use_thinking_mode: Enable Qwen3's thinking capabilities\n            max_new_tokens: Maximum tokens to generate\n        \n        Returns:\n            List[str]: extracted_strategies\n        \"\"\"\n        \n        # Create strategy descriptions in Arabic\n        strategy_descriptions = \"\"\"\nاستراتيجيات الإجابة الطبية:\n(1) المعلومات - إجابات تقدم معلومات وموارد وحقائق طبية\n(2) التوجيه المباشر - إجابات تقدم اقتراحات وتعليمات ونصائح محددة\n(3) الدعم العاطفي - إجابات تقدم الموافقة والطمأنينة أو أشكال أخرى من الدعم العاطفي\n\"\"\"\n        \n        messages = [\n            {\n                \"role\": \"user\", \n                \"content\": f\"\"\"أنت خبير في تصنيف الإجابات الطبية باللغة العربية. مهمتك هي تصنيف الإجابة التالية إلى استراتيجية أو أكثر من الاستراتيجيات المحددة.\n\n{strategy_descriptions}\n\nالإجابة المراد تصنيفها:\n{answer}\n\nتعليمات:\n1. اقرأ الإجابة بعناية وحلل محتواها وأسلوبها\n2. حدد الاستراتيجية أو الاستراتيجيات المناسبة (يمكن أن يكون هناك أكثر من استراتيجية واحدة)\n3. اشرح سبب اختيارك لكل استراتيجية\n4. في النهاية، اكتب الإجابة بالتنسيق التالي:\n   \"التصنيف النهائي: [1,2,3]\" (استخدم الأرقام المناسبة مفصولة بفواصل)\n\nمثال على التنسيق:\n- إذا كانت الإجابة معلوماتية فقط: \"التصنيف النهائي: [1]\"\n- إذا كانت الإجابة تحتوي على معلومات وتوجيه: \"التصنيف النهائي: [1,2]\"\n- إذا كانت الإجابة تحتوي على المعلومات والتوجيه والدعم العاطفي: \"التصنيف النهائي: [1,2,3]\"\n\"\"\"\n            }\n        ]\n        \n        # Apply chat template with thinking mode control\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n            enable_thinking=use_thinking_mode\n        )\n        \n        # Tokenize input\n        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n        \n        # Generate response\n        with torch.no_grad():\n            if use_thinking_mode:\n                generated_ids = self.model.generate(\n                    **model_inputs,\n                    max_new_tokens=max_new_tokens,\n                    temperature=0.6,\n                    top_p=0.95,\n                    top_k=20,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    repetition_penalty=1.1\n                )\n            else:\n                generated_ids = self.model.generate(\n                    **model_inputs,\n                    max_new_tokens=max_new_tokens,\n                    temperature=0.7,\n                    top_p=0.8,\n                    top_k=20,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    repetition_penalty=1.1\n                )\n        \n        # Extract output tokens\n        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n        \n        # Parse thinking content (if thinking mode is enabled)\n        content = \"\"\n        \n        if use_thinking_mode:\n            try:\n                # Find </think> token (151668)\n                index = len(output_ids) - output_ids[::-1].index(151668)\n                content = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()\n            except ValueError:\n                content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n        else:\n            content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n        \n        strategies = self.extract_answer_strategies(content)\n        \n        return strategies\n    \n    def extract_answer_strategies(self, response: str) -> List[str]:\n        \"\"\"Extract answer strategies from Arabic response text\"\"\"\n        \n        # Arabic patterns for strategy extraction\n        patterns = [\n            r'التصنيف النهائي:\\s*\\[([123,\\s]+)\\]',  # \"التصنيف النهائي: [1,2,3]\"\n            r'الاستراتيجيات:\\s*\\[([123,\\s]+)\\]',  # \"الاستراتيجيات: [1,2,3]\"\n            r'التصنيف:\\s*\\[([123,\\s]+)\\]',  # \"التصنيف: [1,2,3]\"\n            r'النتيجة:\\s*\\[([123,\\s]+)\\]',  # \"النتيجة: [1,2,3]\"\n        ]\n        \n        for pattern in patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                strategies_str = match.group(1)\n                strategies = [strat.strip() for strat in strategies_str.split(',')]\n                return [strat for strat in strategies if strat in ['1', '2', '3']]\n        \n        # English patterns as fallback\n        english_patterns = [\n            r'Final Classification:\\s*\\[([123,\\s]+)\\]',\n            r'Strategies:\\s*\\[([123,\\s]+)\\]',\n            r'Classification:\\s*\\[([123,\\s]+)\\]',\n        ]\n        \n        for pattern in english_patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                strategies_str = match.group(1)\n                strategies = [strat.strip() for strat in strategies_str.split(',')]\n                return [strat for strat in strategies if strat in ['1', '2', '3']]\n        \n        # Look for individual strategy mentions\n        found_strategies = []\n        for strategy in ['1', '2', '3']:\n            if f'({strategy})' in response or f'[{strategy}]' in response:\n                found_strategies.append(strategy)\n        \n        return found_strategies if found_strategies else ['1']  # Default to 'Information' if nothing found\n    \n    def process_test_dataset(self, df: pd.DataFrame, use_thinking: bool = True, show_progress: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Process test dataset for answer classification\n        \n        Args:\n            df: DataFrame with 'answer' column\n            use_thinking: Enable thinking mode\n            show_progress: Show progress information\n        \n        Returns:\n            DataFrame with predictions\n        \"\"\"\n        \n        print(\"🚀 Starting Arabic Medical Answer Classification for Test Dataset...\")\n        print(f\"Test dataset size: {len(df)} samples\")\n        print(\"-\" * 60)\n        \n        # Initialize result list\n        predictions = []\n        \n        for idx, row in df.iterrows():\n            if show_progress and idx % 10 == 0:\n                print(f\"Processing sample {idx+1}/{len(df)}\")\n            \n            # Classify answer\n            try:\n                strategies = self.classify_answer(\n                    row['answer'], \n                    use_thinking_mode=use_thinking\n                )\n                # Convert list to comma-separated string as required by submission format\n                prediction_str = ', '.join(sorted(strategies))\n                predictions.append(prediction_str)\n            except Exception as e:\n                print(f\"Error processing answer {idx}: {e}\")\n                predictions.append('1')\n            \n            # Clean up memory periodically\n            if idx % 20 == 0:\n                self.cleanup_memory()\n        \n        # Create result dataframe with just the predictions\n        result_df = pd.DataFrame({'prediction': predictions})\n        \n        return result_df\n    \n    def cleanup_memory(self):\n        \"\"\"Clean up GPU memory\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n\n# Initialize the classifier\nprint(\"Initializing Arabic Medical Answer Classifier with Qwen3-14B...\")\nclassifier = ArabicMedicalAnswerClassifier()\n\ndef generate_test_predictions(test_file_path: str, output_file_path: str = 'prediction_subtask_2.tsv', use_thinking: bool = True):\n    \"\"\"\n    Generate predictions for test dataset and save to TSV file\n    \n    Args:\n        test_file_path: Path to test dataset TSV file\n        output_file_path: Path for output predictions file\n        use_thinking: Enable thinking mode\n    \"\"\"\n    \n    print(\"🚀 Loading test dataset...\")\n    \n    # Load test dataset\n    try:\n        df = pd.read_csv(test_file_path, sep='\\t',header=None)\n        df.columns=['answer']\n        # df=df[:1]\n        print(f\"✅ Test dataset loaded successfully: {len(df)} samples\")\n    except Exception as e:\n        print(f\"❌ Error loading test dataset: {e}\")\n        return\n    \n    # Check if 'answer' column exists\n    if 'answer' not in df.columns:\n        print(\"❌ Error: 'answer' column not found in test dataset\")\n        print(f\"Available columns: {list(df.columns)}\")\n        return\n    \n    # Process test dataset\n    print(\"🔍 Processing test dataset...\")\n    results_df = classifier.process_test_dataset(df, use_thinking=use_thinking)\n    \n    # Save predictions to TSV file\n    try:\n        # Save as TSV without header, just predictions\n        results_df['prediction'].to_csv(output_file_path, sep='\\t', index=False, header=False)\n        print(f\"✅ Predictions saved to: {output_file_path}\")\n        print(f\"📄 Total predictions: {len(results_df)}\")\n        \n        # Show sample predictions\n        print(\"\\n📋 Sample predictions:\")\n        for i in range(min(5, len(results_df))):\n            print(f\"Sample {i+1}: {results_df['prediction'].iloc[i]}\")\n            \n    except Exception as e:\n        print(f\"❌ Error saving predictions: {e}\")\n    \n    # Clean up memory\n    classifier.cleanup_memory()\n    \n    return results_df\n\ndef get_model_status():\n    \"\"\"Display current model status\"\"\"\n    print(\"\\n📊 Model Status:\")\n    print(f\"Model: {classifier.model_name}\")\n    print(f\"Model loaded: {'✅' if classifier.model else '❌'}\")\n    if torch.cuda.is_available():\n        print(f\"GPU memory usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    classifier.print_model_info()\n\n# Main execution\nif __name__ == \"__main__\":\n    print(\"🚀 Arabic Medical Answer Classifier for Test Dataset Ready!\")\n    \n    # Replace with your actual test dataset path\n    TEST_DATASET_PATH = '/kaggle/input/testdf/subtask2_input_test.tsv'  # Update this path\n    \n    # Generate predictions\n    # Uncomment the following line and update the path:\n    generate_test_predictions(TEST_DATASET_PATH)\n    \n    # For demonstration with sample data (remove this in actual usage):\n    # print(\"📚 Creating sample test to demonstrate functionality...\")\n    # sample_data = pd.DataFrame({\n    #     'answer': [\n    #         'الصداع المستمر قد يكون علامة على حالات مختلفة. من المهم استشارة طبيب مختص لتقييم الحالة بشكل صحيح.',\n    #         'يمكنك تناول مسكن للألم واستشارة الطبيب إذا استمر الألم. لا تقلق، معظم الحالات قابلة للعلاج.',\n    #         'السكري مرض مزمن يؤثر على مستويات السكر في الدم. يتطلب متابعة طبية منتظمة وإدارة النظام الغذائي.'\n    #     ]\n    # })\n    \n    # print(\"Processing sample data...\")\n    # sample_results = classifier.process_test_dataset(sample_data, use_thinking=True)\n    # print(\"\\nSample predictions:\")\n    # for i, pred in enumerate(sample_results['prediction']):\n    #     print(f\"Answer {i+1}: {pred}\")\n    \n    # # Save sample predictions\n    # sample_results['prediction'].to_csv('sample_prediction_subtask_2.tsv', sep='\\t', index=False, header=False)\n    # print(\"Sample predictions saved to: sample_prediction_subtask_2.tsv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}