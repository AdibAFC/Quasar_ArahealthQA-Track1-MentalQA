{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12668834,"sourceType":"datasetVersion","datasetId":8005928}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:34:43.274725Z","iopub.execute_input":"2025-07-27T08:34:43.275031Z","iopub.status.idle":"2025-07-27T08:34:43.291282Z","shell.execute_reply.started":"2025-07-27T08:34:43.275005Z","shell.execute_reply":"2025-07-27T08:34:43.290627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers>=4.51.0 torch torchvision torchaudio accelerate bitsandbytes -q\n!pip install sentencepiece protobuf -q\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport gc\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\n# Check GPU availability\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:34:43.292522Z","iopub.execute_input":"2025-07-27T08:34:43.292762Z","execution_failed":"2025-07-27T08:35:32.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nimport re\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom typing import List, Dict, Tuple, Any, Optional\nfrom sklearn.metrics import f1_score, jaccard_score\nimport ast\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass ArabicMedicalAnswerClassifier:\n    \"\"\"\n    Arabic Medical Answer Classifier supporting multiple LLM models including Unsloth and DeepSeek\n    \"\"\"\n    \n    # Model configurations with their specific settings\n    MODEL_CONFIGS = {\n        'qwen2': {\n            'name': 'Qwen/Qwen2-7B-Instruct',\n            'type': 'qwen',\n            'context_length': 32768,\n            'description': 'Qwen2 7B - Strong multilingual model'\n        },\n        'qwen2.5-unsloth': {\n            'name': 'unsloth/Qwen2.5-7B-Instruct',\n            'type': 'qwen',\n            'context_length': 131072,\n            'description': 'Unsloth Qwen2.5 7B - Optimized for inference speed'\n        },\n        'deepseek-r1': {\n            'name': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n            'type': 'qwen',\n            'context_length': 131072,\n            'description': 'DeepSeek R1 Distill Qwen 7B - Advanced reasoning model'\n        },\n        'llama3': {\n            'name': 'meta-llama/Llama-3.1-8B-Instruct',\n            'type': 'llama',\n            'context_length': 128000,\n            'description': 'Llama 3.1 8B - Meta\\'s latest model'\n        },\n        'gemma2': {\n            'name': 'google/gemma-2-9b-it',\n            'type': 'gemma',\n            'context_length': 8192,\n            'description': 'Gemma 2 9B - Google\\'s instruction-tuned model'\n        },\n        'phi3': {\n            'name': 'microsoft/Phi-3-medium-14b-instruct',\n            'type': 'phi',\n            'context_length': 128000,\n            'description': 'Phi-3 Medium - Microsoft\\'s efficient model'\n        }\n    }\n    \n    def __init__(self, model_key: str = 'qwen2.5-unsloth', use_quantization: bool = True, use_thinking_mode: bool = True):\n        \"\"\"\n        Initialize classifier with specified model\n        \n        Args:\n            model_key: Key from MODEL_CONFIGS (default: qwen2.5-unsloth for better performance)\n            use_quantization: Whether to use 4-bit quantization\n            use_thinking_mode: Enable thinking mode for supported models\n        \"\"\"\n        if model_key not in self.MODEL_CONFIGS:\n            raise ValueError(f\"Model '{model_key}' not supported. Available models: {list(self.MODEL_CONFIGS.keys())}\")\n        \n        self.model_config = self.MODEL_CONFIGS[model_key]\n        self.model_key = model_key\n        self.use_quantization = use_quantization\n        self.use_thinking_mode = use_thinking_mode\n        self.tokenizer = None\n        self.model = None\n        self.answer_strategies = {\n            '1': 'Information (answers providing information, resources, etc.)',\n            '2': 'Direct Guidance (answers providing suggestions, instructions, or advice)',\n            '3': 'Emotional Support (answers providing approval, reassurance, or other forms of emotional support)'\n        }\n        self.load_model()\n    \n    def load_model(self):\n        \"\"\"Load the specified model with appropriate configurations\"\"\"\n        print(f\"Loading {self.model_config['description']}...\")\n        print(f\"Model: {self.model_config['name']}\")\n        \n        quantization_config = None\n        if self.use_quantization:\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\"\n            )\n        \n        try:\n            # Special handling for Unsloth models\n            if 'unsloth' in self.model_config['name']:\n                print(\"ğŸš€ Loading Unsloth optimized model...\")\n                try:\n                    # Try to use Unsloth's FastLanguageModel if available\n                    from unsloth import FastLanguageModel\n                    self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n                        model_name=self.model_config['name'],\n                        max_seq_length=self.model_config['context_length'],\n                        dtype=torch.float16,\n                        load_in_4bit=self.use_quantization,\n                    )\n                    FastLanguageModel.for_inference(self.model)  # Enable native 2x faster inference\n                    print(\"âœ… Unsloth FastLanguageModel loaded successfully!\")\n                except ImportError:\n                    print(\"âš ï¸ Unsloth not available, falling back to standard transformers...\")\n                    # Fallback to standard transformers\n                    self._load_standard_model(quantization_config)\n            else:\n                self._load_standard_model(quantization_config)\n            \n            # Set pad token if not available\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            print(f\"âœ… {self.model_config['description']} loaded successfully!\")\n            self.print_model_info()\n            \n        except Exception as e:\n            print(f\"âŒ Error loading model: {e}\")\n            raise\n    \n    def _load_standard_model(self, quantization_config):\n        \"\"\"Load model using standard transformers\"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.model_config['name'],\n            trust_remote_code=True,\n            padding_side='left'\n        )\n        \n        model_kwargs = {\n            'trust_remote_code': True,\n            'torch_dtype': torch.float16,\n            'low_cpu_mem_usage': True,\n            'device_map': 'auto'\n        }\n        \n        if quantization_config:\n            model_kwargs['quantization_config'] = quantization_config\n        \n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_config['name'],\n            **model_kwargs\n        )\n    \n    def print_model_info(self):\n        \"\"\"Print model and memory information\"\"\"\n        print(f\"\\nğŸ“Š Model Information:\")\n        print(f\"Model: {self.model_config['name']}\")\n        print(f\"Type: {self.model_config['type']}\")\n        print(f\"Context Length: {self.model_config['context_length']:,} tokens\")\n        print(f\"Quantization: {'4-bit' if self.use_quantization else 'Full precision'}\")\n        print(f\"Thinking Mode: {'Enabled' if self.use_thinking_mode else 'Disabled'}\")\n        print(f\"Tokenizer Vocab Size: {len(self.tokenizer):,}\")\n        \n        if torch.cuda.is_available():\n            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n    \n    def create_prompt(self, answer: str) -> str:\n        \"\"\"Create model-specific prompt for classification\"\"\"\n        strategy_descriptions = \"\"\"\nØ§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø·Ø¨ÙŠØ©:\n(1) Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª - Ø¥Ø¬Ø§Ø¨Ø§Øª ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆÙ…ÙˆØ§Ø±Ø¯ ÙˆØ­Ù‚Ø§Ø¦Ù‚ Ø·Ø¨ÙŠØ©\n(2) Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± - Ø¥Ø¬Ø§Ø¨Ø§Øª ØªÙ‚Ø¯Ù… Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª ÙˆØªØ¹Ù„ÙŠÙ…Ø§Øª ÙˆÙ†ØµØ§Ø¦Ø­ Ù…Ø­Ø¯Ø¯Ø©\n(3) Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø§Ø·ÙÙŠ - Ø¥Ø¬Ø§Ø¨Ø§Øª ØªÙ‚Ø¯Ù… Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø© ÙˆØ§Ù„Ø·Ù…Ø£Ù†ÙŠÙ†Ø© Ø£Ùˆ Ø£Ø´ÙƒØ§Ù„ Ø£Ø®Ø±Ù‰ Ù…Ù† Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø§Ø·ÙÙŠ\n\"\"\"\n        \n        # Enhanced prompt for DeepSeek R1 model with reasoning capabilities\n        if self.model_key == 'deepseek-r1':\n            base_prompt = f\"\"\"Ø£Ù†Øª Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù„Ø¯ÙŠÙƒ Ù‚Ø¯Ø±Ø§Øª ØªÙÙƒÙŠØ± Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚.\n\n{strategy_descriptions}\n\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØµÙ†ÙŠÙÙ‡Ø§:\n{answer}\n\nØªØ¹Ù„ÙŠÙ…Ø§Øª Ø®Ø§ØµØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…:\n1. Ø§Ø³ØªØ®Ø¯Ù… Ù‚Ø¯Ø±Ø§ØªÙƒ ÙÙŠ Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø¹Ù…Ù‚\n2. Ø­Ù„Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø·Ø¨ÙŠ ÙˆØ§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©\n3. ÙÙƒØ± ÙÙŠ Ø§Ù„Ù‡Ø¯Ù Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆÙ†ÙˆØ¹ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©\n4. Ø§Ø¹ØªØ¨Ø± Ø§Ù„ØªØ¯Ø§Ø®Ù„ Ø¨ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\n5. Ø§Ø´Ø±Ø­ Ù…Ù†Ø·Ù‚ ØªÙÙƒÙŠØ±Ùƒ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©\n6. Ø­Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙˆØ§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©\n7. ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ù‚Ø¯Ù… Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨\n\nØ§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:\n\"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„]\"\n\nÙ…Ø«Ø§Ù„: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØªÙˆØ¬ÙŠÙ‡ØŒ ÙØ§ÙƒØªØ¨: \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [1,2]\"\n\"\"\"\n        elif self.use_thinking_mode:\n            base_prompt = f\"\"\"Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¥Ù„Ù‰ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø£Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.\n\n{strategy_descriptions}\n\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØµÙ†ÙŠÙÙ‡Ø§:\n{answer}\n\nØªØ¹Ù„ÙŠÙ…Ø§Øª:\n1. ÙÙƒØ± Ø¨Ø¹Ù…Ù‚ ÙÙŠ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ­Ù„Ù„ ÙƒÙ„ Ø¬Ø§Ù†Ø¨ Ù…Ù†Ù‡Ø§\n2. Ø§Ø´Ø±Ø­ ØªÙÙƒÙŠØ±Ùƒ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©\n3. Ø­Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙˆØ§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø·Ø¨ÙŠØ©\n4. Ø§Ø±Ø¨Ø· Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ø¹ Ø§Ù„ØªØ¨Ø±ÙŠØ±\n5. Ø§Ø®ØªØ± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø£Ùˆ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ø£Ù†Ø³Ø¨ (ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©)\n6. ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ø§ÙƒØªØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ:\n   \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [1,2,3]\" (Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„)\n\nÙ…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙÙƒÙŠØ± ÙˆØ§Ù„ØªÙ†Ø³ÙŠÙ‚:\n- Ø­Ù„Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: \"Ø§Ù„ØµØ¯Ø§Ø¹ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø¥Ø¬Ù‡Ø§Ø¯. Ø­Ø§ÙˆÙ„ Ø§Ù„Ø±Ø§Ø­Ø©.\"\n- Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: \"Ø³Ø¨Ø¨ØŒ Ø¥Ø¬Ù‡Ø§Ø¯ØŒ Ø­Ø§ÙˆÙ„ Ø§Ù„Ø±Ø§Ø­Ø©\"\n- Ø§Ù„ØªÙÙƒÙŠØ±: Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªÙˆÙØ± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø³Ø¨Ø¨ (Ø§Ù„Ø¥Ø¬Ù‡Ø§Ø¯) ÙˆØªÙ‚Ø¯Ù… Ù†ØµÙŠØ­Ø© (Ø§Ù„Ø±Ø§Ø­Ø©)\n- Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [1,2]\n\"\"\"\n        else:\n            base_prompt = f\"\"\"Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¥Ù„Ù‰ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø£Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.\n\n{strategy_descriptions}\n\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØµÙ†ÙŠÙÙ‡Ø§:\n{answer}\n\nØªØ¹Ù„ÙŠÙ…Ø§Øª:\n1. Ø§Ù‚Ø±Ø£ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø¹Ù†Ø§ÙŠØ© ÙˆØ­Ù„Ù„ Ù…Ø­ØªÙˆØ§Ù‡Ø§\n2. Ø­Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø£Ùˆ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© (ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ÙˆØ§Ø­Ø¯Ø©)\n3. Ø§Ø´Ø±Ø­ Ø³Ø¨Ø¨ Ø§Ø®ØªÙŠØ§Ø±Ùƒ Ù„ÙƒÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©\n4. ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ø§ÙƒØªØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ:\n   \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [1,2,3]\" (Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„)\n\nÙ…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚:\n- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠØ© ÙÙ‚Ø·: \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [1]\"\n- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØªÙˆØ¬ÙŠÙ‡: \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [1,2]\"\n\"\"\"\n        \n        # Apply chat template for Qwen-based models (including Unsloth and DeepSeek)\n        if self.model_config['type'] == 'qwen':\n            messages = [{\"role\": \"user\", \"content\": base_prompt}]\n            try:\n                return self.tokenizer.apply_chat_template(\n                    messages, \n                    tokenize=False, \n                    add_generation_prompt=True,\n                    enable_thinking=self.use_thinking_mode if hasattr(self.tokenizer, 'enable_thinking') else False\n                )\n            except:\n                # Fallback for models that don't support thinking mode\n                return self.tokenizer.apply_chat_template(\n                    messages, \n                    tokenize=False, \n                    add_generation_prompt=True\n                )\n        elif self.model_config['type'] in ['llama']:\n            return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{base_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        elif self.model_config['type'] == 'gemma':\n            return f\"<start_of_turn>user\\n{base_prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n        elif self.model_config['type'] == 'phi':\n            return f\"<|user|>\\n{base_prompt}<|end|>\\n<|assistant|>\\n\"\n        else:\n            return f\"Human: {base_prompt}\\n\\nAssistant:\"\n    \n    def classify_answer(self, answer: str, max_new_tokens: int = 8000) -> List[str]:\n        \"\"\"\n        Classify Arabic medical answer into strategy categories\n        \"\"\"\n        prompt = self.create_prompt(answer)\n        \n        model_inputs = self.tokenizer(\n            prompt, \n            return_tensors=\"pt\", \n            truncation=True,\n            max_length=self.model_config['context_length'] - max_new_tokens\n        ).to(self.model.device)\n        \n        # Optimized generation parameters for different models\n        gen_kwargs = {\n            'max_new_tokens': max_new_tokens,\n            'temperature': 0.7,\n            'top_p': 0.9,\n            'top_k': 50,\n            'do_sample': True,\n            'pad_token_id': self.tokenizer.pad_token_id,\n            'eos_token_id': self.tokenizer.eos_token_id,\n            'repetition_penalty': 1.1\n        }\n        \n        # Model-specific optimizations\n        if self.model_key == 'deepseek-r1':\n            # DeepSeek R1 optimized parameters for reasoning\n            gen_kwargs.update({\n                'temperature': 0.3,  # Lower temperature for more focused reasoning\n                'top_p': 0.9,\n                'top_k': 40,\n                'repetition_penalty': 1.05\n            })\n        elif self.model_key == 'qwen2.5-unsloth':\n            # Unsloth Qwen2.5 optimized parameters\n            gen_kwargs.update({\n                'temperature': 0.6,\n                'top_p': 0.95,\n                'top_k': 30,\n                'repetition_penalty': 1.08\n            })\n        elif self.model_config['type'] == 'qwen':\n            gen_kwargs.update({\n                'temperature': 0.6,\n                'top_p': 0.95,\n                'top_k': 20\n            })\n        elif self.model_config['type'] == 'llama':\n            gen_kwargs.update({\n                'temperature': 0.8,\n                'top_p': 0.95\n            })\n        \n        with torch.no_grad():\n            generated_ids = self.model.generate(\n                **model_inputs,\n                **gen_kwargs\n            )\n        \n        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n        content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n        \n        strategies = self.extract_answer_strategies(content)\n        return strategies\n    \n    def extract_answer_strategies(self, response: str) -> List[str]:\n        \"\"\"Extract answer strategies from Arabic response text\"\"\"\n        patterns = [\n            r'Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:\\s*\\[([123,\\s]+)\\]',\n            r'Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª:\\s*\\[([123,\\s]+)\\]',\n            r'Ø§Ù„ØªØµÙ†ÙŠÙ:\\s*\\[([123,\\s]+)\\]',\n            r'Ø§Ù„Ù†ØªÙŠØ¬Ø©:\\s*\\[([123,\\s]+)\\]',\n            r'Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\\s*\\[([123,\\s]+)\\]',\n        ]\n        \n        for pattern in patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                strategies_str = match.group(1)\n                strategies = [strat.strip() for strat in strategies_str.split(',')]\n                return [strat for strat in strategies if strat in ['1', '2', '3']]\n        \n        english_patterns = [\n            r'Final Classification:\\s*\\[([123,\\s]+)\\]',\n            r'Strategies:\\s*\\[([123,\\s]+)\\]',\n            r'Classification:\\s*\\[([123,\\s]+)\\]',\n            r'Answer:\\s*\\[([123,\\s]+)\\]',\n        ]\n        \n        for pattern in english_patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                strategies_str = match.group(1)\n                strategies = [strat.strip() for strat in strategies_str.split(',')]\n                return [strat for strat in strategies if strat in ['1', '2', '3']]\n        \n        found_strategies = []\n        for strategy in ['1', '2', '3']:\n            if f'({strategy})' in response or f'[{strategy}]' in response or f' {strategy} ' in response:\n                found_strategies.append(strategy)\n        \n        return found_strategies if found_strategies else ['1']\n    \n    def process_test_dataset(self, df: pd.DataFrame, max_new_tokens: int = 8000, show_progress: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Process dataset for answer classification\n        \"\"\"\n        print(f\"ğŸš€ Starting Arabic Medical Answer Classification with {self.model_config['description']}...\")\n        print(f\"Dataset size: {len(df)} samples\")\n        print(\"-\" * 80)\n        \n        predictions = []\n        \n        for idx, row in df.iterrows():\n            if show_progress and idx % 10 == 0:\n                print(f\"Processing sample {idx+1}/{len(df)} - Current GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            \n            try:\n                strategies = self.classify_answer(row['answer'], max_new_tokens=max_new_tokens)\n                prediction_str = ', '.join(sorted(strategies))\n                predictions.append(prediction_str)\n                \n                if show_progress and idx < 3:\n                    print(f\"Sample {idx+1} prediction: {prediction_str}\")\n                    \n            except Exception as e:\n                print(f\"Error processing answer {idx}: {e}\")\n                predictions.append('1')\n            \n            if idx % 20 == 0:\n                self.cleanup_memory()\n        \n        result_df = pd.DataFrame({'prediction': predictions})\n        return result_df\n    \n    def cleanup_memory(self):\n        \"\"\"Clean up GPU memory\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    \n    @classmethod\n    def list_available_models(cls):\n        \"\"\"List all available models with descriptions\"\"\"\n        print(\"ğŸ“‹ Available Models:\")\n        print(\"-\" * 80)\n        \n        for key, config in cls.MODEL_CONFIGS.items():\n            print(f\"Key: '{key}'\")\n            print(f\"  Model: {config['name']}\")\n            print(f\"  Description: {config['description']}\")\n            print(f\"  Context Length: {config['context_length']:,} tokens\")\n            print(f\"  Type: {config['type']}\")\n            print()\n\ndef evaluate_model_on_training_data(\n    train_file_path: str,\n    model_key: str = 'qwen2.5-unsloth',\n    use_quantization: bool = True,\n    max_new_tokens: int = 8000\n) -> dict:\n    \"\"\"\n    Evaluate the model on the training dataset using Weighted F1 Score and Jaccard Score.\n\n    Args:\n        train_file_path: Path to the training dataset (TSV with 'answer' and 'final_AS' columns)\n        model_key: Model key from MODEL_CONFIGS (default: qwen2.5-unsloth)\n        use_quantization: Use 4-bit quantization\n        max_new_tokens: Maximum tokens to generate per answer\n\n    Returns:\n        dict: Dictionary containing evaluation metrics (Weighted F1 Score, Jaccard Score)\n    \"\"\"\n    \n    print(f\"ğŸš€ Initializing {model_key} for evaluation...\")\n    try:\n        classifier = ArabicMedicalAnswerClassifier(\n            model_key=model_key,\n            use_quantization=use_quantization\n        )\n    except Exception as e:\n        print(f\"âŒ Failed to initialize model: {e}\")\n        return None\n\n    # Load training dataset\n    try:\n        df = pd.read_csv(train_file_path, sep='\\t')\n        if 'answer' not in df.columns or 'final_AS' not in df.columns:\n            raise ValueError(\"Dataset must contain 'answer' and 'final_AS' columns\")\n        print(f\"âœ… Training dataset loaded: {len(df)} samples\")\n    except Exception as e:\n        print(f\"âŒ Error loading training dataset: {e}\")\n        return None\n\n    # Parse final_AS labels\n    def parse_labels(label):\n        if isinstance(label, str):\n            try:\n                if label.startswith('['):\n                    return ast.literal_eval(label)\n                else:\n                    return [strat.strip() for strat in label.split(',')]\n            except:\n                print(f\"Warning: Could not parse label '{label}', defaulting to ['1']\")\n                return ['1']\n        return label\n\n    df['final_AS'] = df['final_AS'].apply(parse_labels)\n\n    # Generate predictions\n    print(f\"ğŸš€ Generating predictions for {len(df)} samples...\")\n    predictions = classifier.process_test_dataset(df, max_new_tokens=max_new_tokens)\n\n    # Convert predictions and true labels to multi-label binary format\n    all_strategies = ['1', '2', '3']\n    y_true = []\n    y_pred = []\n\n    for true_labels, pred_labels in zip(df['final_AS'], predictions['prediction']):\n        true_vec = [1 if strat in true_labels else 0 for strat in all_strategies]\n        y_true.append(true_vec)\n        pred_strats = [strat.strip() for strat in pred_labels.split(',') if strat.strip() in all_strategies]\n        pred_vec = [1 if strat in pred_strats else 0 for strat in all_strategies]\n        y_pred.append(pred_vec)\n\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate metrics\n    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n    jaccard = jaccard_score(y_true, y_pred, average='samples')\n\n    # Print results\n    print(\"\\nğŸ“Š Evaluation Results:\")\n    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n    print(f\"Jaccard Score (samples): {jaccard:.4f}\")\n\n    # Detailed per-strategy metrics\n    print(\"\\nğŸ“‹ Per-Strategy Metrics:\")\n    for i, strat in enumerate(all_strategies):\n        strat_f1 = f1_score(y_true[:, i], y_pred[:, i])\n        strat_jaccard = jaccard_score(y_true[:, i], y_pred[:, i])\n        print(f\"Strategy {strat}:\")\n        print(f\"  F1 Score: {strat_f1:.4f}\")\n        print(f\"  Jaccard Score: {strat_jaccard:.4f}\")\n\n    # Cleanup memory\n    classifier.cleanup_memory()\n\n    return {\n        'weighted_f1': weighted_f1,\n        'jaccard_score': jaccard,\n        'per_strategy_f1': {strat: f1_score(y_true[:, i], y_pred[:, i]) for i, strat in enumerate(all_strategies)},\n        'per_strategy_jaccard': {strat: jaccard_score(y_true[:, i], y_pred[:, i]) for i, strat in enumerate(all_strategies)}\n    }\n\ndef compare_all_models(train_file_path: str, use_quantization: bool = True, max_new_tokens: int = 6000):\n    \"\"\"\n    Compare performance of all available models on the training dataset\n    \"\"\"\n    models_to_test = ['qwen2.5-unsloth', 'deepseek-r1']\n    results = {}\n    \n    print(\"ğŸš€ Starting comprehensive model comparison...\")\n    print(\"=\" * 80)\n    \n    for model_key in models_to_test:\n        print(f\"\\nğŸ”„ Testing {model_key}...\")\n        try:\n            result = evaluate_model_on_training_data(\n                train_file_path=train_file_path,\n                model_key=model_key,\n                use_quantization=use_quantization,\n                max_new_tokens=max_new_tokens\n            )\n            if result:\n                results[model_key] = result\n            \n            # Clear memory between models\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n        except Exception as e:\n            print(f\"âŒ Error testing {model_key}: {e}\")\n            continue\n    \n    # Print comparison results\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ğŸ“Š MODEL COMPARISON RESULTS\")\n    print(\"=\" * 80)\n    \n    for model_key, result in results.items():\n        print(f\"\\n{model_key.upper()}:\")\n        print(f\"  Weighted F1 Score: {result['weighted_f1']:.4f}\")\n        print(f\"  Jaccard Score: {result['jaccard_score']:.4f}\")\n    \n    # Find best model\n    if results:\n        best_model = max(results.keys(), key=lambda k: results[k]['weighted_f1'])\n        print(f\"\\nğŸ† BEST MODEL: {best_model}\")\n        print(f\"   Weighted F1: {results[best_model]['weighted_f1']:.4f}\")\n        print(f\"   Jaccard Score: {results[best_model]['jaccard_score']:.4f}\")\n    \n    return results\n\nif __name__ == \"__main__\":\n    # Update this path to your training dataset\n    TRAIN_DATASET_PATH = '/kaggle/input/train-dataset/Train_Dev.tsv'  # Replace with actual path\n\n    # List available models\n    ArabicMedicalAnswerClassifier.list_available_models()\n\n    # Evaluate with Unsloth Qwen2.5 (recommended for speed)\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Testing Unsloth Qwen2.5-7B-Instruct\")\n    print(\"=\" * 50)\n    results_unsloth = evaluate_model_on_training_data(\n        train_file_path=TRAIN_DATASET_PATH,\n        model_key='qwen2.5-unsloth',\n        use_quantization=True,\n        max_new_tokens=6000\n    )\n\n    # Evaluate with DeepSeek R1 (recommended for reasoning)\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Testing DeepSeek-R1-Distill-Qwen-7B\")\n    print(\"=\" * 50)\n    results_deepseek = evaluate_model_on_training_data(\n        train_file_path=TRAIN_DATASET_PATH,\n        model_key='deepseek-r1',\n        use_quantization=True,\n        max_new_tokens=6000\n    )\n\n    # Uncomment to run comprehensive comparison\n    # comprehensive_results = compare_all_models(\n    #     train_file_path=TRAIN_DATASET_PATH,\n    #     use_quantization=True,\n    #     max_new_tokens=6000\n    # )\n\n    # Print summary if both models were tested\n    if results_unsloth and results_deepseek:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ğŸ“Š COMPARISON SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"Unsloth Qwen2.5-7B:\")\n        print(f\"  Weighted F1: {results_unsloth['weighted_f1']:.4f}\")\n        print(f\"  Jaccard Score: {results_unsloth['jaccard_score']:.4f}\")\n        print(f\"  Advantage: Faster inference, optimized performance\")\n        \n        print(f\"\\nDeepSeek R1 Distill Qwen-7B:\")\n        print(f\"  Weighted F1: {results_deepseek['weighted_f1']:.4f}\")\n        print(f\"  Jaccard Score: {results_deepseek['jaccard_score']:.4f}\")\n        print(f\"  Advantage: Advanced reasoning capabilities\")\n        \n        # Determine winner\n        if results_unsloth['weighted_f1'] > results_deepseek['weighted_f1']:\n            print(f\"\\nğŸ† Winner: Unsloth Qwen2.5-7B (Better F1 Score)\")\n        elif results_deepseek['weighted_f1'] > results_unsloth['weighted_f1']:\n            print(f\"\\nğŸ† Winner: DeepSeek R1 (Better F1 Score)\")\n        else:\n            print(f\"\\nğŸ¤ Tie: Both models perform equally well\")\n        \n        print(f\"\\nğŸ’¡ Recommendation:\")\n        print(f\"   - Use 'qwen2.5-unsloth' for faster inference and production deployment\")\n        print(f\"   - Use 'deepseek-r1' for complex reasoning tasks and research\")\n\n# Additional utility functions for model management and optimization\n\ndef install_unsloth():\n    \"\"\"\n    Install Unsloth for optimized inference (run this once)\n    \"\"\"\n    install_commands = [\n        \"pip install unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\",\n        \"pip install --no-deps trl peft accelerate bitsandbytes\"\n    ]\n    \n    print(\"ğŸ“¦ To install Unsloth for optimized inference, run these commands:\")\n    for cmd in install_commands:\n        print(f\"   {cmd}\")\n    \n    print(\"\\nâš ï¸  Note: Restart your runtime after installation!\")\n\ndef benchmark_inference_speed(model_key: str, sample_answers: List[str], use_quantization: bool = True):\n    \"\"\"\n    Benchmark inference speed for a specific model\n    \n    Args:\n        model_key: Model to benchmark\n        sample_answers: List of sample answers to test\n        use_quantization: Whether to use quantization\n    \"\"\"\n    import time\n    \n    print(f\"ğŸš€ Benchmarking {model_key} inference speed...\")\n    \n    classifier = ArabicMedicalAnswerClassifier(\n        model_key=model_key,\n        use_quantization=use_quantization\n    )\n    \n    # Warm-up run\n    classifier.classify_answer(sample_answers[0], max_new_tokens=2000)\n    \n    # Benchmark runs\n    start_time = time.time()\n    total_tokens = 0\n    \n    for answer in sample_answers:\n        result = classifier.classify_answer(answer, max_new_tokens=2000)\n        total_tokens += len(classifier.tokenizer.encode(answer))\n    \n    end_time = time.time()\n    total_time = end_time - start_time\n    \n    print(f\"â±ï¸  Benchmark Results for {model_key}:\")\n    print(f\"   Total Time: {total_time:.2f} seconds\")\n    print(f\"   Samples Processed: {len(sample_answers)}\")\n    print(f\"   Average Time per Sample: {total_time/len(sample_answers):.2f} seconds\")\n    print(f\"   Total Input Tokens: {total_tokens}\")\n    print(f\"   Tokens per Second: {total_tokens/total_time:.2f}\")\n    \n    classifier.cleanup_memory()\n    return {\n        'model_key': model_key,\n        'total_time': total_time,\n        'samples': len(sample_answers),\n        'avg_time_per_sample': total_time/len(sample_answers),\n        'tokens_per_second': total_tokens/total_time\n    }\n\ndef create_optimized_pipeline(model_key: str = 'qwen2.5-unsloth', batch_size: int = 4):\n    \"\"\"\n    Create an optimized inference pipeline for batch processing\n    \n    Args:\n        model_key: Model to use for the pipeline\n        batch_size: Number of samples to process in each batch\n    \n    Returns:\n        Optimized classifier instance\n    \"\"\"\n    print(f\"ğŸ”§ Creating optimized pipeline with {model_key}...\")\n    \n    classifier = ArabicMedicalAnswerClassifier(\n        model_key=model_key,\n        use_quantization=True,\n        use_thinking_mode=False  # Disable for faster inference\n    )\n    \n    # Set model to evaluation mode for inference\n    classifier.model.eval()\n    \n    # Enable torch.compile for PyTorch 2.0+ (if available)\n    try:\n        import torch._dynamo\n        classifier.model = torch.compile(classifier.model, mode=\"reduce-overhead\")\n        print(\"âœ… Torch compile enabled for faster inference\")\n    except:\n        print(\"âš ï¸  Torch compile not available, using standard inference\")\n    \n    print(f\"âœ… Optimized pipeline ready with batch size: {batch_size}\")\n    return classifier\n\n# Example usage and testing functions\n\ndef test_new_models():\n    \"\"\"\n    Test the newly added models with sample data\n    \"\"\"\n    sample_answers = [\n        \"ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„ØµØ¯Ø§Ø¹ Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø¥Ø¬Ù‡Ø§Ø¯ Ø£Ùˆ Ù‚Ù„Ø© Ø§Ù„Ù†ÙˆÙ…. Ø£Ù†ØµØ­Ùƒ Ø¨Ø§Ù„Ø±Ø§Ø­Ø© ÙˆØ´Ø±Ø¨ Ø§Ù„Ù…Ø§Ø¡.\",\n        \"Ù„Ø§ ØªÙ‚Ù„Ù‚ØŒ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø·Ø¨ÙŠØ¹ÙŠØ©. Ø³ØªØ´Ø¹Ø± Ø¨ØªØ­Ø³Ù† Ù‚Ø±ÙŠØ¨Ø§Ù‹.\",\n        \"ÙÙŠØªØ§Ù…ÙŠÙ† Ø¯ Ù…Ù‡Ù… Ù„ØµØ­Ø© Ø§Ù„Ø¹Ø¸Ø§Ù… ÙˆÙŠÙ…ÙƒÙ† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„ÙŠÙ‡ Ù…Ù† Ø£Ø´Ø¹Ø© Ø§Ù„Ø´Ù…Ø³ ÙˆØ§Ù„Ø£Ø·Ø¹Ù…Ø© Ø§Ù„Ù…Ø¯Ø¹Ù…Ø©.\"\n    ]\n    \n    models_to_test = ['qwen2.5-unsloth']  #deepseek-r1\n    \n    for model_key in models_to_test:\n        print(f\"\\n{'='*50}\")\n        print(f\"Testing {model_key}\")\n        print(f\"{'='*50}\")\n        \n        try:\n            classifier = ArabicMedicalAnswerClassifier(\n                model_key=model_key,\n                use_quantization=True\n            )\n            \n            for i, answer in enumerate(sample_answers):\n                result = classifier.classify_answer(answer, max_new_tokens=3000)\n                print(f\"\\nSample {i+1}: {answer[:50]}...\")\n                print(f\"Classification: {result}\")\n            \n            classifier.cleanup_memory()\n            \n        except Exception as e:\n            print(f\"âŒ Error testing {model_key}: {e}\")\n\n# Run tests if this is the main module\nif __name__ == \"__main__\":\n    # Show installation instructions for Unsloth\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸš€ INSTALLATION GUIDE\")\n    print(\"=\"*80)\n    install_unsloth()\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ§ª TESTING NEW MODELS\")\n    print(\"=\"*80)\n    # Uncomment to test the new models with sample data\n    # test_new_models()\n    \n    print(\"\\nâœ… Setup complete! The classifier now supports:\")\n    print(\"   - unsloth/Qwen2.5-7B-Instruct (Optimized for speed)\")\n    print(\"   - deepseek-ai/DeepSeek-R1-Distill-Qwen-7B (Advanced reasoning)\")\n    print(\"\\nğŸ’¡ Use 'qwen2.5-unsloth' as default for best performance!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-27T08:35:32.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}