{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12668834,"sourceType":"datasetVersion","datasetId":8005928}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:34:43.274725Z","iopub.execute_input":"2025-07-27T08:34:43.275031Z","iopub.status.idle":"2025-07-27T08:34:43.291282Z","shell.execute_reply.started":"2025-07-27T08:34:43.275005Z","shell.execute_reply":"2025-07-27T08:34:43.290627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers>=4.51.0 torch torchvision torchaudio accelerate bitsandbytes -q\n!pip install sentencepiece protobuf -q\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport gc\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\n# Check GPU availability\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:34:43.292522Z","iopub.execute_input":"2025-07-27T08:34:43.292762Z","execution_failed":"2025-07-27T08:35:32.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nimport re\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom typing import List, Dict, Tuple, Any, Optional\nfrom sklearn.metrics import f1_score, jaccard_score\nimport ast\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass ArabicMedicalAnswerClassifier:\n    \"\"\"\n    Arabic Medical Answer Classifier supporting multiple LLM models including Unsloth and DeepSeek\n    \"\"\"\n    \n    # Model configurations with their specific settings\n    MODEL_CONFIGS = {\n        'qwen2': {\n            'name': 'Qwen/Qwen2-7B-Instruct',\n            'type': 'qwen',\n            'context_length': 32768,\n            'description': 'Qwen2 7B - Strong multilingual model'\n        },\n        'qwen2.5-unsloth': {\n            'name': 'unsloth/Qwen2.5-7B-Instruct',\n            'type': 'qwen',\n            'context_length': 131072,\n            'description': 'Unsloth Qwen2.5 7B - Optimized for inference speed'\n        },\n        'deepseek-r1': {\n            'name': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n            'type': 'qwen',\n            'context_length': 131072,\n            'description': 'DeepSeek R1 Distill Qwen 7B - Advanced reasoning model'\n        },\n        'llama3': {\n            'name': 'meta-llama/Llama-3.1-8B-Instruct',\n            'type': 'llama',\n            'context_length': 128000,\n            'description': 'Llama 3.1 8B - Meta\\'s latest model'\n        },\n        'gemma2': {\n            'name': 'google/gemma-2-9b-it',\n            'type': 'gemma',\n            'context_length': 8192,\n            'description': 'Gemma 2 9B - Google\\'s instruction-tuned model'\n        },\n        'phi3': {\n            'name': 'microsoft/Phi-3-medium-14b-instruct',\n            'type': 'phi',\n            'context_length': 128000,\n            'description': 'Phi-3 Medium - Microsoft\\'s efficient model'\n        }\n    }\n    \n    def __init__(self, model_key: str = 'qwen2.5-unsloth', use_quantization: bool = True, use_thinking_mode: bool = True):\n        \"\"\"\n        Initialize classifier with specified model\n        \n        Args:\n            model_key: Key from MODEL_CONFIGS (default: qwen2.5-unsloth for better performance)\n            use_quantization: Whether to use 4-bit quantization\n            use_thinking_mode: Enable thinking mode for supported models\n        \"\"\"\n        if model_key not in self.MODEL_CONFIGS:\n            raise ValueError(f\"Model '{model_key}' not supported. Available models: {list(self.MODEL_CONFIGS.keys())}\")\n        \n        self.model_config = self.MODEL_CONFIGS[model_key]\n        self.model_key = model_key\n        self.use_quantization = use_quantization\n        self.use_thinking_mode = use_thinking_mode\n        self.tokenizer = None\n        self.model = None\n        self.answer_strategies = {\n            '1': 'Information (answers providing information, resources, etc.)',\n            '2': 'Direct Guidance (answers providing suggestions, instructions, or advice)',\n            '3': 'Emotional Support (answers providing approval, reassurance, or other forms of emotional support)'\n        }\n        self.load_model()\n    \n    def load_model(self):\n        \"\"\"Load the specified model with appropriate configurations\"\"\"\n        print(f\"Loading {self.model_config['description']}...\")\n        print(f\"Model: {self.model_config['name']}\")\n        \n        quantization_config = None\n        if self.use_quantization:\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\"\n            )\n        \n        try:\n            # Special handling for Unsloth models\n            if 'unsloth' in self.model_config['name']:\n                print(\"üöÄ Loading Unsloth optimized model...\")\n                try:\n                    # Try to use Unsloth's FastLanguageModel if available\n                    from unsloth import FastLanguageModel\n                    self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n                        model_name=self.model_config['name'],\n                        max_seq_length=self.model_config['context_length'],\n                        dtype=torch.float16,\n                        load_in_4bit=self.use_quantization,\n                    )\n                    FastLanguageModel.for_inference(self.model)  # Enable native 2x faster inference\n                    print(\"‚úÖ Unsloth FastLanguageModel loaded successfully!\")\n                except ImportError:\n                    print(\"‚ö†Ô∏è Unsloth not available, falling back to standard transformers...\")\n                    # Fallback to standard transformers\n                    self._load_standard_model(quantization_config)\n            else:\n                self._load_standard_model(quantization_config)\n            \n            # Set pad token if not available\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            print(f\"‚úÖ {self.model_config['description']} loaded successfully!\")\n            self.print_model_info()\n            \n        except Exception as e:\n            print(f\"‚ùå Error loading model: {e}\")\n            raise\n    \n    def _load_standard_model(self, quantization_config):\n        \"\"\"Load model using standard transformers\"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.model_config['name'],\n            trust_remote_code=True,\n            padding_side='left'\n        )\n        \n        model_kwargs = {\n            'trust_remote_code': True,\n            'torch_dtype': torch.float16,\n            'low_cpu_mem_usage': True,\n            'device_map': 'auto'\n        }\n        \n        if quantization_config:\n            model_kwargs['quantization_config'] = quantization_config\n        \n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_config['name'],\n            **model_kwargs\n        )\n    \n    def print_model_info(self):\n        \"\"\"Print model and memory information\"\"\"\n        print(f\"\\nüìä Model Information:\")\n        print(f\"Model: {self.model_config['name']}\")\n        print(f\"Type: {self.model_config['type']}\")\n        print(f\"Context Length: {self.model_config['context_length']:,} tokens\")\n        print(f\"Quantization: {'4-bit' if self.use_quantization else 'Full precision'}\")\n        print(f\"Thinking Mode: {'Enabled' if self.use_thinking_mode else 'Disabled'}\")\n        print(f\"Tokenizer Vocab Size: {len(self.tokenizer):,}\")\n        \n        if torch.cuda.is_available():\n            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n    \n    def create_prompt(self, answer: str) -> str:\n        \"\"\"Create model-specific prompt for classification\"\"\"\n        strategy_descriptions = \"\"\"\nÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿ∑ÿ®Ÿäÿ©:\n(1) ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ - ÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿ™ŸÇÿØŸÖ ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸàŸÖŸàÿßÿ±ÿØ Ÿàÿ≠ŸÇÿßÿ¶ŸÇ ÿ∑ÿ®Ÿäÿ©\n(2) ÿßŸÑÿ™Ÿàÿ¨ŸäŸá ÿßŸÑŸÖÿ®ÿßÿ¥ÿ± - ÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿ™ŸÇÿØŸÖ ÿßŸÇÿ™ÿ±ÿßÿ≠ÿßÿ™ Ÿàÿ™ÿπŸÑŸäŸÖÿßÿ™ ŸàŸÜÿµÿßÿ¶ÿ≠ ŸÖÿ≠ÿØÿØÿ©\n(3) ÿßŸÑÿØÿπŸÖ ÿßŸÑÿπÿßÿ∑ŸÅŸä - ÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿ™ŸÇÿØŸÖ ÿßŸÑŸÖŸàÿßŸÅŸÇÿ© ŸàÿßŸÑÿ∑ŸÖÿ£ŸÜŸäŸÜÿ© ÿ£Ÿà ÿ£ÿ¥ŸÉÿßŸÑ ÿ£ÿÆÿ±Ÿâ ŸÖŸÜ ÿßŸÑÿØÿπŸÖ ÿßŸÑÿπÿßÿ∑ŸÅŸä\n\"\"\"\n        \n        # Enhanced prompt for DeepSeek R1 model with reasoning capabilities\n        if self.model_key == 'deepseek-r1':\n            base_prompt = f\"\"\"ÿ£ŸÜÿ™ ŸÜŸÖŸàÿ∞ÿ¨ ÿ∞ŸÉŸä ŸÖÿ™ÿÆÿµÿµ ŸÅŸä ÿ™ÿ≠ŸÑŸäŸÑ Ÿàÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿßŸÑÿ∑ÿ®Ÿäÿ© ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©. ŸÑÿØŸäŸÉ ŸÇÿØÿ±ÿßÿ™ ÿ™ŸÅŸÉŸäÿ± ŸÖÿ™ŸÇÿØŸÖÿ© Ÿàÿ™ÿ≠ŸÑŸäŸÑ ÿπŸÖŸäŸÇ.\n\n{strategy_descriptions}\n\nÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÖÿ±ÿßÿØ ÿ™ÿµŸÜŸäŸÅŸáÿß:\n{answer}\n\nÿ™ÿπŸÑŸäŸÖÿßÿ™ ÿÆÿßÿµÿ© ŸÑŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖÿ™ŸÇÿØŸÖ:\n1. ÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÇÿØÿ±ÿßÿ™ŸÉ ŸÅŸä ÿßŸÑÿ™ŸÅŸÉŸäÿ± ÿßŸÑŸÖŸÜÿ∑ŸÇŸä ŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿπŸÖŸÇ\n2. ÿ≠ŸÑŸÑ ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑÿ∑ÿ®Ÿä ŸàÿßŸÑŸÖŸÅÿßŸáŸäŸÖ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖÿ©\n3. ŸÅŸÉÿ± ŸÅŸä ÿßŸÑŸáÿØŸÅ ŸÖŸÜ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸàŸÜŸàÿπ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ© ÿßŸÑŸÖŸÇÿØŸÖÿ©\n4. ÿßÿπÿ™ÿ®ÿ± ÿßŸÑÿ™ÿØÿßÿÆŸÑ ÿ®ŸäŸÜ ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ©\n5. ÿßÿ¥ÿ±ÿ≠ ŸÖŸÜÿ∑ŸÇ ÿ™ŸÅŸÉŸäÿ±ŸÉ ÿÆÿ∑Ÿàÿ© ÿ®ÿÆÿ∑Ÿàÿ©\n6. ÿ≠ÿØÿØ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸÅÿ™ÿßÿ≠Ÿäÿ© ŸàÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™ ÿßŸÑŸÑÿ∫ŸàŸäÿ©\n7. ŸÅŸä ÿßŸÑŸÜŸáÿßŸäÿ©ÿå ŸÇÿØŸÖ ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä ÿ®ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®\n\nÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®:\n\"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [ÿ£ÿ±ŸÇÿßŸÖ ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ŸÖŸÅÿµŸàŸÑÿ© ÿ®ŸÅŸàÿßÿµŸÑ]\"\n\nŸÖÿ´ÿßŸÑ: ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÖÿπŸÑŸàŸÖÿßÿ™ Ÿàÿ™Ÿàÿ¨ŸäŸáÿå ŸÅÿßŸÉÿ™ÿ®: \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [1,2]\"\n\"\"\"\n        elif self.use_thinking_mode:\n            base_prompt = f\"\"\"ÿ£ŸÜÿ™ ÿÆÿ®Ÿäÿ± ŸÅŸä ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿßŸÑÿ∑ÿ®Ÿäÿ© ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©. ŸÖŸáŸÖÿ™ŸÉ ŸáŸä ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿ™ÿßŸÑŸäÿ© ÿ•ŸÑŸâ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ÿ£Ÿà ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ÿßŸÑŸÖÿ≠ÿØÿØÿ©.\n\n{strategy_descriptions}\n\nÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÖÿ±ÿßÿØ ÿ™ÿµŸÜŸäŸÅŸáÿß:\n{answer}\n\nÿ™ÿπŸÑŸäŸÖÿßÿ™:\n1. ŸÅŸÉÿ± ÿ®ÿπŸÖŸÇ ŸÅŸä ŸÖÿ≠ÿ™ŸàŸâ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© Ÿàÿ≠ŸÑŸÑ ŸÉŸÑ ÿ¨ÿßŸÜÿ® ŸÖŸÜŸáÿß\n2. ÿßÿ¥ÿ±ÿ≠ ÿ™ŸÅŸÉŸäÿ±ŸÉ ÿÆÿ∑Ÿàÿ© ÿ®ÿÆÿ∑Ÿàÿ©\n3. ÿ≠ÿØÿØ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸÅÿ™ÿßÿ≠Ÿäÿ© ŸàÿßŸÑŸÖŸÅÿßŸáŸäŸÖ ÿßŸÑÿ∑ÿ®Ÿäÿ©\n4. ÿßÿ±ÿ®ÿ∑ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ© ŸÖÿπ ÿßŸÑÿ™ÿ®ÿ±Ÿäÿ±\n5. ÿßÿÆÿ™ÿ± ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ÿ£Ÿà ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ÿßŸÑÿ£ŸÜÿ≥ÿ® (ŸäŸÖŸÉŸÜ ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ©)\n6. ŸÅŸä ÿßŸÑŸÜŸáÿßŸäÿ©ÿå ÿßŸÉÿ™ÿ® ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑÿ™ÿßŸÑŸä:\n   \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [1,2,3]\" (ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ© ŸÖŸÅÿµŸàŸÑÿ© ÿ®ŸÅŸàÿßÿµŸÑ)\n\nŸÖÿ´ÿßŸÑ ÿπŸÑŸâ ÿßŸÑÿ™ŸÅŸÉŸäÿ± ŸàÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ:\n- ÿ≠ŸÑŸÑ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©: \"ÿßŸÑÿµÿØÿßÿπ ŸÇÿØ ŸäŸÉŸàŸÜ ÿ®ÿ≥ÿ®ÿ® ÿßŸÑÿ•ÿ¨ŸáÿßÿØ. ÿ≠ÿßŸàŸÑ ÿßŸÑÿ±ÿßÿ≠ÿ©.\"\n- ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸÅÿ™ÿßÿ≠Ÿäÿ©: \"ÿ≥ÿ®ÿ®ÿå ÿ•ÿ¨ŸáÿßÿØÿå ÿ≠ÿßŸàŸÑ ÿßŸÑÿ±ÿßÿ≠ÿ©\"\n- ÿßŸÑÿ™ŸÅŸÉŸäÿ±: ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ™ŸàŸÅÿ± ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿπŸÜ ÿ≥ÿ®ÿ® (ÿßŸÑÿ•ÿ¨ŸáÿßÿØ) Ÿàÿ™ŸÇÿØŸÖ ŸÜÿµŸäÿ≠ÿ© (ÿßŸÑÿ±ÿßÿ≠ÿ©)\n- ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [1,2]\n\"\"\"\n        else:\n            base_prompt = f\"\"\"ÿ£ŸÜÿ™ ÿÆÿ®Ÿäÿ± ŸÅŸä ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿßŸÑÿ∑ÿ®Ÿäÿ© ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©. ŸÖŸáŸÖÿ™ŸÉ ŸáŸä ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿ™ÿßŸÑŸäÿ© ÿ•ŸÑŸâ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ÿ£Ÿà ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ÿßŸÑŸÖÿ≠ÿØÿØÿ©.\n\n{strategy_descriptions}\n\nÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÖÿ±ÿßÿØ ÿ™ÿµŸÜŸäŸÅŸáÿß:\n{answer}\n\nÿ™ÿπŸÑŸäŸÖÿßÿ™:\n1. ÿßŸÇÿ±ÿ£ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿπŸÜÿßŸäÿ© Ÿàÿ≠ŸÑŸÑ ŸÖÿ≠ÿ™ŸàÿßŸáÿß\n2. ÿ≠ÿØÿØ ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ÿ£Ÿà ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ© (ŸäŸÖŸÉŸÜ ÿ£ŸÜ ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© Ÿàÿßÿ≠ÿØÿ©)\n3. ÿßÿ¥ÿ±ÿ≠ ÿ≥ÿ®ÿ® ÿßÿÆÿ™Ÿäÿßÿ±ŸÉ ŸÑŸÉŸÑ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ©\n4. ŸÅŸä ÿßŸÑŸÜŸáÿßŸäÿ©ÿå ÿßŸÉÿ™ÿ® ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑÿ™ÿßŸÑŸä:\n   \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [1,2,3]\" (ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ© ŸÖŸÅÿµŸàŸÑÿ© ÿ®ŸÅŸàÿßÿµŸÑ)\n\nŸÖÿ´ÿßŸÑ ÿπŸÑŸâ ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ:\n- ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸÖÿπŸÑŸàŸÖÿßÿ™Ÿäÿ© ŸÅŸÇÿ∑: \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [1]\"\n- ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÖÿπŸÑŸàŸÖÿßÿ™ Ÿàÿ™Ÿàÿ¨ŸäŸá: \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [1,2]\"\n\"\"\"\n        \n        # Apply chat template for Qwen-based models (including Unsloth and DeepSeek)\n        if self.model_config['type'] == 'qwen':\n            messages = [{\"role\": \"user\", \"content\": base_prompt}]\n            try:\n                return self.tokenizer.apply_chat_template(\n                    messages, \n                    tokenize=False, \n                    add_generation_prompt=True,\n                    enable_thinking=self.use_thinking_mode if hasattr(self.tokenizer, 'enable_thinking') else False\n                )\n            except:\n                # Fallback for models that don't support thinking mode\n                return self.tokenizer.apply_chat_template(\n                    messages, \n                    tokenize=False, \n                    add_generation_prompt=True\n                )\n        elif self.model_config['type'] in ['llama']:\n            return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{base_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        elif self.model_config['type'] == 'gemma':\n            return f\"<start_of_turn>user\\n{base_prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n        elif self.model_config['type'] == 'phi':\n            return f\"<|user|>\\n{base_prompt}<|end|>\\n<|assistant|>\\n\"\n        else:\n            return f\"Human: {base_prompt}\\n\\nAssistant:\"\n    \n    def classify_answer(self, answer: str, max_new_tokens: int = 8000) -> List[str]:\n        \"\"\"\n        Classify Arabic medical answer into strategy categories\n        \"\"\"\n        prompt = self.create_prompt(answer)\n        \n        model_inputs = self.tokenizer(\n            prompt, \n            return_tensors=\"pt\", \n            truncation=True,\n            max_length=self.model_config['context_length'] - max_new_tokens\n        ).to(self.model.device)\n        \n        # Optimized generation parameters for different models\n        gen_kwargs = {\n            'max_new_tokens': max_new_tokens,\n            'temperature': 0.7,\n            'top_p': 0.9,\n            'top_k': 50,\n            'do_sample': True,\n            'pad_token_id': self.tokenizer.pad_token_id,\n            'eos_token_id': self.tokenizer.eos_token_id,\n            'repetition_penalty': 1.1\n        }\n        \n        # Model-specific optimizations\n        if self.model_key == 'deepseek-r1':\n            # DeepSeek R1 optimized parameters for reasoning\n            gen_kwargs.update({\n                'temperature': 0.3,  # Lower temperature for more focused reasoning\n                'top_p': 0.9,\n                'top_k': 40,\n                'repetition_penalty': 1.05\n            })\n        elif self.model_key == 'qwen2.5-unsloth':\n            # Unsloth Qwen2.5 optimized parameters\n            gen_kwargs.update({\n                'temperature': 0.6,\n                'top_p': 0.95,\n                'top_k': 30,\n                'repetition_penalty': 1.08\n            })\n        elif self.model_config['type'] == 'qwen':\n            gen_kwargs.update({\n                'temperature': 0.6,\n                'top_p': 0.95,\n                'top_k': 20\n            })\n        elif self.model_config['type'] == 'llama':\n            gen_kwargs.update({\n                'temperature': 0.8,\n                'top_p': 0.95\n            })\n        \n        with torch.no_grad():\n            generated_ids = self.model.generate(\n                **model_inputs,\n                **gen_kwargs\n            )\n        \n        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n        content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n        \n        strategies = self.extract_answer_strategies(content)\n        return strategies\n    \n    def extract_answer_strategies(self, response: str) -> List[str]:\n        \"\"\"Extract answer strategies from Arabic response text\"\"\"\n        patterns = [\n            r'ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä:\\s*\\[([123,\\s]+)\\]',\n            r'ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™:\\s*\\[([123,\\s]+)\\]',\n            r'ÿßŸÑÿ™ÿµŸÜŸäŸÅ:\\s*\\[([123,\\s]+)\\]',\n            r'ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©:\\s*\\[([123,\\s]+)\\]',\n            r'ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\\s*\\[([123,\\s]+)\\]',\n        ]\n        \n        for pattern in patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                strategies_str = match.group(1)\n                strategies = [strat.strip() for strat in strategies_str.split(',')]\n                return [strat for strat in strategies if strat in ['1', '2', '3']]\n        \n        english_patterns = [\n            r'Final Classification:\\s*\\[([123,\\s]+)\\]',\n            r'Strategies:\\s*\\[([123,\\s]+)\\]',\n            r'Classification:\\s*\\[([123,\\s]+)\\]',\n            r'Answer:\\s*\\[([123,\\s]+)\\]',\n        ]\n        \n        for pattern in english_patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                strategies_str = match.group(1)\n                strategies = [strat.strip() for strat in strategies_str.split(',')]\n                return [strat for strat in strategies if strat in ['1', '2', '3']]\n        \n        found_strategies = []\n        for strategy in ['1', '2', '3']:\n            if f'({strategy})' in response or f'[{strategy}]' in response or f' {strategy} ' in response:\n                found_strategies.append(strategy)\n        \n        return found_strategies if found_strategies else ['1']\n    \n    def process_test_dataset(self, df: pd.DataFrame, max_new_tokens: int = 8000, show_progress: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Process dataset for answer classification\n        \"\"\"\n        print(f\"üöÄ Starting Arabic Medical Answer Classification with {self.model_config['description']}...\")\n        print(f\"Dataset size: {len(df)} samples\")\n        print(\"-\" * 80)\n        \n        predictions = []\n        \n        for idx, row in df.iterrows():\n            if show_progress and idx % 10 == 0:\n                print(f\"Processing sample {idx+1}/{len(df)} - Current GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            \n            try:\n                strategies = self.classify_answer(row['answer'], max_new_tokens=max_new_tokens)\n                prediction_str = ', '.join(sorted(strategies))\n                predictions.append(prediction_str)\n                \n                if show_progress and idx < 3:\n                    print(f\"Sample {idx+1} prediction: {prediction_str}\")\n                    \n            except Exception as e:\n                print(f\"Error processing answer {idx}: {e}\")\n                predictions.append('1')\n            \n            if idx % 20 == 0:\n                self.cleanup_memory()\n        \n        result_df = pd.DataFrame({'prediction': predictions})\n        return result_df\n    \n    def cleanup_memory(self):\n        \"\"\"Clean up GPU memory\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    \n    @classmethod\n    def list_available_models(cls):\n        \"\"\"List all available models with descriptions\"\"\"\n        print(\"üìã Available Models:\")\n        print(\"-\" * 80)\n        \n        for key, config in cls.MODEL_CONFIGS.items():\n            print(f\"Key: '{key}'\")\n            print(f\"  Model: {config['name']}\")\n            print(f\"  Description: {config['description']}\")\n            print(f\"  Context Length: {config['context_length']:,} tokens\")\n            print(f\"  Type: {config['type']}\")\n            print()\n\ndef evaluate_model_on_training_data(\n    train_file_path: str,\n    model_key: str = 'qwen2.5-unsloth',\n    use_quantization: bool = True,\n    max_new_tokens: int = 8000\n) -> dict:\n    \"\"\"\n    Evaluate the model on the training dataset using Weighted F1 Score and Jaccard Score.\n\n    Args:\n        train_file_path: Path to the training dataset (TSV with 'answer' and 'final_AS' columns)\n        model_key: Model key from MODEL_CONFIGS (default: qwen2.5-unsloth)\n        use_quantization: Use 4-bit quantization\n        max_new_tokens: Maximum tokens to generate per answer\n\n    Returns:\n        dict: Dictionary containing evaluation metrics (Weighted F1 Score, Jaccard Score)\n    \"\"\"\n    \n    print(f\"üöÄ Initializing {model_key} for evaluation...\")\n    try:\n        classifier = ArabicMedicalAnswerClassifier(\n            model_key=model_key,\n            use_quantization=use_quantization\n        )\n    except Exception as e:\n        print(f\"‚ùå Failed to initialize model: {e}\")\n        return None\n\n    # Load training dataset\n    try:\n        df = pd.read_csv(train_file_path, sep='\\t')\n        if 'answer' not in df.columns or 'final_AS' not in df.columns:\n            raise ValueError(\"Dataset must contain 'answer' and 'final_AS' columns\")\n        print(f\"‚úÖ Training dataset loaded: {len(df)} samples\")\n    except Exception as e:\n        print(f\"‚ùå Error loading training dataset: {e}\")\n        return None\n\n    # Parse final_AS labels\n    def parse_labels(label):\n        if isinstance(label, str):\n            try:\n                if label.startswith('['):\n                    return ast.literal_eval(label)\n                else:\n                    return [strat.strip() for strat in label.split(',')]\n            except:\n                print(f\"Warning: Could not parse label '{label}', defaulting to ['1']\")\n                return ['1']\n        return label\n\n    df['final_AS'] = df['final_AS'].apply(parse_labels)\n\n    # Generate predictions\n    print(f\"üöÄ Generating predictions for {len(df)} samples...\")\n    predictions = classifier.process_test_dataset(df, max_new_tokens=max_new_tokens)\n\n    # Convert predictions and true labels to multi-label binary format\n    all_strategies = ['1', '2', '3']\n    y_true = []\n    y_pred = []\n\n    for true_labels, pred_labels in zip(df['final_AS'], predictions['prediction']):\n        true_vec = [1 if strat in true_labels else 0 for strat in all_strategies]\n        y_true.append(true_vec)\n        pred_strats = [strat.strip() for strat in pred_labels.split(',') if strat.strip() in all_strategies]\n        pred_vec = [1 if strat in pred_strats else 0 for strat in all_strategies]\n        y_pred.append(pred_vec)\n\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate metrics\n    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n    jaccard = jaccard_score(y_true, y_pred, average='samples')\n\n    # Print results\n    print(\"\\nüìä Evaluation Results:\")\n    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n    print(f\"Jaccard Score (samples): {jaccard:.4f}\")\n\n    # Detailed per-strategy metrics\n    print(\"\\nüìã Per-Strategy Metrics:\")\n    for i, strat in enumerate(all_strategies):\n        strat_f1 = f1_score(y_true[:, i], y_pred[:, i])\n        strat_jaccard = jaccard_score(y_true[:, i], y_pred[:, i])\n        print(f\"Strategy {strat}:\")\n        print(f\"  F1 Score: {strat_f1:.4f}\")\n        print(f\"  Jaccard Score: {strat_jaccard:.4f}\")\n\n    # Cleanup memory\n    classifier.cleanup_memory()\n\n    return {\n        'weighted_f1': weighted_f1,\n        'jaccard_score': jaccard,\n        'per_strategy_f1': {strat: f1_score(y_true[:, i], y_pred[:, i]) for i, strat in enumerate(all_strategies)},\n        'per_strategy_jaccard': {strat: jaccard_score(y_true[:, i], y_pred[:, i]) for i, strat in enumerate(all_strategies)}\n    }\n\ndef compare_all_models(train_file_path: str, use_quantization: bool = True, max_new_tokens: int = 6000):\n    \"\"\"\n    Compare performance of all available models on the training dataset\n    \"\"\"\n    models_to_test = ['qwen2.5-unsloth', 'deepseek-r1']\n    results = {}\n    \n    print(\"üöÄ Starting comprehensive model comparison...\")\n    print(\"=\" * 80)\n    \n    for model_key in models_to_test:\n        print(f\"\\nüîÑ Testing {model_key}...\")\n        try:\n            result = evaluate_model_on_training_data(\n                train_file_path=train_file_path,\n                model_key=model_key,\n                use_quantization=use_quantization,\n                max_new_tokens=max_new_tokens\n            )\n            if result:\n                results[model_key] = result\n            \n            # Clear memory between models\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n        except Exception as e:\n            print(f\"‚ùå Error testing {model_key}: {e}\")\n            continue\n    \n    # Print comparison results\n    print(\"\\n\" + \"=\" * 80)\n    print(\"üìä MODEL COMPARISON RESULTS\")\n    print(\"=\" * 80)\n    \n    for model_key, result in results.items():\n        print(f\"\\n{model_key.upper()}:\")\n        print(f\"  Weighted F1 Score: {result['weighted_f1']:.4f}\")\n        print(f\"  Jaccard Score: {result['jaccard_score']:.4f}\")\n    \n    # Find best model\n    if results:\n        best_model = max(results.keys(), key=lambda k: results[k]['weighted_f1'])\n        print(f\"\\nüèÜ BEST MODEL: {best_model}\")\n        print(f\"   Weighted F1: {results[best_model]['weighted_f1']:.4f}\")\n        print(f\"   Jaccard Score: {results[best_model]['jaccard_score']:.4f}\")\n    \n    return results\n\nif __name__ == \"__main__\":\n    # Update this path to your training dataset\n    TRAIN_DATASET_PATH = '/kaggle/input/train-dataset/Train_Dev.tsv'  # Replace with actual path\n\n    # List available models\n    ArabicMedicalAnswerClassifier.list_available_models()\n\n    # Evaluate with Unsloth Qwen2.5 (recommended for speed)\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Testing Unsloth Qwen2.5-7B-Instruct\")\n    print(\"=\" * 50)\n    results_unsloth = evaluate_model_on_training_data(\n        train_file_path=TRAIN_DATASET_PATH,\n        model_key='qwen2.5-unsloth',\n        use_quantization=True,\n        max_new_tokens=6000\n    )\n\n    # Evaluate with DeepSeek R1 (recommended for reasoning)\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Testing DeepSeek-R1-Distill-Qwen-7B\")\n    print(\"=\" * 50)\n    results_deepseek = evaluate_model_on_training_data(\n        train_file_path=TRAIN_DATASET_PATH,\n        model_key='deepseek-r1',\n        use_quantization=True,\n        max_new_tokens=6000\n    )\n\n    # Uncomment to run comprehensive comparison\n    # comprehensive_results = compare_all_models(\n    #     train_file_path=TRAIN_DATASET_PATH,\n    #     use_quantization=True,\n    #     max_new_tokens=6000\n    # )\n\n    # Print summary if both models were tested\n    if results_unsloth and results_deepseek:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"üìä COMPARISON SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"Unsloth Qwen2.5-7B:\")\n        print(f\"  Weighted F1: {results_unsloth['weighted_f1']:.4f}\")\n        print(f\"  Jaccard Score: {results_unsloth['jaccard_score']:.4f}\")\n        print(f\"  Advantage: Faster inference, optimized performance\")\n        \n        print(f\"\\nDeepSeek R1 Distill Qwen-7B:\")\n        print(f\"  Weighted F1: {results_deepseek['weighted_f1']:.4f}\")\n        print(f\"  Jaccard Score: {results_deepseek['jaccard_score']:.4f}\")\n        print(f\"  Advantage: Advanced reasoning capabilities\")\n        \n        # Determine winner\n        if results_unsloth['weighted_f1'] > results_deepseek['weighted_f1']:\n            print(f\"\\nüèÜ Winner: Unsloth Qwen2.5-7B (Better F1 Score)\")\n        elif results_deepseek['weighted_f1'] > results_unsloth['weighted_f1']:\n            print(f\"\\nüèÜ Winner: DeepSeek R1 (Better F1 Score)\")\n        else:\n            print(f\"\\nü§ù Tie: Both models perform equally well\")\n        \n        print(f\"\\nüí° Recommendation:\")\n        print(f\"   - Use 'qwen2.5-unsloth' for faster inference and production deployment\")\n        print(f\"   - Use 'deepseek-r1' for complex reasoning tasks and research\")\n\n# Additional utility functions for model management and optimization\n\ndef install_unsloth():\n    \"\"\"\n    Install Unsloth for optimized inference (run this once)\n    \"\"\"\n    install_commands = [\n        \"pip install unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\",\n        \"pip install --no-deps trl peft accelerate bitsandbytes\"\n    ]\n    \n    print(\"üì¶ To install Unsloth for optimized inference, run these commands:\")\n    for cmd in install_commands:\n        print(f\"   {cmd}\")\n    \n    print(\"\\n‚ö†Ô∏è  Note: Restart your runtime after installation!\")\n\ndef benchmark_inference_speed(model_key: str, sample_answers: List[str], use_quantization: bool = True):\n    \"\"\"\n    Benchmark inference speed for a specific model\n    \n    Args:\n        model_key: Model to benchmark\n        sample_answers: List of sample answers to test\n        use_quantization: Whether to use quantization\n    \"\"\"\n    import time\n    \n    print(f\"üöÄ Benchmarking {model_key} inference speed...\")\n    \n    classifier = ArabicMedicalAnswerClassifier(\n        model_key=model_key,\n        use_quantization=use_quantization\n    )\n    \n    # Warm-up run\n    classifier.classify_answer(sample_answers[0], max_new_tokens=2000)\n    \n    # Benchmark runs\n    start_time = time.time()\n    total_tokens = 0\n    \n    for answer in sample_answers:\n        result = classifier.classify_answer(answer, max_new_tokens=2000)\n        total_tokens += len(classifier.tokenizer.encode(answer))\n    \n    end_time = time.time()\n    total_time = end_time - start_time\n    \n    print(f\"‚è±Ô∏è  Benchmark Results for {model_key}:\")\n    print(f\"   Total Time: {total_time:.2f} seconds\")\n    print(f\"   Samples Processed: {len(sample_answers)}\")\n    print(f\"   Average Time per Sample: {total_time/len(sample_answers):.2f} seconds\")\n    print(f\"   Total Input Tokens: {total_tokens}\")\n    print(f\"   Tokens per Second: {total_tokens/total_time:.2f}\")\n    \n    classifier.cleanup_memory()\n    return {\n        'model_key': model_key,\n        'total_time': total_time,\n        'samples': len(sample_answers),\n        'avg_time_per_sample': total_time/len(sample_answers),\n        'tokens_per_second': total_tokens/total_time\n    }\n\ndef create_optimized_pipeline(model_key: str = 'qwen2.5-unsloth', batch_size: int = 4):\n    \"\"\"\n    Create an optimized inference pipeline for batch processing\n    \n    Args:\n        model_key: Model to use for the pipeline\n        batch_size: Number of samples to process in each batch\n    \n    Returns:\n        Optimized classifier instance\n    \"\"\"\n    print(f\"üîß Creating optimized pipeline with {model_key}...\")\n    \n    classifier = ArabicMedicalAnswerClassifier(\n        model_key=model_key,\n        use_quantization=True,\n        use_thinking_mode=False  # Disable for faster inference\n    )\n    \n    # Set model to evaluation mode for inference\n    classifier.model.eval()\n    \n    # Enable torch.compile for PyTorch 2.0+ (if available)\n    try:\n        import torch._dynamo\n        classifier.model = torch.compile(classifier.model, mode=\"reduce-overhead\")\n        print(\"‚úÖ Torch compile enabled for faster inference\")\n    except:\n        print(\"‚ö†Ô∏è  Torch compile not available, using standard inference\")\n    \n    print(f\"‚úÖ Optimized pipeline ready with batch size: {batch_size}\")\n    return classifier\n\n# Example usage and testing functions\n\ndef test_new_models():\n    \"\"\"\n    Test the newly added models with sample data\n    \"\"\"\n    sample_answers = [\n        \"ŸäŸÖŸÉŸÜ ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿµÿØÿßÿπ ÿ®ÿ≥ÿ®ÿ® ÿßŸÑÿ•ÿ¨ŸáÿßÿØ ÿ£Ÿà ŸÇŸÑÿ© ÿßŸÑŸÜŸàŸÖ. ÿ£ŸÜÿµÿ≠ŸÉ ÿ®ÿßŸÑÿ±ÿßÿ≠ÿ© Ÿàÿ¥ÿ±ÿ® ÿßŸÑŸÖÿßÿ°.\",\n        \"ŸÑÿß ÿ™ŸÇŸÑŸÇÿå Ÿáÿ∞Ÿá ÿßŸÑÿ£ÿπÿ±ÿßÿ∂ ÿ∑ÿ®ŸäÿπŸäÿ©. ÿ≥ÿ™ÿ¥ÿπÿ± ÿ®ÿ™ÿ≠ÿ≥ŸÜ ŸÇÿ±Ÿäÿ®ÿßŸã.\",\n        \"ŸÅŸäÿ™ÿßŸÖŸäŸÜ ÿØ ŸÖŸáŸÖ ŸÑÿµÿ≠ÿ© ÿßŸÑÿπÿ∏ÿßŸÖ ŸàŸäŸÖŸÉŸÜ ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸäŸá ŸÖŸÜ ÿ£ÿ¥ÿπÿ© ÿßŸÑÿ¥ŸÖÿ≥ ŸàÿßŸÑÿ£ÿ∑ÿπŸÖÿ© ÿßŸÑŸÖÿØÿπŸÖÿ©.\"\n    ]\n    \n    models_to_test = ['qwen2.5-unsloth']  #deepseek-r1\n    \n    for model_key in models_to_test:\n        print(f\"\\n{'='*50}\")\n        print(f\"Testing {model_key}\")\n        print(f\"{'='*50}\")\n        \n        try:\n            classifier = ArabicMedicalAnswerClassifier(\n                model_key=model_key,\n                use_quantization=True\n            )\n            \n            for i, answer in enumerate(sample_answers):\n                result = classifier.classify_answer(answer, max_new_tokens=3000)\n                print(f\"\\nSample {i+1}: {answer[:50]}...\")\n                print(f\"Classification: {result}\")\n            \n            classifier.cleanup_memory()\n            \n        except Exception as e:\n            print(f\"‚ùå Error testing {model_key}: {e}\")\n\n# Run tests if this is the main module\nif __name__ == \"__main__\":\n    # Show installation instructions for Unsloth\n    print(\"\\n\" + \"=\"*80)\n    print(\"üöÄ INSTALLATION GUIDE\")\n    print(\"=\"*80)\n    install_unsloth()\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üß™ TESTING NEW MODELS\")\n    print(\"=\"*80)\n    # Uncomment to test the new models with sample data\n    # test_new_models()\n    \n    print(\"\\n‚úÖ Setup complete! The classifier now supports:\")\n    print(\"   - unsloth/Qwen2.5-7B-Instruct (Optimized for speed)\")\n    print(\"   - deepseek-ai/DeepSeek-R1-Distill-Qwen-7B (Advanced reasoning)\")\n    print(\"\\nüí° Use 'qwen2.5-unsloth' as default for best performance!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-27T08:35:32.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}