{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12479800,"sourceType":"datasetVersion","datasetId":7874336},{"sourceId":12589423,"sourceType":"datasetVersion","datasetId":7951272}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:34:43.274725Z","iopub.execute_input":"2025-07-27T08:34:43.275031Z","iopub.status.idle":"2025-07-27T08:34:43.291282Z","shell.execute_reply.started":"2025-07-27T08:34:43.275005Z","shell.execute_reply":"2025-07-27T08:34:43.290627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers>=4.51.0 torch torchvision torchaudio accelerate bitsandbytes -q\n!pip install sentencepiece protobuf -q\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport gc\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\n# Check GPU availability\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:34:43.292522Z","iopub.execute_input":"2025-07-27T08:34:43.292762Z","execution_failed":"2025-07-27T08:35:32.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nimport re\nimport pandas as pd\nimport numpy as np\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    BitsAndBytesConfig\n)\nfrom typing import List, Dict, Tuple, Any, Optional\nimport warnings\nfrom sklearn.metrics import f1_score, jaccard_score\nfrom huggingface_hub import login\nimport ast\n\nHF_TOKEN = \"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\"\nlogin(HF_TOKEN)\nwarnings.filterwarnings('ignore')\n\nclass MultiModelArabicMedicalClassifier:\n    \"\"\"\n    Arabic Medical Question Classifier supporting multiple LLM models including Unsloth and DeepSeek\n    \"\"\"\n    \n    # Model configurations with their specific settings\n    MODEL_CONFIGS = {\n        'qwen3': {\n            'name': 'Qwen/Qwen3-14B',\n            'type': 'qwen',\n            'context_length': 32768,\n            'description': 'Qwen3 14B - Latest Qwen model'\n        },\n        'jais': {\n            'name': 'core42/jais-13b-chat',\n            'type': 'jais',\n            'context_length': 2048,\n            'description': 'JAIS - Arabic-focused LLM'\n        },\n        'qwen2': {\n            'name': 'Qwen/Qwen2-7B-Instruct',\n            'type': 'qwen',\n            'context_length': 32768,\n            'description': 'Qwen2 7B - Strong multilingual model'\n        },\n        'qwen2.5': {\n            'name': 'unsloth/Qwen2.5-7B-Instruct',\n            'type': 'qwen',\n            'context_length': 131072,\n            'description': 'Qwen2.5 7B Instruct - Unsloth optimized'\n        },\n        'qwen2.5-unsloth': {\n            'name': 'unsloth/Qwen2.5-7B-Instruct',\n            'type': 'qwen',\n            'context_length': 131072,\n            'description': 'Unsloth Qwen2.5 7B - Optimized for inference speed'\n        },\n        'deepseek-r1': {\n            'name': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n            'type': 'qwen',\n            'context_length': 131072,\n            'description': 'DeepSeek R1 Distill Qwen 7B - Advanced reasoning model'\n        },\n        'deepseek_r1': {\n            'name': 'DeepSeek-R1-Distill-Qwen-7B',\n            'type': 'qwen',\n            'context_length': 32768,\n            'description': 'DeepSeek R1 Distill Qwen 7B - Reasoning optimized'\n        },\n        'llama3': {\n            'name': 'meta-llama/Llama-3.1-8B-Instruct',\n            'type': 'llama',\n            'context_length': 128000,\n            'description': 'Llama 3.1 8B - Meta\\'s latest model'\n        },\n        'mixtral': {\n            'name': 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n            'type': 'mixtral',\n            'context_length': 32768,\n            'description': 'Mixtral 8x7B - Mixture of Experts'\n        },\n        'command_r': {\n            'name': 'CohereForAI/c4ai-command-r-v01',\n            'type': 'command_r',\n            'context_length': 128000,\n            'description': 'Command R - Cohere\\'s multilingual model'\n        },\n        'gemma2': {\n            'name': 'google/gemma-2-9b-it',\n            'type': 'gemma',\n            'context_length': 8192,\n            'description': 'Gemma 2 9B - Google\\'s instruction-tuned model'\n        },\n        'phi3': {\n            'name': 'microsoft/Phi-3-medium-14b-instruct',\n            'type': 'phi',\n            'context_length': 128000,\n            'description': 'Phi-3 Medium - Microsoft\\'s efficient model'\n        }\n    }\n    \n    def __init__(self, model_key: str = 'qwen2.5-unsloth', use_quantization: bool = True, use_thinking_mode: bool = True):\n        \"\"\"\n        Initialize classifier with specified model\n        \n        Args:\n            model_key: Key from MODEL_CONFIGS (default: qwen2.5-unsloth for better performance)\n            use_quantization: Whether to use 4-bit quantization\n            use_thinking_mode: Enable thinking mode for supported models\n        \"\"\"\n        if model_key not in self.MODEL_CONFIGS:\n            raise ValueError(f\"Model '{model_key}' not supported. Available models: {list(self.MODEL_CONFIGS.keys())}\")\n        \n        self.model_config = self.MODEL_CONFIGS[model_key]\n        self.model_key = model_key\n        self.use_quantization = use_quantization\n        self.use_thinking_mode = use_thinking_mode\n        self.tokenizer = None\n        self.model = None\n        \n        self.question_categories = {\n            'A': 'Diagnosis (questions about interpreting clinical findings)',\n            'B': 'Treatment (questions about seeking treatments)', \n            'C': 'Anatomy and Physiology (questions about basic medical knowledge)',\n            'D': 'Epidemiology (questions about the course, prognosis, and etiology of diseases)',\n            'E': 'Healthy Lifestyle (questions related to diet, exercise, and mood control)',\n            'F': 'Provider Choices (questions seeking recommendations for medical professionals and facilities)',\n            'Z': 'Other (questions that do not fall under the above-mentioned categories)'\n        }\n        \n        self.load_model()\n    \n    def load_model(self):\n        \"\"\"Load the specified model with appropriate configurations\"\"\"\n        print(f\"Loading {self.model_config['description']}...\")\n        print(f\"Model: {self.model_config['name']}\")\n        \n        quantization_config = None\n        if self.use_quantization:\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\"\n            )\n        \n        try:\n            # Special handling for Unsloth models\n            if 'unsloth' in self.model_config['name']:\n                print(\"üöÄ Loading Unsloth optimized model...\")\n                try:\n                    # Try to use Unsloth's FastLanguageModel if available\n                    from unsloth import FastLanguageModel\n                    self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n                        model_name=self.model_config['name'],\n                        max_seq_length=self.model_config['context_length'],\n                        dtype=torch.float16,\n                        load_in_4bit=self.use_quantization,\n                    )\n                    FastLanguageModel.for_inference(self.model)  # Enable native 2x faster inference\n                    print(\"‚úÖ Unsloth FastLanguageModel loaded successfully!\")\n                except ImportError:\n                    print(\"‚ö†Ô∏è Unsloth not available, falling back to standard transformers...\")\n                    # Fallback to standard transformers\n                    self._load_standard_model(quantization_config)\n            else:\n                self._load_standard_model(quantization_config)\n            \n            # Set pad token if not available\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n                \n            print(f\"‚úÖ {self.model_config['description']} loaded successfully!\")\n            self.print_model_info()\n            \n        except Exception as e:\n            print(f\"‚ùå Error loading model: {e}\")\n            raise\n    \n    def _load_standard_model(self, quantization_config):\n        \"\"\"Load model using standard transformers\"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.model_config['name'],\n            trust_remote_code=True,\n            padding_side='left'\n        )\n        \n        model_kwargs = {\n            'trust_remote_code': True,\n            'torch_dtype': torch.float16,\n            'low_cpu_mem_usage': True,\n            'device_map': 'auto'\n        }\n        \n        if quantization_config:\n            model_kwargs['quantization_config'] = quantization_config\n        \n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_config['name'],\n            **model_kwargs\n        )\n    \n    def print_model_info(self):\n        \"\"\"Print model and memory information\"\"\"\n        print(f\"\\nüìä Model Information:\")\n        print(f\"Model: {self.model_config['name']}\")\n        print(f\"Type: {self.model_config['type']}\")\n        print(f\"Context Length: {self.model_config['context_length']:,} tokens\")\n        print(f\"Quantization: {'4-bit' if self.use_quantization else 'Full precision'}\")\n        print(f\"Thinking Mode: {'Enabled' if self.use_thinking_mode else 'Disabled'}\")\n        print(f\"Tokenizer Vocab Size: {len(self.tokenizer):,}\")\n        \n        if torch.cuda.is_available():\n            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n    \n    def create_prompt(self, question: str) -> str:\n        \"\"\"Create model-specific prompt for classification\"\"\"\n        category_descriptions = \"\"\"\nŸÅÿ¶ÿßÿ™ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ∑ÿ®Ÿäÿ©:\n(A) ÿßŸÑÿ™ÿ¥ÿÆŸäÿµ - ÿ£ÿ≥ÿ¶ŸÑÿ© ÿ≠ŸàŸÑ ÿ™ŸÅÿ≥Ÿäÿ± ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑÿ≥ÿ±Ÿäÿ±Ÿäÿ© ŸàÿßŸÑÿ£ÿπÿ±ÿßÿ∂\n(B) ÿßŸÑÿπŸÑÿßÿ¨ - ÿ£ÿ≥ÿ¶ŸÑÿ© ÿ≠ŸàŸÑ ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿπŸÑÿßÿ¨ÿßÿ™ Ÿàÿ∑ÿ±ŸÇ ÿßŸÑÿπŸÑÿßÿ¨\n(C) ÿßŸÑÿ™ÿ¥ÿ±Ÿäÿ≠ ŸàÿπŸÑŸÖ Ÿàÿ∏ÿßÿ¶ŸÅ ÿßŸÑÿ£ÿπÿ∂ÿßÿ° - ÿ£ÿ≥ÿ¶ŸÑÿ© ÿ≠ŸàŸÑ ÿßŸÑŸÖÿπÿ±ŸÅÿ© ÿßŸÑÿ∑ÿ®Ÿäÿ© ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©\n(D) ÿπŸÑŸÖ ÿßŸÑÿ£Ÿàÿ®ÿ¶ÿ© - ÿ£ÿ≥ÿ¶ŸÑÿ© ÿ≠ŸàŸÑ ŸÖÿ≥ÿßÿ± ÿßŸÑŸÖÿ±ÿ∂ Ÿàÿ™ÿ¥ÿÆŸäÿµŸá Ÿàÿ£ÿ≥ÿ®ÿßÿ®Ÿá\n(E) ŸÜŸÖÿ∑ ÿßŸÑÿ≠Ÿäÿßÿ© ÿßŸÑÿµÿ≠Ÿä - ÿ£ÿ≥ÿ¶ŸÑÿ© ŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ÿßŸÑŸÜÿ∏ÿßŸÖ ÿßŸÑÿ∫ÿ∞ÿßÿ¶Ÿä ŸàÿßŸÑÿ±Ÿäÿßÿ∂ÿ© ŸàÿßŸÑÿµÿ≠ÿ© ÿßŸÑŸÜŸÅÿ≥Ÿäÿ©\n(F) ÿßÿÆÿ™Ÿäÿßÿ± ŸÖŸÇÿØŸÖ ÿßŸÑÿ±ÿπÿßŸäÿ© - ÿ£ÿ≥ÿ¶ŸÑÿ© ÿ™ÿ∑ŸÑÿ® ÿ™ŸàÿµŸäÿßÿ™ ŸÑŸÑŸÖŸáŸÜŸäŸäŸÜ ÿßŸÑÿ∑ÿ®ŸäŸäŸÜ ŸàÿßŸÑŸÖÿ±ÿßŸÅŸÇ\n(Z) ÿ£ÿÆÿ±Ÿâ - ÿ£ÿ≥ÿ¶ŸÑÿ© ŸÑÿß ÿ™ŸÜÿØÿ±ÿ¨ ÿ™ÿ≠ÿ™ ÿßŸÑŸÅÿ¶ÿßÿ™ ÿßŸÑŸÖÿ∞ŸÉŸàÿ±ÿ© ÿ£ÿπŸÑÿßŸá\n\"\"\"\n        \n        # Enhanced prompt for DeepSeek R1 model with reasoning capabilities\n        if self.model_key in ['deepseek-r1', 'deepseek_r1']:\n            base_prompt = f\"\"\"ÿ£ŸÜÿ™ ŸÜŸÖŸàÿ∞ÿ¨ ÿ∞ŸÉŸä ŸÖÿ™ÿÆÿµÿµ ŸÅŸä ÿ™ÿ≠ŸÑŸäŸÑ Ÿàÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ∑ÿ®Ÿäÿ© ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©. ŸÑÿØŸäŸÉ ŸÇÿØÿ±ÿßÿ™ ÿ™ŸÅŸÉŸäÿ± ŸÖÿ™ŸÇÿØŸÖÿ© Ÿàÿ™ÿ≠ŸÑŸäŸÑ ÿπŸÖŸäŸÇ.\n\n{category_descriptions}\n\nÿßŸÑÿ≥ÿ§ÿßŸÑ ÿßŸÑŸÖÿ±ÿßÿØ ÿ™ÿµŸÜŸäŸÅŸá:\n{question}\n\nÿ™ÿπŸÑŸäŸÖÿßÿ™ ÿÆÿßÿµÿ© ŸÑŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖÿ™ŸÇÿØŸÖ:\n1. ÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÇÿØÿ±ÿßÿ™ŸÉ ŸÅŸä ÿßŸÑÿ™ŸÅŸÉŸäÿ± ÿßŸÑŸÖŸÜÿ∑ŸÇŸä ŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ®ÿπŸÖŸÇ\n2. ÿ≠ŸÑŸÑ ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑÿ∑ÿ®Ÿä ŸàÿßŸÑŸÖŸÅÿßŸáŸäŸÖ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖÿ©\n3. ŸÅŸÉÿ± ŸÅŸä ÿßŸÑŸáÿØŸÅ ŸÖŸÜ ÿßŸÑÿ≥ÿ§ÿßŸÑ ŸàŸÜŸàÿπ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©\n4. ÿßÿπÿ™ÿ®ÿ± ÿßŸÑÿ™ÿØÿßÿÆŸÑ ÿ®ŸäŸÜ ÿßŸÑŸÅÿ¶ÿßÿ™ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ©\n5. ÿßÿ¥ÿ±ÿ≠ ŸÖŸÜÿ∑ŸÇ ÿ™ŸÅŸÉŸäÿ±ŸÉ ÿÆÿ∑Ÿàÿ© ÿ®ÿÆÿ∑Ÿàÿ©\n6. ÿ≠ÿØÿØ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸÅÿ™ÿßÿ≠Ÿäÿ© ŸàÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™ ÿßŸÑŸÑÿ∫ŸàŸäÿ©\n7. ŸÅŸä ÿßŸÑŸÜŸáÿßŸäÿ©ÿå ŸÇÿØŸÖ ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä ÿ®ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®\n\nÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®:\n\"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [ÿ£ÿ≠ÿ±ŸÅ ÿßŸÑŸÅÿ¶ÿßÿ™ ŸÖŸÅÿµŸàŸÑÿ© ÿ®ŸÅŸàÿßÿµŸÑ]\"\n\nŸÖÿ´ÿßŸÑ: ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿ≥ÿ§ÿßŸÑ Ÿäÿ™ÿπŸÑŸÇ ÿ®ÿßŸÑÿ™ÿ¥ÿÆŸäÿµ ŸàÿßŸÑÿπŸÑÿßÿ¨ÿå ŸÅÿßŸÉÿ™ÿ®: \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [A,B]\"\n\"\"\"\n        elif self.use_thinking_mode:\n            base_prompt = f\"\"\"ÿ£ŸÜÿ™ ÿÆÿ®Ÿäÿ± ŸÅŸä ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ∑ÿ®Ÿäÿ© ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©. ŸÖŸáŸÖÿ™ŸÉ ŸáŸä ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿßŸÑÿ™ÿßŸÑŸä ÿ•ŸÑŸâ ŸÅÿ¶ÿ© ÿ£Ÿà ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿßŸÑŸÅÿ¶ÿßÿ™ ÿßŸÑŸÖÿ≠ÿØÿØÿ©.\n\n{category_descriptions}\n\nÿßŸÑÿ≥ÿ§ÿßŸÑ ÿßŸÑŸÖÿ±ÿßÿØ ÿ™ÿµŸÜŸäŸÅŸá:\n{question}\n\nÿ™ÿπŸÑŸäŸÖÿßÿ™:\n1. ŸÅŸÉÿ± ÿ®ÿπŸÖŸÇ ŸÅŸä ŸÖÿ≠ÿ™ŸàŸâ ÿßŸÑÿ≥ÿ§ÿßŸÑ Ÿàÿ≠ŸÑŸÑ ŸÉŸÑ ÿ¨ÿßŸÜÿ® ŸÖŸÜŸá\n2. ÿßÿ¥ÿ±ÿ≠ ÿ™ŸÅŸÉŸäÿ±ŸÉ ÿÆÿ∑Ÿàÿ© ÿ®ÿÆÿ∑Ÿàÿ©\n3. ÿ≠ÿØÿØ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸÅÿ™ÿßÿ≠Ÿäÿ© ŸàÿßŸÑŸÖŸÅÿßŸáŸäŸÖ ÿßŸÑÿ∑ÿ®Ÿäÿ©\n4. ÿßÿ±ÿ®ÿ∑ ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ®ÿßŸÑŸÅÿ¶ÿßÿ™ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ© ŸÖÿπ ÿßŸÑÿ™ÿ®ÿ±Ÿäÿ±\n5. ÿßÿÆÿ™ÿ± ÿßŸÑŸÅÿ¶ÿ© ÿ£Ÿà ÿßŸÑŸÅÿ¶ÿßÿ™ ÿßŸÑÿ£ŸÜÿ≥ÿ® (ŸäŸÖŸÉŸÜ ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ŸÅÿ¶ÿ©)\n6. ŸÅŸä ÿßŸÑŸÜŸáÿßŸäÿ©ÿå ÿßŸÉÿ™ÿ® ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑÿ™ÿßŸÑŸä:\n   \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [A,B,C]\" (ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿ£ÿ≠ÿ±ŸÅ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ© ŸÖŸÅÿµŸàŸÑÿ© ÿ®ŸÅŸàÿßÿµŸÑ)\n\nŸÖÿ´ÿßŸÑ ÿπŸÑŸâ ÿßŸÑÿ™ŸÅŸÉŸäÿ± ŸàÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ:\n- ÿ≠ŸÑŸÑ ÿßŸÑÿ≥ÿ§ÿßŸÑ: \"ŸÖÿß ÿ≥ÿ®ÿ® Ÿáÿ∞ÿß ÿßŸÑÿπÿ±ÿ∂ÿü\"\n- ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸÅÿ™ÿßÿ≠Ÿäÿ©: \"ÿ≥ÿ®ÿ®ÿå ÿπÿ±ÿ∂\"\n- ÿßŸÑÿ™ŸÅŸÉŸäÿ±: Ÿáÿ∞ÿß ÿ≥ÿ§ÿßŸÑ Ÿäÿ™ÿπŸÑŸÇ ÿ®ÿ™ÿ≠ÿØŸäÿØ ÿ£ÿ≥ÿ®ÿßÿ® ÿßŸÑÿ£ÿπÿ±ÿßÿ∂ÿå ŸàŸáŸà ÿ¨ÿ≤ÿ° ŸÖŸÜ ÿπŸÖŸÑŸäÿ© ÿßŸÑÿ™ÿ¥ÿÆŸäÿµ\n- ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [A]\n\nÿßŸÑÿ¢ŸÜ ÿ≠ŸÑŸÑ ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ£ÿπŸÑÿßŸá ÿ®ŸÜŸÅÿ≥ ÿßŸÑÿ∑ÿ±ŸäŸÇÿ©.\n\"\"\"\n        else:\n            base_prompt = f\"\"\"ÿ£ŸÜÿ™ ÿÆÿ®Ÿäÿ± ŸÅŸä ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ∑ÿ®Ÿäÿ© ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©. ŸÖŸáŸÖÿ™ŸÉ ŸáŸä ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿßŸÑÿ™ÿßŸÑŸä ÿ•ŸÑŸâ ŸÅÿ¶ÿ© ÿ£Ÿà ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿßŸÑŸÅÿ¶ÿßÿ™ ÿßŸÑŸÖÿ≠ÿØÿØÿ©.\n\n{category_descriptions}\n\nÿßŸÑÿ≥ÿ§ÿßŸÑ ÿßŸÑŸÖÿ±ÿßÿØ ÿ™ÿµŸÜŸäŸÅŸá:\n{question}\n\nÿ™ÿπŸÑŸäŸÖÿßÿ™:\n1. ÿßŸÇÿ±ÿ£ ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ®ÿπŸÜÿßŸäÿ© Ÿàÿ≠ŸÑŸÑ ŸÖÿ≠ÿ™ŸàÿßŸá\n2. ÿ≠ÿØÿØ ÿßŸÑŸÅÿ¶ÿ© ÿ£Ÿà ÿßŸÑŸÅÿ¶ÿßÿ™ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ© (ŸäŸÖŸÉŸÜ ÿ£ŸÜ ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ŸÅÿ¶ÿ© Ÿàÿßÿ≠ÿØÿ©)\n3. ÿßÿ¥ÿ±ÿ≠ ÿ≥ÿ®ÿ® ÿßÿÆÿ™Ÿäÿßÿ±ŸÉ ŸÑŸÉŸÑ ŸÅÿ¶ÿ©\n4. ŸÅŸä ÿßŸÑŸÜŸáÿßŸäÿ©ÿå ÿßŸÉÿ™ÿ® ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑÿ™ÿßŸÑŸä:\n   \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [A,B,C]\" (ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿ£ÿ≠ÿ±ŸÅ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ© ŸÖŸÅÿµŸàŸÑÿ© ÿ®ŸÅŸàÿßÿµŸÑ)\n\nŸÖÿ´ÿßŸÑ ÿπŸÑŸâ ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ:\n- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿπŸÜ ÿßŸÑÿ™ÿ¥ÿÆŸäÿµ ŸÅŸÇÿ∑: \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [A]\"\n- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿπŸÜ ÿßŸÑÿ™ÿ¥ÿÆŸäÿµ ŸàÿßŸÑÿπŸÑÿßÿ¨: \"ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä: [A,B]\"\n\"\"\"\n        \n        # Apply chat template for Qwen-based models (including Unsloth and DeepSeek)\n        if self.model_config['type'] == 'qwen':\n            messages = [{\"role\": \"user\", \"content\": base_prompt}]\n            try:\n                return self.tokenizer.apply_chat_template(\n                    messages, \n                    tokenize=False, \n                    add_generation_prompt=True,\n                    enable_thinking=self.use_thinking_mode if hasattr(self.tokenizer, 'enable_thinking') else False\n                )\n            except:\n                # Fallback for models that don't support thinking mode\n                return self.tokenizer.apply_chat_template(\n                    messages, \n                    tokenize=False, \n                    add_generation_prompt=True\n                )\n        elif self.model_config['type'] in ['llama', 'mixtral']:\n            return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{base_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        elif self.model_config['type'] == 'jais':\n            return f\"### Instruction: {base_prompt}\\n### Response:\"\n        elif self.model_config['type'] == 'gemma':\n            return f\"<start_of_turn>user\\n{base_prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n        elif self.model_config['type'] == 'phi':\n            return f\"<|user|>\\n{base_prompt}<|end|>\\n<|assistant|>\\n\"\n        else:\n            return f\"Human: {base_prompt}\\n\\nAssistant:\"\n    \n    def classify_question(self, question: str, max_new_tokens: int = 8000) -> List[str]:\n        \"\"\"\n        Classify Arabic medical question into categories\n        \"\"\"\n        prompt = self.create_prompt(question)\n        \n        inputs = self.tokenizer(\n            prompt, \n            return_tensors=\"pt\", \n            truncation=True,\n            max_length=self.model_config['context_length'] - max_new_tokens\n        ).to(self.model.device)\n        \n        # Optimized generation parameters for different models\n        gen_kwargs = {\n            'max_new_tokens': max_new_tokens,\n            'temperature': 0.7,\n            'top_p': 0.9,\n            'top_k': 50,\n            'do_sample': True,\n            'pad_token_id': self.tokenizer.pad_token_id,\n            'eos_token_id': self.tokenizer.eos_token_id,\n            'repetition_penalty': 1.1\n        }\n        \n        # Model-specific optimizations\n        if self.model_key in ['deepseek-r1', 'deepseek_r1']:\n            # DeepSeek R1 optimized parameters for reasoning\n            gen_kwargs.update({\n                'temperature': 0.3,  # Lower temperature for more focused reasoning\n                'top_p': 0.9,\n                'top_k': 40,\n                'repetition_penalty': 1.05\n            })\n        elif self.model_key in ['qwen2.5-unsloth', 'qwen2.5']:\n            # Unsloth Qwen2.5 optimized parameters\n            gen_kwargs.update({\n                'temperature': 0.6,\n                'top_p': 0.95,\n                'top_k': 30,\n                'repetition_penalty': 1.08\n            })\n        elif self.model_config['type'] == 'qwen':\n            gen_kwargs.update({\n                'temperature': 0.6,\n                'top_p': 0.95,\n                'top_k': 20\n            })\n        elif self.model_config['type'] == 'llama':\n            gen_kwargs.update({\n                'temperature': 0.8,\n                'top_p': 0.95\n            })\n        \n        with torch.no_grad():\n            generated_ids = self.model.generate(\n                **inputs,\n                **gen_kwargs\n            )\n        \n        output_ids = generated_ids[0][len(inputs.input_ids[0]):]\n        response = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n        \n        categories = self.extract_question_categories(response)\n        return categories\n    \n    def extract_question_categories(self, response: str) -> List[str]:\n        \"\"\"Extract question categories from Arabic response text\"\"\"\n        patterns = [\n            r'ÿßŸÑÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜŸáÿßÿ¶Ÿä:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'ÿßŸÑŸÅÿ¶ÿßÿ™:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'ÿßŸÑÿ™ÿµŸÜŸäŸÅ:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n        ]\n        \n        for pattern in patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                categories_str = match.group(1)\n                categories = [cat.strip().upper() for cat in categories_str.split(',')]\n                return [cat for cat in categories if cat in ['A', 'B', 'C', 'D', 'E', 'F', 'Z']]\n        \n        english_patterns = [\n            r'Final Classification:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'Categories:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'Classification:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'Answer:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n        ]\n        \n        for pattern in english_patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                categories_str = match.group(1)\n                categories = [cat.strip().upper() for cat in categories_str.split(',')]\n                return [cat for cat in categories if cat in ['A', 'B', 'C', 'D', 'E', 'F', 'Z']]\n        \n        found_categories = []\n        for category in ['A', 'B', 'C', 'D', 'E', 'F', 'Z']:\n            if f'({category})' in response or f'[{category}]' in response or f' {category} ' in response:\n                found_categories.append(category)\n        \n        return found_categories if found_categories else ['Z']\n    \n    def process_test_dataset(self, df: pd.DataFrame, max_new_tokens: int = 8000, show_progress: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Process test dataset for question classification\n        \"\"\"\n        print(f\"üöÄ Starting Arabic Medical Question Classification with {self.model_config['description']}...\")\n        print(f\"Test dataset size: {len(df)} samples\")\n        print(\"-\" * 80)\n        \n        predictions = []\n        \n        for idx, row in df.iterrows():\n            if show_progress and idx % 10 == 0:\n                print(f\"Processing sample {idx+1}/{len(df)} - Current GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            \n            try:\n                categories = self.classify_question(row['question'], max_new_tokens=max_new_tokens)\n                prediction_str = ', '.join(sorted(categories))\n                predictions.append(prediction_str)\n                \n                if show_progress and idx < 3:\n                    print(f\"Sample {idx+1} prediction: {prediction_str}\")\n                    \n            except Exception as e:\n                print(f\"Error processing question {idx}: {e}\")\n                predictions.append('Z')\n            \n            if idx % 20 == 0:\n                self.cleanup_memory()\n        \n        result_df = pd.DataFrame({'prediction': predictions})\n        return result_df\n    \n    def cleanup_memory(self):\n        \"\"\"Clean up GPU memory\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    \n    @classmethod\n    def list_available_models(cls):\n        \"\"\"List all available models with descriptions\"\"\"\n        print(\"üìã Available Models:\")\n        print(\"-\" * 80)\n        \n        for key, config in cls.MODEL_CONFIGS.items():\n            print(f\"Key: '{key}'\")\n            print(f\"  Model: {config['name']}\")\n            print(f\"  Description: {config['description']}\")\n            print(f\"  Context Length: {config['context_length']:,} tokens\")\n            print(f\"  Type: {config['type']}\")\n            print()\n\ndef evaluate_model_on_training_data(\n    train_file_path: str,\n    model_key: str = 'qwen2.5-unsloth',\n    use_quantization: bool = True,\n    max_new_tokens: int = 8000\n) -> dict:\n    \"\"\"\n    Evaluate the model on the training dataset using Weighted F1 Score and Jaccard Score.\n\n    Args:\n        train_file_path: Path to the training dataset (TSV with 'question' and 'final_QT' columns)\n        model_key: Model key from MODEL_CONFIGS (default: qwen2.5-unsloth)\n        use_quantization: Use 4-bit quantization\n        max_new_tokens: Maximum tokens to generate per question\n\n    Returns:\n        dict: Dictionary containing evaluation metrics (Weighted F1 Score, Jaccard Score)\n    \"\"\"\n    \n    # Initialize the classifier\n    print(f\"üöÄ Initializing {model_key} for evaluation...\")\n    try:\n        classifier = MultiModelArabicMedicalClassifier(\n            model_key=model_key,\n            use_quantization=use_quantization\n        )\n    except Exception as e:\n        print(f\"‚ùå Failed to initialize model: {e}\")\n        return None\n\n    # Load training dataset\n    try:\n        df = pd.read_csv(train_file_path, sep='\\t')\n        if 'question' not in df.columns or 'final_QT' not in df.columns:\n            raise ValueError(\"Dataset must contain 'question' and 'final_QT' columns\")\n        print(f\"‚úÖ Training dataset loaded: {len(df)} samples\")\n    except Exception as e:\n        print(f\"‚ùå Error loading training dataset: {e}\")\n        return None\n\n    # Parse final_QT labels\n    def parse_labels(label):\n        if isinstance(label, str):\n            try:\n                if label.startswith('['):\n                    return ast.literal_eval(label)\n                else:\n                    return [cat.strip() for cat in label.split(',')]\n            except:\n                print(f\"Warning: Could not parse label '{label}', defaulting to ['Z']\")\n                return ['Z']\n        return label\n\n    df['final_QT'] = df['final_QT'].apply(parse_labels)\n\n    # Generate predictions\n    print(f\"üöÄ Generating predictions for {len(df)} samples...\")\n    predictions = classifier.process_test_dataset(df, max_new_tokens=max_new_tokens)\n\n    # Convert predictions and true labels to multi-label binary format\n    all_categories = ['A', 'B', 'C', 'D', 'E', 'F', 'Z']\n    y_true = []\n    y_pred = []\n\n    for true_labels, pred_labels in zip(df['final_QT'], predictions['prediction']):\n        true_vec = [1 if cat in true_labels else 0 for cat in all_categories]\n        y_true.append(true_vec)\n        pred_cats = [cat.strip() for cat in pred_labels.split(',') if cat.strip() in all_categories]\n        pred_vec = [1 if cat in pred_cats else 0 for cat in all_categories]\n        y_pred.append(pred_vec)\n\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate metrics\n    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n    jaccard = jaccard_score(y_true, y_pred, average='samples')\n\n    # Print results\n    print(\"\\nüìä Evaluation Results:\")\n    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n    print(f\"Jaccard Score (samples): {jaccard:.4f}\")\n\n    # Detailed per-category metrics\n    print(\"\\nüìã Per-Category Metrics:\")\n    for i, cat in enumerate(all_categories):\n        cat_f1 = f1_score(y_true[:, i], y_pred[:, i])\n        cat_jaccard = jaccard_score(y_true[:, i], y_pred[:, i])\n        print(f\"Category {cat}:\")\n        print(f\"  F1 Score: {cat_f1:.4f}\")\n        print(f\"  Jaccard Score: {cat_jaccard:.4f}\")\n\n    # Cleanup memory\n    classifier.cleanup_memory()\n\n    return {\n        'weighted_f1': weighted_f1,\n        'jaccard_score': jaccard,\n        'per_category_f1': {cat: f1_score(y_true[:, i], y_pred[:, i]) for i, cat in enumerate(all_categories)},\n        'per_category_jaccard': {cat: jaccard_score(y_true[:, i], y_pred[:, i]) for i, cat in enumerate(all_categories)}\n    }\n\ndef run_multiple_model_evaluation(train_file_path: str, models_to_test: List[str] = None):\n    \"\"\"\n    Run evaluation on multiple models and compare results\n    \"\"\"\n    if models_to_test is None:\n        models_to_test = ['qwen2.5-unsloth', 'deepseek-r1']  # Default to new models\n    \n    results = {}\n    \n    for model_key in models_to_test:\n        print(f\"\\n{'='*100}\")\n        print(f\"üîÑ Evaluating Model: {model_key}\")\n        print(f\"{'='*100}\")\n        \n        try:\n            model_results = evaluate_model_on_training_data(\n                train_file_path=train_file_path,\n                model_key=model_key,\n                use_quantization=True,\n                max_new_tokens=8000\n            )\n            \n            if model_results:\n                results[model_key] = model_results\n                print(f\"‚úÖ {model_key} evaluation completed successfully!\")\n            else:\n                print(f\"‚ùå {model_key} evaluation failed!\")\n                \n        except Exception as e:\n            print(f\"‚ùå Error evaluating {model_key}: {e}\")\n            continue\n        \n        # Clean up memory between models\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    \n    # Print comparison results\n    if results:\n        print(f\"\\n{'='*100}\")\n        print(\"üìä COMPARISON RESULTS\")\n        print(f\"{'='*100}\")\n        \n        print(f\"{'Model':<20} {'Weighted F1':<15} {'Jaccard Score':<15}\")\n        print(\"-\" * 50)\n        \n        for model_key, result in results.items():\n            print(f\"{model_key:<20} {result['weighted_f1']:<15.4f} {result['jaccard_score']:<15.4f}\")\n    \n    return results\n\ndef compare_all_models(train_file_path: str, use_quantization: bool = True, max_new_tokens: int = 6000):\n    \"\"\"\n    Compare performance of all available models on the training dataset\n    \"\"\"\n    models_to_test = ['qwen2.5-unsloth', 'deepseek-r1']\n    results = {}\n    \n    print(\"üöÄ Starting comprehensive model comparison...\")\n    print(\"=\" * 80)\n    \n    for model_key in models_to_test:\n        print(f\"\\nüîÑ Testing {model_key}...\")\n        try:\n            result = evaluate_model_on_training_data(\n                train_file_path=train_file_path,\n                model_key=model_key,\n                use_quantization=use_quantization,\n                max_new_tokens=max_new_tokens\n            )\n            if result:\n                results[model_key] = result\n            \n            # Clear memory between models\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n        except Exception as e:\n            print(f\"‚ùå Error testing {model_key}: {e}\")\n            continue\n    \n    # Print comparison results\n    print(\"\\n\" + \"=\" * 80)\n    print(\"üìä MODEL COMPARISON RESULTS\")\n    print(\"=\" * 80)\n    \n    for model_key, result in results.items():\n        print(f\"\\n{model_key.upper()}:\")\n        print(f\"  Weighted F1 Score: {result['weighted_f1']:.4f}\")\n        print(f\"  Jaccard Score: {result['jaccard_score']:.4f}\")\n    \n    # Find best model\n    if results:\n        best_model = max(results.keys(), key=lambda k: results[k]['weighted_f1'])\n        print(f\"\\nüèÜ BEST MODEL: {best_model}\")\n        print(f\"   Weighted F1: {results[best_model]['weighted_f1']:.4f}\")\n        print(f\"   Jaccard Score: {results[best_model]['jaccard_score']:.4f}\")\n    \n    return results\n\ndef install_unsloth():\n    \"\"\"\n    Install Unsloth for optimized inference (run this once)\n    \"\"\"\n    install_commands = [\n        \"pip install unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\",\n        \"pip install --no-deps trl peft accelerate bitsandbytes\"\n    ]\n    \n    print(\"üì¶ To install Unsloth for optimized inference, run these commands:\")\n    for cmd in install_commands:\n        print(f\"   {cmd}\")\n    \n    print(\"\\n‚ö†Ô∏è  Note: Restart your runtime after installation!\")\n\ndef benchmark_inference_speed(model_key: str, sample_questions: List[str], use_quantization: bool = True):\n    \"\"\"\n    Benchmark inference speed for a specific model\n    \n    Args:\n        model_key: Model to benchmark\n        sample_questions: List of sample questions to test\n        use_quantization: Whether to use quantization\n    \"\"\"\n    import time\n    \n    print(f\"üöÄ Benchmarking {model_key} inference speed...\")\n    \n    classifier = MultiModelArabicMedicalClassifier(\n        model_key=model_key,\n        use_quantization=use_quantization\n    )\n    \n    # Warm-up run\n    classifier.classify_question(sample_questions[0], max_new_tokens=2000)\n    \n    # Benchmark runs\n    start_time = time.time()\n    total_tokens = 0\n    \n    for question in sample_questions:\n        result = classifier.classify_question(question, max_new_tokens=2000)\n        total_tokens += len(classifier.tokenizer.encode(question))\n    \n    end_time = time.time()\n    total_time = end_time - start_time\n    \n    print(f\"‚è±Ô∏è  Benchmark Results for {model_key}:\")\n    print(f\"   Total Time: {total_time:.2f} seconds\")\n    print(f\"   Samples Processed: {len(sample_questions)}\")\n    print(f\"   Average Time per Sample: {total_time/len(sample_questions):.2f} seconds\")\n    print(f\"   Total Input Tokens: {total_tokens}\")\n    print(f\"   Tokens per Second: {total_tokens/total_time:.2f}\")\n    \n    classifier.cleanup_memory()\n    return {\n        'model_key': model_key,\n        'total_time': total_time,\n        'samples': len(sample_questions),\n        'avg_time_per_sample': total_time/len(sample_questions),\n        'tokens_per_second': total_tokens/total_time\n    }\n\ndef create_optimized_pipeline(model_key: str = 'qwen2.5-unsloth', batch_size: int = 4):\n    \"\"\"\n    Create an optimized inference pipeline for batch processing\n    \n    Args:\n        model_key: Model to use for the pipeline\n        batch_size: Number of samples to process in each batch\n    \n    Returns:\n        Optimized classifier instance\n    \"\"\"\n    print(f\"üîß Creating optimized pipeline with {model_key}...\")\n    \n    classifier = MultiModelArabicMedicalClassifier(\n        model_key=model_key,\n        use_quantization=True,\n        use_thinking_mode=False  # Disable for faster inference\n    )\n    \n    # Set model to evaluation mode for inference\n    classifier.model.eval()\n    \n    # Enable torch.compile for PyTorch 2.0+ (if available)\n    try:\n        import torch._dynamo\n        classifier.model = torch.compile(classifier.model, mode=\"reduce-overhead\")\n        print(\"‚úÖ Torch compile enabled for faster inference\")\n    except:\n        print(\"‚ö†Ô∏è  Torch compile not available, using standard inference\")\n    \n    print(f\"‚úÖ Optimized pipeline ready with batch size: {batch_size}\")\n    return classifier\n\ndef test_new_models():\n    \"\"\"\n    Test the newly added models with sample data\n    \"\"\"\n    sample_questions = [\n        \"ŸÖÿß ÿ£ÿ≥ÿ®ÿßÿ® ÿßŸÑÿµÿØÿßÿπ ÿßŸÑŸÖÿ≥ÿ™ŸÖÿ±ÿü\",\n        \"ŸÉŸäŸÅ ŸäŸÖŸÉŸÜ ÿπŸÑÿßÿ¨ ÿ¢ŸÑÿßŸÖ ÿßŸÑŸÖŸÅÿßÿµŸÑÿü\",\n        \"ŸÖÿß ŸáŸä Ÿàÿ∏ÿßÿ¶ŸÅ ÿßŸÑŸÉÿ®ÿØ ŸÅŸä ÿßŸÑÿ¨ÿ≥ŸÖÿü\",\n        \"ŸáŸÑ ŸäŸÖŸÉŸÜŸÉ ÿ£ŸÜ ÿ™ŸÜÿµÿ≠ŸÜŸä ÿ®ÿ∑ÿ®Ÿäÿ® ÿ¨ŸäÿØ ŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿ£ÿ∑ŸÅÿßŸÑÿü\",\n        \"ŸÖÿß ŸáŸä ÿ£ŸÅÿ∂ŸÑ ÿßŸÑÿ™ŸÖÿßÿ±ŸäŸÜ ŸÑŸÑÿ≠ŸÅÿßÿ∏ ÿπŸÑŸâ ÿßŸÑÿµÿ≠ÿ©ÿü\"\n    ]\n    \n    models_to_test = ['qwen2.5-unsloth', 'deepseek-r1']\n    \n    for model_key in models_to_test:\n        print(f\"\\n{'='*50}\")\n        print(f\"Testing {model_key}\")\n        print(f\"{'='*50}\")\n        \n        try:\n            classifier = MultiModelArabicMedicalClassifier(\n                model_key=model_key,\n                use_quantization=True\n            )\n            \n            for i, question in enumerate(sample_questions):\n                result = classifier.classify_question(question, max_new_tokens=3000)\n                print(f\"\\nSample {i+1}: {question}\")\n                print(f\"Classification: {result}\")\n            \n            classifier.cleanup_memory()\n            \n        except Exception as e:\n            print(f\"‚ùå Error testing {model_key}: {e}\")\n\nif __name__ == \"__main__\":\n    # Update this path to your training dataset\n    TRAIN_DATASET_PATH = '/kaggle/input/train-dataset/Train_Dev.tsv'  # Replace with actual path\n\n    # List available models\n    MultiModelArabicMedicalClassifier.list_available_models()\n    \n    # Show installation instructions for Unsloth\n    print(\"\\n\" + \"=\"*80)\n    print(\"üöÄ INSTALLATION GUIDE\")\n    print(\"=\"*80)\n    install_unsloth()\n\n    # Evaluate with Unsloth Qwen2.5 (recommended for speed)\n    # print(\"\\n\" + \"=\" * 50)\n    # print(\"Testing Unsloth Qwen2.5-7B-Instruct\")\n    # print(\"=\" * 50)\n    # results_unsloth = evaluate_model_on_training_data(\n    #     train_file_path=TRAIN_DATASET_PATH,\n    #     model_key='qwen2.5-unsloth',\n    #     use_quantization=True,\n    #     max_new_tokens=6000\n    # )\n\n    # Evaluate with DeepSeek R1 (recommended for reasoning)\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Testing DeepSeek-R1-Distill-Qwen-7B\")\n    print(\"=\" * 50)\n    results_deepseek = evaluate_model_on_training_data(\n        train_file_path=TRAIN_DATASET_PATH,\n        model_key='deepseek-r1',\n        use_quantization=True,\n        max_new_tokens=6000\n    )\n    \n    # Uncomment to run comprehensive comparison\n    # comprehensive_results = compare_all_models(\n    #     train_file_path=TRAIN_DATASET_PATH,\n    #     use_quantization=True,\n    #     max_new_tokens=6000\n    # )\n\n    # Print summary if both models were tested\n    if results_deepseek:\n        print(\"\\n\" + \"=\" * 80)\n        \n        print(f\"\\nDeepSeek R1 Distill Qwen-7B:\")\n        print(f\"  Weighted F1: {results_deepseek['weighted_f1']:.4f}\")\n        print(f\"  Jaccard Score: {results_deepseek['jaccard_score']:.4f}\")\n        print(f\"  Advantage: Advanced reasoning capabilities\")\n        \n        \n        print(f\"\\nüí° Recommendation:\")\n        print(f\"   - Use 'qwen2.5-unsloth' for faster inference and production deployment\")\n        print(f\"   - Use 'deepseek-r1' for complex reasoning tasks and research\")\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"üß™ TESTING NEW MODELS\")\n    print(\"=\"*80)\n    # Uncomment to test the new models with sample data\n    # test_new_models()\n    \n    print(\"\\n‚úÖ Setup complete! The classifier now supports:\")\n    print(\"   - unsloth/Qwen2.5-7B-Instruct (Optimized for speed)\")\n    print(\"   - deepseek-ai/DeepSeek-R1-Distill-Qwen-7B (Advanced reasoning)\")\n    print(\"\\nüí° Use 'qwen2.5-unsloth' as default for best performance!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-27T08:35:32.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}