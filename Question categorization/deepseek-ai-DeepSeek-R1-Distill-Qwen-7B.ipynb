{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12479800,"sourceType":"datasetVersion","datasetId":7874336},{"sourceId":12589423,"sourceType":"datasetVersion","datasetId":7951272}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:34:43.274725Z","iopub.execute_input":"2025-07-27T08:34:43.275031Z","iopub.status.idle":"2025-07-27T08:34:43.291282Z","shell.execute_reply.started":"2025-07-27T08:34:43.275005Z","shell.execute_reply":"2025-07-27T08:34:43.290627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers>=4.51.0 torch torchvision torchaudio accelerate bitsandbytes -q\n!pip install sentencepiece protobuf -q\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport gc\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\n# Check GPU availability\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:34:43.292522Z","iopub.execute_input":"2025-07-27T08:34:43.292762Z","execution_failed":"2025-07-27T08:35:32.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nimport re\nimport pandas as pd\nimport numpy as np\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    BitsAndBytesConfig\n)\nfrom typing import List, Dict, Tuple, Any, Optional\nimport warnings\nfrom sklearn.metrics import f1_score, jaccard_score\nfrom huggingface_hub import login\nimport ast\n\nHF_TOKEN = \"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\"\nlogin(HF_TOKEN)\nwarnings.filterwarnings('ignore')\n\nclass MultiModelArabicMedicalClassifier:\n    \"\"\"\n    Arabic Medical Question Classifier supporting multiple LLM models including Unsloth and DeepSeek\n    \"\"\"\n    \n    # Model configurations with their specific settings\n    MODEL_CONFIGS = {\n        'qwen3': {\n            'name': 'Qwen/Qwen3-14B',\n            'type': 'qwen',\n            'context_length': 32768,\n            'description': 'Qwen3 14B - Latest Qwen model'\n        },\n        'jais': {\n            'name': 'core42/jais-13b-chat',\n            'type': 'jais',\n            'context_length': 2048,\n            'description': 'JAIS - Arabic-focused LLM'\n        },\n        'qwen2': {\n            'name': 'Qwen/Qwen2-7B-Instruct',\n            'type': 'qwen',\n            'context_length': 32768,\n            'description': 'Qwen2 7B - Strong multilingual model'\n        },\n        'qwen2.5': {\n            'name': 'unsloth/Qwen2.5-7B-Instruct',\n            'type': 'qwen',\n            'context_length': 131072,\n            'description': 'Qwen2.5 7B Instruct - Unsloth optimized'\n        },\n        'qwen2.5-unsloth': {\n            'name': 'unsloth/Qwen2.5-7B-Instruct',\n            'type': 'qwen',\n            'context_length': 131072,\n            'description': 'Unsloth Qwen2.5 7B - Optimized for inference speed'\n        },\n        'deepseek-r1': {\n            'name': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n            'type': 'qwen',\n            'context_length': 131072,\n            'description': 'DeepSeek R1 Distill Qwen 7B - Advanced reasoning model'\n        },\n        'deepseek_r1': {\n            'name': 'DeepSeek-R1-Distill-Qwen-7B',\n            'type': 'qwen',\n            'context_length': 32768,\n            'description': 'DeepSeek R1 Distill Qwen 7B - Reasoning optimized'\n        },\n        'llama3': {\n            'name': 'meta-llama/Llama-3.1-8B-Instruct',\n            'type': 'llama',\n            'context_length': 128000,\n            'description': 'Llama 3.1 8B - Meta\\'s latest model'\n        },\n        'mixtral': {\n            'name': 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n            'type': 'mixtral',\n            'context_length': 32768,\n            'description': 'Mixtral 8x7B - Mixture of Experts'\n        },\n        'command_r': {\n            'name': 'CohereForAI/c4ai-command-r-v01',\n            'type': 'command_r',\n            'context_length': 128000,\n            'description': 'Command R - Cohere\\'s multilingual model'\n        },\n        'gemma2': {\n            'name': 'google/gemma-2-9b-it',\n            'type': 'gemma',\n            'context_length': 8192,\n            'description': 'Gemma 2 9B - Google\\'s instruction-tuned model'\n        },\n        'phi3': {\n            'name': 'microsoft/Phi-3-medium-14b-instruct',\n            'type': 'phi',\n            'context_length': 128000,\n            'description': 'Phi-3 Medium - Microsoft\\'s efficient model'\n        }\n    }\n    \n    def __init__(self, model_key: str = 'qwen2.5-unsloth', use_quantization: bool = True, use_thinking_mode: bool = True):\n        \"\"\"\n        Initialize classifier with specified model\n        \n        Args:\n            model_key: Key from MODEL_CONFIGS (default: qwen2.5-unsloth for better performance)\n            use_quantization: Whether to use 4-bit quantization\n            use_thinking_mode: Enable thinking mode for supported models\n        \"\"\"\n        if model_key not in self.MODEL_CONFIGS:\n            raise ValueError(f\"Model '{model_key}' not supported. Available models: {list(self.MODEL_CONFIGS.keys())}\")\n        \n        self.model_config = self.MODEL_CONFIGS[model_key]\n        self.model_key = model_key\n        self.use_quantization = use_quantization\n        self.use_thinking_mode = use_thinking_mode\n        self.tokenizer = None\n        self.model = None\n        \n        self.question_categories = {\n            'A': 'Diagnosis (questions about interpreting clinical findings)',\n            'B': 'Treatment (questions about seeking treatments)', \n            'C': 'Anatomy and Physiology (questions about basic medical knowledge)',\n            'D': 'Epidemiology (questions about the course, prognosis, and etiology of diseases)',\n            'E': 'Healthy Lifestyle (questions related to diet, exercise, and mood control)',\n            'F': 'Provider Choices (questions seeking recommendations for medical professionals and facilities)',\n            'Z': 'Other (questions that do not fall under the above-mentioned categories)'\n        }\n        \n        self.load_model()\n    \n    def load_model(self):\n        \"\"\"Load the specified model with appropriate configurations\"\"\"\n        print(f\"Loading {self.model_config['description']}...\")\n        print(f\"Model: {self.model_config['name']}\")\n        \n        quantization_config = None\n        if self.use_quantization:\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\"\n            )\n        \n        try:\n            # Special handling for Unsloth models\n            if 'unsloth' in self.model_config['name']:\n                print(\"ğŸš€ Loading Unsloth optimized model...\")\n                try:\n                    # Try to use Unsloth's FastLanguageModel if available\n                    from unsloth import FastLanguageModel\n                    self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n                        model_name=self.model_config['name'],\n                        max_seq_length=self.model_config['context_length'],\n                        dtype=torch.float16,\n                        load_in_4bit=self.use_quantization,\n                    )\n                    FastLanguageModel.for_inference(self.model)  # Enable native 2x faster inference\n                    print(\"âœ… Unsloth FastLanguageModel loaded successfully!\")\n                except ImportError:\n                    print(\"âš ï¸ Unsloth not available, falling back to standard transformers...\")\n                    # Fallback to standard transformers\n                    self._load_standard_model(quantization_config)\n            else:\n                self._load_standard_model(quantization_config)\n            \n            # Set pad token if not available\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n                \n            print(f\"âœ… {self.model_config['description']} loaded successfully!\")\n            self.print_model_info()\n            \n        except Exception as e:\n            print(f\"âŒ Error loading model: {e}\")\n            raise\n    \n    def _load_standard_model(self, quantization_config):\n        \"\"\"Load model using standard transformers\"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.model_config['name'],\n            trust_remote_code=True,\n            padding_side='left'\n        )\n        \n        model_kwargs = {\n            'trust_remote_code': True,\n            'torch_dtype': torch.float16,\n            'low_cpu_mem_usage': True,\n            'device_map': 'auto'\n        }\n        \n        if quantization_config:\n            model_kwargs['quantization_config'] = quantization_config\n        \n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_config['name'],\n            **model_kwargs\n        )\n    \n    def print_model_info(self):\n        \"\"\"Print model and memory information\"\"\"\n        print(f\"\\nğŸ“Š Model Information:\")\n        print(f\"Model: {self.model_config['name']}\")\n        print(f\"Type: {self.model_config['type']}\")\n        print(f\"Context Length: {self.model_config['context_length']:,} tokens\")\n        print(f\"Quantization: {'4-bit' if self.use_quantization else 'Full precision'}\")\n        print(f\"Thinking Mode: {'Enabled' if self.use_thinking_mode else 'Disabled'}\")\n        print(f\"Tokenizer Vocab Size: {len(self.tokenizer):,}\")\n        \n        if torch.cuda.is_available():\n            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n    \n    def create_prompt(self, question: str) -> str:\n        \"\"\"Create model-specific prompt for classification\"\"\"\n        category_descriptions = \"\"\"\nÙØ¦Ø§Øª Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø·Ø¨ÙŠØ©:\n(A) Ø§Ù„ØªØ´Ø®ÙŠØµ - Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„ ØªÙØ³ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø³Ø±ÙŠØ±ÙŠØ© ÙˆØ§Ù„Ø£Ø¹Ø±Ø§Ø¶\n(B) Ø§Ù„Ø¹Ù„Ø§Ø¬ - Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù„Ø§Ø¬Ø§Øª ÙˆØ·Ø±Ù‚ Ø§Ù„Ø¹Ù„Ø§Ø¬\n(C) Ø§Ù„ØªØ´Ø±ÙŠØ­ ÙˆØ¹Ù„Ù… ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡ - Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n(D) Ø¹Ù„Ù… Ø§Ù„Ø£ÙˆØ¨Ø¦Ø© - Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø±Ø¶ ÙˆØªØ´Ø®ÙŠØµÙ‡ ÙˆØ£Ø³Ø¨Ø§Ø¨Ù‡\n(E) Ù†Ù…Ø· Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ØµØ­ÙŠ - Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØºØ°Ø§Ø¦ÙŠ ÙˆØ§Ù„Ø±ÙŠØ§Ø¶Ø© ÙˆØ§Ù„ØµØ­Ø© Ø§Ù„Ù†ÙØ³ÙŠØ©\n(F) Ø§Ø®ØªÙŠØ§Ø± Ù…Ù‚Ø¯Ù… Ø§Ù„Ø±Ø¹Ø§ÙŠØ© - Ø£Ø³Ø¦Ù„Ø© ØªØ·Ù„Ø¨ ØªÙˆØµÙŠØ§Øª Ù„Ù„Ù…Ù‡Ù†ÙŠÙŠÙ† Ø§Ù„Ø·Ø¨ÙŠÙŠÙ† ÙˆØ§Ù„Ù…Ø±Ø§ÙÙ‚\n(Z) Ø£Ø®Ø±Ù‰ - Ø£Ø³Ø¦Ù„Ø© Ù„Ø§ ØªÙ†Ø¯Ø±Ø¬ ØªØ­Øª Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© Ø£Ø¹Ù„Ø§Ù‡\n\"\"\"\n        \n        # Enhanced prompt for DeepSeek R1 model with reasoning capabilities\n        if self.model_key in ['deepseek-r1', 'deepseek_r1']:\n            base_prompt = f\"\"\"Ø£Ù†Øª Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø·Ø¨ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù„Ø¯ÙŠÙƒ Ù‚Ø¯Ø±Ø§Øª ØªÙÙƒÙŠØ± Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚.\n\n{category_descriptions}\n\nØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØµÙ†ÙŠÙÙ‡:\n{question}\n\nØªØ¹Ù„ÙŠÙ…Ø§Øª Ø®Ø§ØµØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…:\n1. Ø§Ø³ØªØ®Ø¯Ù… Ù‚Ø¯Ø±Ø§ØªÙƒ ÙÙŠ Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¹Ù…Ù‚\n2. Ø­Ù„Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø·Ø¨ÙŠ ÙˆØ§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©\n3. ÙÙƒØ± ÙÙŠ Ø§Ù„Ù‡Ø¯Ù Ù…Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆÙ†ÙˆØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n4. Ø§Ø¹ØªØ¨Ø± Ø§Ù„ØªØ¯Ø§Ø®Ù„ Ø¨ÙŠÙ† Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\n5. Ø§Ø´Ø±Ø­ Ù…Ù†Ø·Ù‚ ØªÙÙƒÙŠØ±Ùƒ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©\n6. Ø­Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙˆØ§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©\n7. ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ù‚Ø¯Ù… Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨\n\nØ§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:\n\"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [Ø£Ø­Ø±Ù Ø§Ù„ÙØ¦Ø§Øª Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„]\"\n\nÙ…Ø«Ø§Ù„: Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„ØªØ´Ø®ÙŠØµ ÙˆØ§Ù„Ø¹Ù„Ø§Ø¬ØŒ ÙØ§ÙƒØªØ¨: \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [A,B]\"\n\"\"\"\n        elif self.use_thinking_mode:\n            base_prompt = f\"\"\"Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø·Ø¨ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ù„Ù‰ ÙØ¦Ø© Ø£Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.\n\n{category_descriptions}\n\nØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØµÙ†ÙŠÙÙ‡:\n{question}\n\nØªØ¹Ù„ÙŠÙ…Ø§Øª:\n1. ÙÙƒØ± Ø¨Ø¹Ù…Ù‚ ÙÙŠ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ­Ù„Ù„ ÙƒÙ„ Ø¬Ø§Ù†Ø¨ Ù…Ù†Ù‡\n2. Ø§Ø´Ø±Ø­ ØªÙÙƒÙŠØ±Ùƒ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©\n3. Ø­Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙˆØ§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø·Ø¨ÙŠØ©\n4. Ø§Ø±Ø¨Ø· Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ø¹ Ø§Ù„ØªØ¨Ø±ÙŠØ±\n5. Ø§Ø®ØªØ± Ø§Ù„ÙØ¦Ø© Ø£Ùˆ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø£Ù†Ø³Ø¨ (ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø£ÙƒØ«Ø± Ù…Ù† ÙØ¦Ø©)\n6. ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ø§ÙƒØªØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ:\n   \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [A,B,C]\" (Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„)\n\nÙ…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙÙƒÙŠØ± ÙˆØ§Ù„ØªÙ†Ø³ÙŠÙ‚:\n- Ø­Ù„Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„: \"Ù…Ø§ Ø³Ø¨Ø¨ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø±Ø¶ØŸ\"\n- Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: \"Ø³Ø¨Ø¨ØŒ Ø¹Ø±Ø¶\"\n- Ø§Ù„ØªÙÙƒÙŠØ±: Ù‡Ø°Ø§ Ø³Ø¤Ø§Ù„ ÙŠØªØ¹Ù„Ù‚ Ø¨ØªØ­Ø¯ÙŠØ¯ Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ØŒ ÙˆÙ‡Ùˆ Ø¬Ø²Ø¡ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ´Ø®ÙŠØµ\n- Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [A]\n\nØ§Ù„Ø¢Ù† Ø­Ù„Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø£Ø¹Ù„Ø§Ù‡ Ø¨Ù†ÙØ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©.\n\"\"\"\n        else:\n            base_prompt = f\"\"\"Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø·Ø¨ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ù„Ù‰ ÙØ¦Ø© Ø£Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.\n\n{category_descriptions}\n\nØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØµÙ†ÙŠÙÙ‡:\n{question}\n\nØªØ¹Ù„ÙŠÙ…Ø§Øª:\n1. Ø§Ù‚Ø±Ø£ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¹Ù†Ø§ÙŠØ© ÙˆØ­Ù„Ù„ Ù…Ø­ØªÙˆØ§Ù‡\n2. Ø­Ø¯Ø¯ Ø§Ù„ÙØ¦Ø© Ø£Ùˆ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© (ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø£ÙƒØ«Ø± Ù…Ù† ÙØ¦Ø© ÙˆØ§Ø­Ø¯Ø©)\n3. Ø§Ø´Ø±Ø­ Ø³Ø¨Ø¨ Ø§Ø®ØªÙŠØ§Ø±Ùƒ Ù„ÙƒÙ„ ÙØ¦Ø©\n4. ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ø§ÙƒØªØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ:\n   \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [A,B,C]\" (Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„)\n\nÙ…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚:\n- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„ØªØ´Ø®ÙŠØµ ÙÙ‚Ø·: \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [A]\"\n- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„ØªØ´Ø®ÙŠØµ ÙˆØ§Ù„Ø¹Ù„Ø§Ø¬: \"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: [A,B]\"\n\"\"\"\n        \n        # Apply chat template for Qwen-based models (including Unsloth and DeepSeek)\n        if self.model_config['type'] == 'qwen':\n            messages = [{\"role\": \"user\", \"content\": base_prompt}]\n            try:\n                return self.tokenizer.apply_chat_template(\n                    messages, \n                    tokenize=False, \n                    add_generation_prompt=True,\n                    enable_thinking=self.use_thinking_mode if hasattr(self.tokenizer, 'enable_thinking') else False\n                )\n            except:\n                # Fallback for models that don't support thinking mode\n                return self.tokenizer.apply_chat_template(\n                    messages, \n                    tokenize=False, \n                    add_generation_prompt=True\n                )\n        elif self.model_config['type'] in ['llama', 'mixtral']:\n            return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{base_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        elif self.model_config['type'] == 'jais':\n            return f\"### Instruction: {base_prompt}\\n### Response:\"\n        elif self.model_config['type'] == 'gemma':\n            return f\"<start_of_turn>user\\n{base_prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n        elif self.model_config['type'] == 'phi':\n            return f\"<|user|>\\n{base_prompt}<|end|>\\n<|assistant|>\\n\"\n        else:\n            return f\"Human: {base_prompt}\\n\\nAssistant:\"\n    \n    def classify_question(self, question: str, max_new_tokens: int = 8000) -> List[str]:\n        \"\"\"\n        Classify Arabic medical question into categories\n        \"\"\"\n        prompt = self.create_prompt(question)\n        \n        inputs = self.tokenizer(\n            prompt, \n            return_tensors=\"pt\", \n            truncation=True,\n            max_length=self.model_config['context_length'] - max_new_tokens\n        ).to(self.model.device)\n        \n        # Optimized generation parameters for different models\n        gen_kwargs = {\n            'max_new_tokens': max_new_tokens,\n            'temperature': 0.7,\n            'top_p': 0.9,\n            'top_k': 50,\n            'do_sample': True,\n            'pad_token_id': self.tokenizer.pad_token_id,\n            'eos_token_id': self.tokenizer.eos_token_id,\n            'repetition_penalty': 1.1\n        }\n        \n        # Model-specific optimizations\n        if self.model_key in ['deepseek-r1', 'deepseek_r1']:\n            # DeepSeek R1 optimized parameters for reasoning\n            gen_kwargs.update({\n                'temperature': 0.3,  # Lower temperature for more focused reasoning\n                'top_p': 0.9,\n                'top_k': 40,\n                'repetition_penalty': 1.05\n            })\n        elif self.model_key in ['qwen2.5-unsloth', 'qwen2.5']:\n            # Unsloth Qwen2.5 optimized parameters\n            gen_kwargs.update({\n                'temperature': 0.6,\n                'top_p': 0.95,\n                'top_k': 30,\n                'repetition_penalty': 1.08\n            })\n        elif self.model_config['type'] == 'qwen':\n            gen_kwargs.update({\n                'temperature': 0.6,\n                'top_p': 0.95,\n                'top_k': 20\n            })\n        elif self.model_config['type'] == 'llama':\n            gen_kwargs.update({\n                'temperature': 0.8,\n                'top_p': 0.95\n            })\n        \n        with torch.no_grad():\n            generated_ids = self.model.generate(\n                **inputs,\n                **gen_kwargs\n            )\n        \n        output_ids = generated_ids[0][len(inputs.input_ids[0]):]\n        response = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n        \n        categories = self.extract_question_categories(response)\n        return categories\n    \n    def extract_question_categories(self, response: str) -> List[str]:\n        \"\"\"Extract question categories from Arabic response text\"\"\"\n        patterns = [\n            r'Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'Ø§Ù„ÙØ¦Ø§Øª:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'Ø§Ù„ØªØµÙ†ÙŠÙ:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'Ø§Ù„Ù†ØªÙŠØ¬Ø©:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n        ]\n        \n        for pattern in patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                categories_str = match.group(1)\n                categories = [cat.strip().upper() for cat in categories_str.split(',')]\n                return [cat for cat in categories if cat in ['A', 'B', 'C', 'D', 'E', 'F', 'Z']]\n        \n        english_patterns = [\n            r'Final Classification:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'Categories:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'Classification:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'Answer:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n        ]\n        \n        for pattern in english_patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                categories_str = match.group(1)\n                categories = [cat.strip().upper() for cat in categories_str.split(',')]\n                return [cat for cat in categories if cat in ['A', 'B', 'C', 'D', 'E', 'F', 'Z']]\n        \n        found_categories = []\n        for category in ['A', 'B', 'C', 'D', 'E', 'F', 'Z']:\n            if f'({category})' in response or f'[{category}]' in response or f' {category} ' in response:\n                found_categories.append(category)\n        \n        return found_categories if found_categories else ['Z']\n    \n    def process_test_dataset(self, df: pd.DataFrame, max_new_tokens: int = 8000, show_progress: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Process test dataset for question classification\n        \"\"\"\n        print(f\"ğŸš€ Starting Arabic Medical Question Classification with {self.model_config['description']}...\")\n        print(f\"Test dataset size: {len(df)} samples\")\n        print(\"-\" * 80)\n        \n        predictions = []\n        \n        for idx, row in df.iterrows():\n            if show_progress and idx % 10 == 0:\n                print(f\"Processing sample {idx+1}/{len(df)} - Current GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            \n            try:\n                categories = self.classify_question(row['question'], max_new_tokens=max_new_tokens)\n                prediction_str = ', '.join(sorted(categories))\n                predictions.append(prediction_str)\n                \n                if show_progress and idx < 3:\n                    print(f\"Sample {idx+1} prediction: {prediction_str}\")\n                    \n            except Exception as e:\n                print(f\"Error processing question {idx}: {e}\")\n                predictions.append('Z')\n            \n            if idx % 20 == 0:\n                self.cleanup_memory()\n        \n        result_df = pd.DataFrame({'prediction': predictions})\n        return result_df\n    \n    def cleanup_memory(self):\n        \"\"\"Clean up GPU memory\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    \n    @classmethod\n    def list_available_models(cls):\n        \"\"\"List all available models with descriptions\"\"\"\n        print(\"ğŸ“‹ Available Models:\")\n        print(\"-\" * 80)\n        \n        for key, config in cls.MODEL_CONFIGS.items():\n            print(f\"Key: '{key}'\")\n            print(f\"  Model: {config['name']}\")\n            print(f\"  Description: {config['description']}\")\n            print(f\"  Context Length: {config['context_length']:,} tokens\")\n            print(f\"  Type: {config['type']}\")\n            print()\n\ndef evaluate_model_on_training_data(\n    train_file_path: str,\n    model_key: str = 'qwen2.5-unsloth',\n    use_quantization: bool = True,\n    max_new_tokens: int = 8000\n) -> dict:\n    \"\"\"\n    Evaluate the model on the training dataset using Weighted F1 Score and Jaccard Score.\n\n    Args:\n        train_file_path: Path to the training dataset (TSV with 'question' and 'final_QT' columns)\n        model_key: Model key from MODEL_CONFIGS (default: qwen2.5-unsloth)\n        use_quantization: Use 4-bit quantization\n        max_new_tokens: Maximum tokens to generate per question\n\n    Returns:\n        dict: Dictionary containing evaluation metrics (Weighted F1 Score, Jaccard Score)\n    \"\"\"\n    \n    # Initialize the classifier\n    print(f\"ğŸš€ Initializing {model_key} for evaluation...\")\n    try:\n        classifier = MultiModelArabicMedicalClassifier(\n            model_key=model_key,\n            use_quantization=use_quantization\n        )\n    except Exception as e:\n        print(f\"âŒ Failed to initialize model: {e}\")\n        return None\n\n    # Load training dataset\n    try:\n        df = pd.read_csv(train_file_path, sep='\\t')\n        if 'question' not in df.columns or 'final_QT' not in df.columns:\n            raise ValueError(\"Dataset must contain 'question' and 'final_QT' columns\")\n        print(f\"âœ… Training dataset loaded: {len(df)} samples\")\n    except Exception as e:\n        print(f\"âŒ Error loading training dataset: {e}\")\n        return None\n\n    # Parse final_QT labels\n    def parse_labels(label):\n        if isinstance(label, str):\n            try:\n                if label.startswith('['):\n                    return ast.literal_eval(label)\n                else:\n                    return [cat.strip() for cat in label.split(',')]\n            except:\n                print(f\"Warning: Could not parse label '{label}', defaulting to ['Z']\")\n                return ['Z']\n        return label\n\n    df['final_QT'] = df['final_QT'].apply(parse_labels)\n\n    # Generate predictions\n    print(f\"ğŸš€ Generating predictions for {len(df)} samples...\")\n    predictions = classifier.process_test_dataset(df, max_new_tokens=max_new_tokens)\n\n    # Convert predictions and true labels to multi-label binary format\n    all_categories = ['A', 'B', 'C', 'D', 'E', 'F', 'Z']\n    y_true = []\n    y_pred = []\n\n    for true_labels, pred_labels in zip(df['final_QT'], predictions['prediction']):\n        true_vec = [1 if cat in true_labels else 0 for cat in all_categories]\n        y_true.append(true_vec)\n        pred_cats = [cat.strip() for cat in pred_labels.split(',') if cat.strip() in all_categories]\n        pred_vec = [1 if cat in pred_cats else 0 for cat in all_categories]\n        y_pred.append(pred_vec)\n\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate metrics\n    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n    jaccard = jaccard_score(y_true, y_pred, average='samples')\n\n    # Print results\n    print(\"\\nğŸ“Š Evaluation Results:\")\n    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n    print(f\"Jaccard Score (samples): {jaccard:.4f}\")\n\n    # Detailed per-category metrics\n    print(\"\\nğŸ“‹ Per-Category Metrics:\")\n    for i, cat in enumerate(all_categories):\n        cat_f1 = f1_score(y_true[:, i], y_pred[:, i])\n        cat_jaccard = jaccard_score(y_true[:, i], y_pred[:, i])\n        print(f\"Category {cat}:\")\n        print(f\"  F1 Score: {cat_f1:.4f}\")\n        print(f\"  Jaccard Score: {cat_jaccard:.4f}\")\n\n    # Cleanup memory\n    classifier.cleanup_memory()\n\n    return {\n        'weighted_f1': weighted_f1,\n        'jaccard_score': jaccard,\n        'per_category_f1': {cat: f1_score(y_true[:, i], y_pred[:, i]) for i, cat in enumerate(all_categories)},\n        'per_category_jaccard': {cat: jaccard_score(y_true[:, i], y_pred[:, i]) for i, cat in enumerate(all_categories)}\n    }\n\ndef run_multiple_model_evaluation(train_file_path: str, models_to_test: List[str] = None):\n    \"\"\"\n    Run evaluation on multiple models and compare results\n    \"\"\"\n    if models_to_test is None:\n        models_to_test = ['qwen2.5-unsloth', 'deepseek-r1']  # Default to new models\n    \n    results = {}\n    \n    for model_key in models_to_test:\n        print(f\"\\n{'='*100}\")\n        print(f\"ğŸ”„ Evaluating Model: {model_key}\")\n        print(f\"{'='*100}\")\n        \n        try:\n            model_results = evaluate_model_on_training_data(\n                train_file_path=train_file_path,\n                model_key=model_key,\n                use_quantization=True,\n                max_new_tokens=8000\n            )\n            \n            if model_results:\n                results[model_key] = model_results\n                print(f\"âœ… {model_key} evaluation completed successfully!\")\n            else:\n                print(f\"âŒ {model_key} evaluation failed!\")\n                \n        except Exception as e:\n            print(f\"âŒ Error evaluating {model_key}: {e}\")\n            continue\n        \n        # Clean up memory between models\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    \n    # Print comparison results\n    if results:\n        print(f\"\\n{'='*100}\")\n        print(\"ğŸ“Š COMPARISON RESULTS\")\n        print(f\"{'='*100}\")\n        \n        print(f\"{'Model':<20} {'Weighted F1':<15} {'Jaccard Score':<15}\")\n        print(\"-\" * 50)\n        \n        for model_key, result in results.items():\n            print(f\"{model_key:<20} {result['weighted_f1']:<15.4f} {result['jaccard_score']:<15.4f}\")\n    \n    return results\n\ndef compare_all_models(train_file_path: str, use_quantization: bool = True, max_new_tokens: int = 6000):\n    \"\"\"\n    Compare performance of all available models on the training dataset\n    \"\"\"\n    models_to_test = ['qwen2.5-unsloth', 'deepseek-r1']\n    results = {}\n    \n    print(\"ğŸš€ Starting comprehensive model comparison...\")\n    print(\"=\" * 80)\n    \n    for model_key in models_to_test:\n        print(f\"\\nğŸ”„ Testing {model_key}...\")\n        try:\n            result = evaluate_model_on_training_data(\n                train_file_path=train_file_path,\n                model_key=model_key,\n                use_quantization=use_quantization,\n                max_new_tokens=max_new_tokens\n            )\n            if result:\n                results[model_key] = result\n            \n            # Clear memory between models\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n        except Exception as e:\n            print(f\"âŒ Error testing {model_key}: {e}\")\n            continue\n    \n    # Print comparison results\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ğŸ“Š MODEL COMPARISON RESULTS\")\n    print(\"=\" * 80)\n    \n    for model_key, result in results.items():\n        print(f\"\\n{model_key.upper()}:\")\n        print(f\"  Weighted F1 Score: {result['weighted_f1']:.4f}\")\n        print(f\"  Jaccard Score: {result['jaccard_score']:.4f}\")\n    \n    # Find best model\n    if results:\n        best_model = max(results.keys(), key=lambda k: results[k]['weighted_f1'])\n        print(f\"\\nğŸ† BEST MODEL: {best_model}\")\n        print(f\"   Weighted F1: {results[best_model]['weighted_f1']:.4f}\")\n        print(f\"   Jaccard Score: {results[best_model]['jaccard_score']:.4f}\")\n    \n    return results\n\ndef install_unsloth():\n    \"\"\"\n    Install Unsloth for optimized inference (run this once)\n    \"\"\"\n    install_commands = [\n        \"pip install unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\",\n        \"pip install --no-deps trl peft accelerate bitsandbytes\"\n    ]\n    \n    print(\"ğŸ“¦ To install Unsloth for optimized inference, run these commands:\")\n    for cmd in install_commands:\n        print(f\"   {cmd}\")\n    \n    print(\"\\nâš ï¸  Note: Restart your runtime after installation!\")\n\ndef benchmark_inference_speed(model_key: str, sample_questions: List[str], use_quantization: bool = True):\n    \"\"\"\n    Benchmark inference speed for a specific model\n    \n    Args:\n        model_key: Model to benchmark\n        sample_questions: List of sample questions to test\n        use_quantization: Whether to use quantization\n    \"\"\"\n    import time\n    \n    print(f\"ğŸš€ Benchmarking {model_key} inference speed...\")\n    \n    classifier = MultiModelArabicMedicalClassifier(\n        model_key=model_key,\n        use_quantization=use_quantization\n    )\n    \n    # Warm-up run\n    classifier.classify_question(sample_questions[0], max_new_tokens=2000)\n    \n    # Benchmark runs\n    start_time = time.time()\n    total_tokens = 0\n    \n    for question in sample_questions:\n        result = classifier.classify_question(question, max_new_tokens=2000)\n        total_tokens += len(classifier.tokenizer.encode(question))\n    \n    end_time = time.time()\n    total_time = end_time - start_time\n    \n    print(f\"â±ï¸  Benchmark Results for {model_key}:\")\n    print(f\"   Total Time: {total_time:.2f} seconds\")\n    print(f\"   Samples Processed: {len(sample_questions)}\")\n    print(f\"   Average Time per Sample: {total_time/len(sample_questions):.2f} seconds\")\n    print(f\"   Total Input Tokens: {total_tokens}\")\n    print(f\"   Tokens per Second: {total_tokens/total_time:.2f}\")\n    \n    classifier.cleanup_memory()\n    return {\n        'model_key': model_key,\n        'total_time': total_time,\n        'samples': len(sample_questions),\n        'avg_time_per_sample': total_time/len(sample_questions),\n        'tokens_per_second': total_tokens/total_time\n    }\n\ndef create_optimized_pipeline(model_key: str = 'qwen2.5-unsloth', batch_size: int = 4):\n    \"\"\"\n    Create an optimized inference pipeline for batch processing\n    \n    Args:\n        model_key: Model to use for the pipeline\n        batch_size: Number of samples to process in each batch\n    \n    Returns:\n        Optimized classifier instance\n    \"\"\"\n    print(f\"ğŸ”§ Creating optimized pipeline with {model_key}...\")\n    \n    classifier = MultiModelArabicMedicalClassifier(\n        model_key=model_key,\n        use_quantization=True,\n        use_thinking_mode=False  # Disable for faster inference\n    )\n    \n    # Set model to evaluation mode for inference\n    classifier.model.eval()\n    \n    # Enable torch.compile for PyTorch 2.0+ (if available)\n    try:\n        import torch._dynamo\n        classifier.model = torch.compile(classifier.model, mode=\"reduce-overhead\")\n        print(\"âœ… Torch compile enabled for faster inference\")\n    except:\n        print(\"âš ï¸  Torch compile not available, using standard inference\")\n    \n    print(f\"âœ… Optimized pipeline ready with batch size: {batch_size}\")\n    return classifier\n\ndef test_new_models():\n    \"\"\"\n    Test the newly added models with sample data\n    \"\"\"\n    sample_questions = [\n        \"Ù…Ø§ Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„ØµØ¯Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ…Ø±ØŸ\",\n        \"ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø¹Ù„Ø§Ø¬ Ø¢Ù„Ø§Ù… Ø§Ù„Ù…ÙØ§ØµÙ„ØŸ\",\n        \"Ù…Ø§ Ù‡ÙŠ ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ÙƒØ¨Ø¯ ÙÙŠ Ø§Ù„Ø¬Ø³Ù…ØŸ\",\n        \"Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø£Ù† ØªÙ†ØµØ­Ù†ÙŠ Ø¨Ø·Ø¨ÙŠØ¨ Ø¬ÙŠØ¯ Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø£Ø·ÙØ§Ù„ØŸ\",\n        \"Ù…Ø§ Ù‡ÙŠ Ø£ÙØ¶Ù„ Ø§Ù„ØªÙ…Ø§Ø±ÙŠÙ† Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØµØ­Ø©ØŸ\"\n    ]\n    \n    models_to_test = ['qwen2.5-unsloth', 'deepseek-r1']\n    \n    for model_key in models_to_test:\n        print(f\"\\n{'='*50}\")\n        print(f\"Testing {model_key}\")\n        print(f\"{'='*50}\")\n        \n        try:\n            classifier = MultiModelArabicMedicalClassifier(\n                model_key=model_key,\n                use_quantization=True\n            )\n            \n            for i, question in enumerate(sample_questions):\n                result = classifier.classify_question(question, max_new_tokens=3000)\n                print(f\"\\nSample {i+1}: {question}\")\n                print(f\"Classification: {result}\")\n            \n            classifier.cleanup_memory()\n            \n        except Exception as e:\n            print(f\"âŒ Error testing {model_key}: {e}\")\n\nif __name__ == \"__main__\":\n    # Update this path to your training dataset\n    TRAIN_DATASET_PATH = '/kaggle/input/train-dataset/Train_Dev.tsv'  # Replace with actual path\n\n    # List available models\n    MultiModelArabicMedicalClassifier.list_available_models()\n    \n    # Show installation instructions for Unsloth\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸš€ INSTALLATION GUIDE\")\n    print(\"=\"*80)\n    install_unsloth()\n\n    # Evaluate with Unsloth Qwen2.5 (recommended for speed)\n    # print(\"\\n\" + \"=\" * 50)\n    # print(\"Testing Unsloth Qwen2.5-7B-Instruct\")\n    # print(\"=\" * 50)\n    # results_unsloth = evaluate_model_on_training_data(\n    #     train_file_path=TRAIN_DATASET_PATH,\n    #     model_key='qwen2.5-unsloth',\n    #     use_quantization=True,\n    #     max_new_tokens=6000\n    # )\n\n    # Evaluate with DeepSeek R1 (recommended for reasoning)\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Testing DeepSeek-R1-Distill-Qwen-7B\")\n    print(\"=\" * 50)\n    results_deepseek = evaluate_model_on_training_data(\n        train_file_path=TRAIN_DATASET_PATH,\n        model_key='deepseek-r1',\n        use_quantization=True,\n        max_new_tokens=6000\n    )\n    \n    # Uncomment to run comprehensive comparison\n    # comprehensive_results = compare_all_models(\n    #     train_file_path=TRAIN_DATASET_PATH,\n    #     use_quantization=True,\n    #     max_new_tokens=6000\n    # )\n\n    # Print summary if both models were tested\n    if results_deepseek:\n        print(\"\\n\" + \"=\" * 80)\n        \n        print(f\"\\nDeepSeek R1 Distill Qwen-7B:\")\n        print(f\"  Weighted F1: {results_deepseek['weighted_f1']:.4f}\")\n        print(f\"  Jaccard Score: {results_deepseek['jaccard_score']:.4f}\")\n        print(f\"  Advantage: Advanced reasoning capabilities\")\n        \n        \n        print(f\"\\nğŸ’¡ Recommendation:\")\n        print(f\"   - Use 'qwen2.5-unsloth' for faster inference and production deployment\")\n        print(f\"   - Use 'deepseek-r1' for complex reasoning tasks and research\")\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ§ª TESTING NEW MODELS\")\n    print(\"=\"*80)\n    # Uncomment to test the new models with sample data\n    # test_new_models()\n    \n    print(\"\\nâœ… Setup complete! The classifier now supports:\")\n    print(\"   - unsloth/Qwen2.5-7B-Instruct (Optimized for speed)\")\n    print(\"   - deepseek-ai/DeepSeek-R1-Distill-Qwen-7B (Advanced reasoning)\")\n    print(\"\\nğŸ’¡ Use 'qwen2.5-unsloth' as default for best performance!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-27T08:35:32.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}