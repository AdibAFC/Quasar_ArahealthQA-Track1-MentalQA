{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12479800,"sourceType":"datasetVersion","datasetId":7874336},{"sourceId":12589423,"sourceType":"datasetVersion","datasetId":7951272}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:34:43.274725Z","iopub.execute_input":"2025-07-27T08:34:43.275031Z","iopub.status.idle":"2025-07-27T08:34:43.291282Z","shell.execute_reply.started":"2025-07-27T08:34:43.275005Z","shell.execute_reply":"2025-07-27T08:34:43.290627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers>=4.51.0 torch torchvision torchaudio accelerate bitsandbytes -q\n!pip install sentencepiece protobuf -q\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport gc\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\n# Check GPU availability\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:34:43.292522Z","iopub.execute_input":"2025-07-27T08:34:43.292762Z","execution_failed":"2025-07-27T08:35:32.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nimport re\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom typing import List, Dict, Tuple, Any\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass ArabicMedicalQuestionClassifier:\n    def __init__(self):\n        self.model_name = \"Qwen/Qwen3-14B\"\n        self.tokenizer = None\n        self.model = None\n        self.question_categories = {\n            'A': 'Diagnosis (questions about interpreting clinical findings)',\n            'B': 'Treatment (questions about seeking treatments)',\n            'C': 'Anatomy and Physiology (questions about basic medical knowledge)',\n            'D': 'Epidemiology (questions about the course, prognosis, and etiology of diseases)',\n            'E': 'Healthy Lifestyle (questions related to diet, exercise, and mood control)',\n            'F': 'Provider Choices (questions seeking recommendations for medical professionals and facilities)',\n            'Z': 'Other (questions that do not fall under the above-mentioned categories)'\n        }\n        self.load_model()\n    \n    def load_model(self):\n        \"\"\"Load Qwen3-14B with optimizations for Arabic medical text classification\"\"\"\n        print(\"Loading Qwen3-14B model for Arabic Medical Question Classification...\")\n        \n        # Configure quantization for memory efficiency\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.model_name,\n            trust_remote_code=True\n        )\n        \n        # Load model with quantization\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            quantization_config=quantization_config,\n            device_map=\"auto\",\n            trust_remote_code=True,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True\n        )\n        \n        print(\"Qwen3-14B model loaded successfully!\")\n        self.print_model_info()\n    \n    def print_model_info(self):\n        \"\"\"Print model and memory information\"\"\"\n        print(f\"\\nModel Information:\")\n        print(f\"Model Name: {self.model_name}\")\n        print(f\"Model Parameters: 14.8B (13.2B non-embedding)\")\n        print(f\"Context Length: 32,768 tokens\")\n        print(f\"Tokenizer Vocab Size: {len(self.tokenizer):,}\")\n        \n        if torch.cuda.is_available():\n            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n    \n    def classify_question(self, question: str, use_thinking_mode: bool = True, max_new_tokens: int = 8000) -> List[str]:\n        \"\"\"\n        Classify Arabic medical question into categories (Sub-Task 1)\n        \n        Args:\n            question: Arabic medical question\n            use_thinking_mode: Enable Qwen3's thinking capabilities\n            max_new_tokens: Maximum tokens to generate\n        \n        Returns:\n            List[str]: extracted_categories\n        \"\"\"\n        \n        # Create category descriptions in Arabic for better understanding\n        category_descriptions = \"\"\"\nفئات الأسئلة الطبية:\n(A) التشخيص - أسئلة حول تفسير النتائج السريرية والأعراض\n(B) العلاج - أسئلة حول البحث عن علاجات وطرق العلاج\n(C) التشريح وعلم وظائف الأعضاء - أسئلة حول المعرفة الطبية الأساسية\n(D) علم الأوبئة - أسئلة حول مسار المرض وتشخيصه وأسبابه\n(E) نمط الحياة الصحي - أسئلة متعلقة بالنظام الغذائي والرياضة والصحة النفسية\n(F) اختيار مقدم الرعاية - أسئلة تطلب توصيات للمهنيين الطبيين والمرافق\n(Z) أخرى - أسئلة لا تندرج تحت الفئات المذكورة أعلاه\n\"\"\"\n        \n        messages = [\n            {\n                \"role\": \"user\", \n                \"content\": f\"\"\"أنت خبير في تصنيف الأسئلة الطبية باللغة العربية. مهمتك هي تصنيف السؤال التالي إلى فئة أو أكثر من الفئات المحددة.\n\n{category_descriptions}\n\nالسؤال المراد تصنيفه:\n{question}\n\nتعليمات:\n1. اقرأ السؤال بعناية وحلل محتواه\n2. حدد الفئة أو الفئات المناسبة (يمكن أن يكون هناك أكثر من فئة واحدة)\n3. اشرح سبب اختيارك لكل فئة\n4. في النهاية، اكتب الإجابة بالتنسيق التالي:\n   \"التصنيف النهائي: [A,B,C]\" (استخدم الأحرف المناسبة مفصولة بفواصل)\n\nمثال على التنسيق:\n- إذا كان السؤال عن التشخيص فقط: \"التصنيف النهائي: [A]\"\n- إذا كان السؤال عن التشخيص والعلاج: \"التصنيف النهائي: [A,B]\"\n\"\"\"\n            }\n        ]\n        \n        # Apply chat template with thinking mode control\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n            enable_thinking=use_thinking_mode\n        )\n        \n        # Tokenize input\n        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n        \n        # Generate response\n        with torch.no_grad():\n            if use_thinking_mode:\n                generated_ids = self.model.generate(\n                    **model_inputs,\n                    max_new_tokens=max_new_tokens,\n                    temperature=0.6,\n                    top_p=0.95,\n                    top_k=20,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    repetition_penalty=1.1\n                )\n            else:\n                generated_ids = self.model.generate(\n                    **model_inputs,\n                    max_new_tokens=max_new_tokens,\n                    temperature=0.7,\n                    top_p=0.8,\n                    top_k=20,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    repetition_penalty=1.1\n                )\n        \n        # Extract output tokens\n        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n        \n        # Parse thinking content (if thinking mode is enabled)\n        content = \"\"\n        \n        if use_thinking_mode:\n            try:\n                # Find </think> token (151668)\n                index = len(output_ids) - output_ids[::-1].index(151668)\n                content = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()\n            except ValueError:\n                content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n        else:\n            content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n        \n        categories = self.extract_question_categories(content)\n        \n        return categories\n    \n    def extract_question_categories(self, response: str) -> List[str]:\n        \"\"\"Extract question categories from Arabic response text\"\"\"\n        \n        # Arabic patterns for category extraction\n        patterns = [\n            r'التصنيف النهائي:\\s*\\[([ABCDEFZ,\\s]+)\\]',  # \"التصنيف النهائي: [A,B,C]\"\n            r'الفئات:\\s*\\[([ABCDEFZ,\\s]+)\\]',  # \"الفئات: [A,B,C]\"\n            r'التصنيف:\\s*\\[([ABCDEFZ,\\s]+)\\]',  # \"التصنيف: [A,B,C]\"\n            r'النتيجة:\\s*\\[([ABCDEFZ,\\s]+)\\]',  # \"النتيجة: [A,B,C]\"\n        ]\n        \n        for pattern in patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                categories_str = match.group(1)\n                categories = [cat.strip().upper() for cat in categories_str.split(',')]\n                return [cat for cat in categories if cat in ['A', 'B', 'C', 'D', 'E', 'F', 'Z']]\n        \n        # English patterns as fallback\n        english_patterns = [\n            r'Final Classification:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'Categories:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n            r'Classification:\\s*\\[([ABCDEFZ,\\s]+)\\]',\n        ]\n        \n        for pattern in english_patterns:\n            match = re.search(pattern, response, re.IGNORECASE)\n            if match:\n                categories_str = match.group(1)\n                categories = [cat.strip().upper() for cat in categories_str.split(',')]\n                return [cat for cat in categories if cat in ['A', 'B', 'C', 'D', 'E', 'F', 'Z']]\n        \n        # Look for individual category mentions\n        found_categories = []\n        for category in ['A', 'B', 'C', 'D', 'E', 'F', 'Z']:\n            if f'({category})' in response or f'[{category}]' in response:\n                found_categories.append(category)\n        \n        return found_categories if found_categories else ['Z']  # Default to 'Other' if nothing found\n    \n    def process_test_dataset(self, df: pd.DataFrame, use_thinking: bool = True, show_progress: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Process test dataset for question classification only\n        \n        Args:\n            df: DataFrame with 'question' column\n            use_thinking: Enable thinking mode\n            show_progress: Show progress information\n        \n        Returns:\n            DataFrame with predictions\n        \"\"\"\n        \n        print(\"🚀 Starting Arabic Medical Question Classification for Test Dataset...\")\n        print(f\"Test dataset size: {len(df)} samples\")\n        print(\"-\" * 60)\n        \n        # Initialize result list\n        predictions = []\n        \n        for idx, row in df.iterrows():\n            if show_progress and idx % 10 == 0:\n                print(f\"Processing sample {idx+1}/{len(df)}\")\n            \n            # Classify question\n            try:\n                categories = self.classify_question(\n                    row['question'], \n                    use_thinking_mode=use_thinking\n                )\n                # Convert list to comma-separated string as required by submission format\n                prediction_str = ', '.join(sorted(categories))\n                predictions.append(prediction_str)\n            except Exception as e:\n                print(f\"Error processing question {idx}: {e}\")\n                predictions.append('Z')\n            \n            # Clean up memory periodically\n            if idx % 20 == 0:\n                self.cleanup_memory()\n        \n        # Create result dataframe with just the predictions\n        result_df = pd.DataFrame({'prediction': predictions})\n        \n        return result_df\n    \n    def cleanup_memory(self):\n        \"\"\"Clean up GPU memory\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n\n# Initialize the classifier\nprint(\"Initializing Arabic Medical Question Classifier with Qwen3-14B...\")\nclassifier = ArabicMedicalQuestionClassifier()\n\ndef generate_test_predictions(test_file_path: str, output_file_path: str = 'prediction_subtask_1.tsv', use_thinking: bool = True):\n    \"\"\"\n    Generate predictions for test dataset and save to TSV file\n    \n    Args:\n        test_file_path: Path to test dataset TSV file\n        output_file_path: Path for output predictions file\n        use_thinking: Enable thinking mode\n    \"\"\"\n    \n    print(\"🚀 Loading test dataset...\")\n    \n    # Load test dataset\n    try:\n        df = pd.read_csv(test_file_path, sep='\\t', header=None)\n        df.columns = ['question']\n        # df=df[:1]\n        print(f\"✅ Test dataset loaded successfully: {len(df)} samples\")\n    except Exception as e:\n        print(f\"❌ Error loading test dataset: {e}\")\n        return\n    \n    # Check if 'question' column exists\n    if 'question' not in df.columns:\n        print(\"❌ Error: 'question' column not found in test dataset\")\n        print(f\"Available columns: {list(df.columns)}\")\n        return\n    \n    # Process test dataset\n    print(\"🔍 Processing test dataset...\")\n    results_df = classifier.process_test_dataset(df, use_thinking=use_thinking)\n    \n    # Save predictions to TSV file\n    try:\n        # Save as TSV without header, just predictions\n        results_df['prediction'].to_csv(output_file_path, sep='\\t', index=False, header=False)\n        print(f\"✅ Predictions saved to: {output_file_path}\")\n        print(f\"📄 Total predictions: {len(results_df)}\")\n        \n        # Show sample predictions\n        print(\"\\n📋 Sample predictions:\")\n        for i in range(min(5, len(results_df))):\n            print(f\"Sample {i+1}: {results_df['prediction'].iloc[i]}\")\n            \n    except Exception as e:\n        print(f\"❌ Error saving predictions: {e}\")\n    \n    # Clean up memory\n    classifier.cleanup_memory()\n    \n    return results_df\n\ndef get_model_status():\n    \"\"\"Display current model status\"\"\"\n    print(\"\\n📊 Model Status:\")\n    print(f\"Model: {classifier.model_name}\")\n    print(f\"Model loaded: {'✅' if classifier.model else '❌'}\")\n    if torch.cuda.is_available():\n        print(f\"GPU memory usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    classifier.print_model_info()\n\n# Main execution\nif __name__ == \"__main__\":\n    print(\"🚀 Arabic Medical Question Classifier for Test Dataset Ready!\")\n    \n    # Replace with your actual test dataset path\n    TEST_DATASET_PATH = '/kaggle/input/testdf/subtask1_input_test.tsv'  # Update this path\n    \n    # Generate predictions\n    # Uncomment the following line and update the path:\n    generate_test_predictions(TEST_DATASET_PATH)\n    \n    # For demonstration with sample data (remove this in actual usage):\n    # print(\"📚 Creating sample test to demonstrate functionality...\")\n    # sample_data = pd.DataFrame({\n    #     'question': [\n    #         'هل يمكن أن يكون الصداع المستمر علامة على مرض خطير؟'\n    #     ]\n    # })\n    \n    # print(\"Processing sample data...\")\n    # sample_results = classifier.process_test_dataset(sample_data, use_thinking=True)\n    # print(\"\\nSample predictions:\")\n    # for i, pred in enumerate(sample_results['prediction']):\n    #     print(f\"Question {i+1}: {pred}\")\n    \n    # # Save sample predictions\n    # sample_results['prediction'].to_csv('sample_prediction_subtask_1.tsv', sep='\\t', index=False, header=False)\n    # print(\"Sample predictions saved to: sample_prediction_subtask_1.tsv\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-27T08:35:32.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}